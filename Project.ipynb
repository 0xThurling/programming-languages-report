{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692e25c2-5048-4816-b5a8-8c0dc496a8e0",
   "metadata": {},
   "source": [
    "## Initial Import of my Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a4f47a-f862-4cc5-b4b9-31fa299083b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OS lib\n",
    "import os\n",
    "\n",
    "# Prepare for OCR to get information from the initial images\n",
    "import easyocr\n",
    "\n",
    "# Prepare for crawling\n",
    "from googlesearch import search\n",
    "\n",
    "# Prepare language data\n",
    "import csv\n",
    "\n",
    "# Webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Analysis\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from PIL import Image\n",
    "\n",
    "# Show all matplotlib graphs inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set all graphs to a seaborn style with a grey background grid which makes reading graphs easier\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34558a-b3e2-446f-95be-06b705506dd9",
   "metadata": {},
   "source": [
    "## Collection of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6a265e0-ae33-4690-9885-e1860dea5cad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "languages = {}\n",
    "async_tools = {}\n",
    "ai_tools = {}\n",
    "cloud_tools = {}\n",
    "development_environment = {}\n",
    "frameworks = {}\n",
    "web_frameworks = {}\n",
    "sync_tools = {}\n",
    "tools = {}\n",
    "\n",
    "with open(\"languages.csv\", \"r\") as file:\n",
    "    language_dict = csv.DictReader(file)\n",
    "\n",
    "    for row in language_dict:\n",
    "        if row[\"type\"] == \"pl\":\n",
    "            key_id = row[\"pldb_id\"]\n",
    "            languages[key_id] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99157b1d-7654-4b34-a335-f7e25ecaad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('./images/') \n",
    "         if os.path.isfile(os.path.join('./images/', f))]\n",
    "\n",
    "# Show list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c3336c-68d3-4a47-8bff-6351e3ecc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnesesarry_info(text:str):\n",
    "        if '%' in text:\n",
    "            return None\n",
    "            \n",
    "        match text:\n",
    "            case 'Source: surveystackoverflow.co/2024' : return None\n",
    "            case 'Source: survey stackoverflow.co/2024' : return None\n",
    "            case text if 'Most' in text: return None\n",
    "            case text if 'popular' in text : return None\n",
    "            case text if 'Respondents' in text : return None\n",
    "            case 'Web frameworks and technologies': return None\n",
    "            case 'Developer': return None\n",
    "            case 'Survey' : return None\n",
    "            case 'Integrated development environment' : return None\n",
    "            case 'Data licensed under Open Database License (ODbL)' : return None\n",
    "            case '2024' : return None\n",
    "                \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482e6f6d-f4dd-46ab-8e7f-74b0d0f3b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_languages = []\n",
    "\n",
    "def process_languages(text):\n",
    "    if text is None or'%' in text or len(text.split()) > 1:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'JS' : temp_list_of_languages.append('JavaScript')\n",
    "        case 'C+-': temp_list_of_languages.append('Cpp')\n",
    "        case 'PY' : temp_list_of_languages.append('Python')\n",
    "        case 'TS' : temp_list_of_languages.append('TypeScript')\n",
    "        case 'C#' : temp_list_of_languages.append('CSharp')\n",
    "        case '2024' : return\n",
    "        case _ : temp_list_of_languages.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d0e4c00-b23d-4228-8493-ae5a77ad0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_web_frameworks = []\n",
    "\n",
    "def process_web_frameworks(text):\n",
    "    if text is None or'%' in text:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'Node:js' : temp_list_of_web_frameworks.append('Node.js')\n",
    "        case 'Express' : temp_list_of_web_frameworks.append('Express.js')\n",
    "        case 'Next js': temp_list_of_web_frameworks.append('Next.js')\n",
    "        case 'Vuejs': temp_list_of_web_frameworks.append('Vue.js')\n",
    "        case 'Flask': temp_list_of_web_frameworks.append('Flask python')\n",
    "        case '2024' : return\n",
    "        case _ : temp_list_of_web_frameworks.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003e2a0f-aceb-44be-9eba-e6760f9d9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_ides = []\n",
    "\n",
    "def process_ides(text):\n",
    "    if text is None or'%' in text:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'Notepad+ +' : temp_list_of_ides.append('Notepad++')\n",
    "        case 'Jupyter NotebooklJupyterLab' : temp_list_of_ides.append('JupyterLab')\n",
    "        case _ : temp_list_of_ides.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b73de9d5-7332-48cf-b81e-70cf54353c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_tools = []\n",
    "\n",
    "def process_tools(text):\n",
    "    if text is None or'%' in text:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'Other tools' : return\n",
    "        case _ : temp_list_of_tools.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c214ea3d-c5c4-4ff8-98f0-0c63e713ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = easyocr.Reader([\"en\"])\n",
    "\n",
    "files = [f for f in os.listdir('./images/') \n",
    "         if os.path.isfile(os.path.join('./images/', f))]\n",
    "\n",
    "for file in files:\n",
    "    result = reader.readtext(\"./images/\" + file)\n",
    "    \n",
    "    # Fix data given from the image - only extract the programming languages\n",
    "    for (bbox, text, prob) in result:\n",
    "        match file:\n",
    "            case \"popular-languages.png\": process_languages(remove_unnesesarry_info(text))\n",
    "            case \"popular-web-framework.png\": process_web_frameworks(remove_unnesesarry_info(text))\n",
    "            case \"popular-development-environment.png\": process_ides(remove_unnesesarry_info(text))\n",
    "            case \"popular-tools.png\": process_tools(remove_unnesesarry_info(text))\n",
    "\n",
    "# Clear memory\n",
    "del reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "921278b2-cf19-429b-ac8d-685e1606e8f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JavaScript', 'Python', 'TypeScript', 'Java', 'CSharp', 'Cpp', 'PHP', 'PowerShell', 'Rust', 'Kotlin']\n",
      "['Node.js', 'React', 'jQuery', 'Next.js', 'Express.js', 'Angular', 'ASPNET CORE', 'Vue.js', 'ASPNET', 'Flask python']\n",
      "['Visual Studio Code', 'Visual Studio', 'Intellij IDEA', 'Notepad++', 'Vim', 'Android Studio', 'PyCharm', 'JupyterLab', 'Neovim', 'Sublime Text']\n",
      "['Docker', 'npm', 'Pip', 'Homebrew', 'Make', 'Vite', 'Kubernetes', 'Yarn', 'Webpack', 'NuGet']\n"
     ]
    }
   ],
   "source": [
    "# Get the top 10 languages - this excludes things like SQL\n",
    "languages_for_analysis = [lang for lang in temp_list_of_languages if lang.lower() in languages][:10]\n",
    "web_frameworks_for_analysis = [w_frame for w_frame in temp_list_of_web_frameworks][:10]\n",
    "ides_for_analysis = [ide for ide in temp_list_of_ides][:10]\n",
    "tools_for_analysis = [tool for tool in temp_list_of_tools][:10]\n",
    "\n",
    "print(languages_for_analysis)\n",
    "print(web_frameworks_for_analysis)\n",
    "print(ides_for_analysis)\n",
    "print(tools_for_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdca7049-bec0-44f8-8942-c426b4315ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit_data(url: str):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for comments to load\n",
    "    try:\n",
    "        comments = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[id=\"-post-rtjson-content\"]')))\n",
    "    \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        paragraphs = soup.select('div[id=\"-post-rtjson-content\"] p')\n",
    "    \n",
    "        texts = [p.get_text(strip=True) for p in paragraphs]\n",
    "\n",
    "        driver.close()\n",
    "    \n",
    "        return texts\n",
    "    except:\n",
    "        driver.close()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3b129f-ab27-4bc9-b3d2-fb98ed7214bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_hackernews_data(url: str):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for comments to load\n",
    "    try:\n",
    "        \n",
    "        comments = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.comment div.commtext.c00')))\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        paragraphs = soup.select('div.comment div.commtext.c00')     \n",
    "        \n",
    "        texts = [p.get_text(strip=True) for p in paragraphs]\n",
    "\n",
    "        driver.close()\n",
    "    \n",
    "        return texts\n",
    "    except:\n",
    "        driver.close()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b398fc-032f-4766-a480-ead9d27e2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search\n",
    "# Reddit\n",
    "# Hacknews i.e. Hackernews\n",
    "\n",
    "languages = {}\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for lang in languages_for_analysis:\n",
    "    # # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + lang + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in languages:\n",
    "            languages[lang] = data\n",
    "        else:\n",
    "            languages[lang] += data\n",
    "\n",
    "    query =  \"Opinion on \" + lang + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in languages:\n",
    "            languages[lang] = data\n",
    "        else:\n",
    "            languages[lang] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7091f-ce4a-4390-ab13-163e2c4e3839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70142863-5e39-4be4-bdea-3594af531d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information for the web frameworks\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for w_frame in web_frameworks_for_analysis:\n",
    "    # # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + w_frame + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in web_frameworks:\n",
    "            web_frameworks[w_frame] = data\n",
    "        else:\n",
    "            web_frameworks[w_frame] += data\n",
    "\n",
    "    query =  \"Opinion on \" + w_frame + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in web_frameworks:\n",
    "            web_frameworks[w_frame] = data\n",
    "        else:\n",
    "            web_frameworks[w_frame] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ee9aa05-b5b3-4696-a109-332158047a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information for the integrated development environment\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for ide in ides_for_analysis:\n",
    "    # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + ide + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in development_environment:\n",
    "            development_environment[ide] = data\n",
    "        else:\n",
    "            development_environment[ide] += data\n",
    "\n",
    "    query =  \"Opinion on \" + ide + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in development_environment:\n",
    "            development_environment[ide] = data\n",
    "        else:\n",
    "            development_environment[ide] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22b010a9-758d-4b49-a081-5c1d7eacfa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information for the integrated development environment\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for dev in temp_list_of_tools:\n",
    "    # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + dev + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in tools:\n",
    "            tools[dev] = data\n",
    "        else:\n",
    "            tools[dev] += data\n",
    "\n",
    "    query =  \"Opinion on \" + dev + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in tools:\n",
    "            tools[dev] = data\n",
    "        else:\n",
    "            tools[dev] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf2c3036-01ea-44e8-b542-30bc5d9da324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the text corpus to txt files\n",
    "# Languages\n",
    "for key, value in languages.items():\n",
    "    try:\n",
    "        with open(f\"./languages/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue\n",
    "\n",
    "# Development environment\n",
    "for key, value in development_environment.items():\n",
    "    try:\n",
    "        with open(f\"./ide/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue\n",
    "\n",
    "# Development tools\n",
    "for key, value in tools.items():\n",
    "    try:\n",
    "        with open(f\"./tools/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue\n",
    "\n",
    "# Web Frameworks\n",
    "for key, value in web_frameworks.items():\n",
    "    try:\n",
    "        with open(f\"./web-framework/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c9b34b-627d-4f34-a4e9-6b27e3094ceb",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f50d1d82-fb0b-4075-9772-f530a21a6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_files = [f for f in os.listdir('./languages/') \n",
    "         if os.path.isfile(os.path.join('./languages/', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e393c078-aeb8-4d1a-926c-1b3bf808a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = pd.DataFrame(columns=['Programming_language', 'Comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfda9641-de9c-4d15-b4cc-060fd2293bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Programming_language</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Java</td>\n",
       "      <td>but java also is catching up with modern featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Programming_language                                           Comments\n",
       "count                    10                                                 10\n",
       "unique                   10                                                 10\n",
       "top                    Java  but java also is catching up with modern featu...\n",
       "freq                      1                                                  1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, file_name in enumerate(language_files):\n",
    "    file_text = \"\"\n",
    "    with open(f\"./languages/{file_name}\", 'r') as file:\n",
    "        file_text = file.read()\n",
    "    \n",
    "    programming_language = file_name.replace('.txt', '')\n",
    "    scraped_data.loc[idx] = {\n",
    "        \"Programming_language\" : programming_language,\n",
    "        \"Comments\" : file_text\n",
    "    }\n",
    "\n",
    "scraped_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96b26eb4-af92-4ba6-ad42-f62823ae9431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java has a total corpus of 66859 words\n",
      "Rust has a total corpus of 91473 words\n",
      "JavaScript has a total corpus of 57585 words\n",
      "PowerShell has a total corpus of 22634 words\n",
      "Python has a total corpus of 66986 words\n",
      "Cpp has a total corpus of 102126 words\n",
      "PHP has a total corpus of 94804 words\n",
      "TypeScript has a total corpus of 68149 words\n",
      "Kotlin has a total corpus of 32613 words\n",
      "CSharp has a total corpus of 42993 words\n"
     ]
    }
   ],
   "source": [
    "# Looking at collected corpura\n",
    "for n in range(0, len(scraped_data)):\n",
    "    msg = \"\"\"{Programming_language} has a total corpus of {total} words\"\"\".format(\n",
    "        Programming_language = scraped_data[\"Programming_language\"][n],\n",
    "        total = len(scraped_data[\"Comments\"][n].split(' '))\n",
    "    )\n",
    "\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8425c9-4ec5-49d6-b7f9-60c6558ad33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
