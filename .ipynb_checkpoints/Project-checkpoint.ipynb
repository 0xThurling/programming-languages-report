{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692e25c2-5048-4816-b5a8-8c0dc496a8e0",
   "metadata": {},
   "source": [
    "## Initial Import of my Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a4f47a-f862-4cc5-b4b9-31fa299083b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OS lib\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Prepare for OCR to get information from the initial images\n",
    "import easyocr\n",
    "\n",
    "# Prepare for crawling\n",
    "from googlesearch import search\n",
    "\n",
    "# Prepare language data\n",
    "import csv\n",
    "\n",
    "# Webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Analysis\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Show all matplotlib graphs inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set all graphs to a seaborn style with a grey background grid which makes reading graphs easier\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34558a-b3e2-446f-95be-06b705506dd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Collection of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6a265e0-ae33-4690-9885-e1860dea5cad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "languages = {}\n",
    "development_environment = {}\n",
    "web_frameworks = {}\n",
    "\n",
    "with open(\"languages.csv\", \"r\") as file:\n",
    "    language_dict = csv.DictReader(file)\n",
    "\n",
    "    for row in language_dict:\n",
    "        if row[\"type\"] == \"pl\":\n",
    "            key_id = row[\"pldb_id\"]\n",
    "            languages[key_id] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99157b1d-7654-4b34-a335-f7e25ecaad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('./images/') \n",
    "         if os.path.isfile(os.path.join('./images/', f))]\n",
    "\n",
    "# Show list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6c3336c-68d3-4a47-8bff-6351e3ecc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnesesarry_info(text:str):\n",
    "        if '%' in text:\n",
    "            return None\n",
    "            \n",
    "        match text:\n",
    "            case 'Source: surveystackoverflow.co/2024' : return None\n",
    "            case 'Source: survey stackoverflow.co/2024' : return None\n",
    "            case text if 'Most' in text: return None\n",
    "            case text if 'popular' in text : return None\n",
    "            case text if 'Respondents' in text : return None\n",
    "            case 'Web frameworks and technologies': return None\n",
    "            case 'Developer': return None\n",
    "            case 'Survey' : return None\n",
    "            case 'Integrated development environment' : return None\n",
    "            case 'Data licensed under Open Database License (ODbL)' : return None\n",
    "            case '2024' : return None\n",
    "                \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "482e6f6d-f4dd-46ab-8e7f-74b0d0f3b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_languages = []\n",
    "\n",
    "def process_languages(text):\n",
    "    if text is None or'%' in text or len(text.split()) > 1:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'JS' : temp_list_of_languages.append('JavaScript')\n",
    "        case 'C+-': temp_list_of_languages.append('Cpp')\n",
    "        case 'PY' : temp_list_of_languages.append('Python')\n",
    "        case 'TS' : temp_list_of_languages.append('TypeScript')\n",
    "        case 'C#' : temp_list_of_languages.append('CSharp')\n",
    "        case '2024' : return\n",
    "        case _ : temp_list_of_languages.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d0e4c00-b23d-4228-8493-ae5a77ad0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_web_frameworks = []\n",
    "\n",
    "def process_web_frameworks(text):\n",
    "    if text is None or'%' in text:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'Node:js' : temp_list_of_web_frameworks.append('Node.js')\n",
    "        case 'Express' : temp_list_of_web_frameworks.append('Express.js')\n",
    "        case 'Next js': temp_list_of_web_frameworks.append('Next.js')\n",
    "        case 'Vuejs': temp_list_of_web_frameworks.append('Vue.js')\n",
    "        case 'Flask': temp_list_of_web_frameworks.append('Flask python')\n",
    "        case '2024' : return\n",
    "        case _ : temp_list_of_web_frameworks.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "003e2a0f-aceb-44be-9eba-e6760f9d9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_ides = []\n",
    "\n",
    "def process_ides(text):\n",
    "    if text is None or'%' in text:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'Notepad+ +' : temp_list_of_ides.append('Notepad++')\n",
    "        case 'Jupyter NotebooklJupyterLab' : temp_list_of_ides.append('JupyterLab')\n",
    "        case _ : temp_list_of_ides.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c214ea3d-c5c4-4ff8-98f0-0c63e713ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = easyocr.Reader([\"en\"])\n",
    "\n",
    "files = [f for f in os.listdir('./images/') \n",
    "         if os.path.isfile(os.path.join('./images/', f))]\n",
    "\n",
    "for file in files:\n",
    "    result = reader.readtext(\"./images/\" + file)\n",
    "    \n",
    "    # Fix data given from the image - only extract the programming languages\n",
    "    for (bbox, text, prob) in result:\n",
    "        match file:\n",
    "            case \"popular-languages.png\": process_languages(remove_unnesesarry_info(text))\n",
    "            case \"popular-web-framework.png\": process_web_frameworks(remove_unnesesarry_info(text))\n",
    "            case \"popular-development-environment.png\": process_ides(remove_unnesesarry_info(text))\n",
    "\n",
    "# Clear memory\n",
    "del reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "921278b2-cf19-429b-ac8d-685e1606e8f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JavaScript', 'Python', 'TypeScript', 'Java', 'CSharp', 'Cpp', 'PHP', 'PowerShell', 'Rust', 'Kotlin']\n",
      "['Node.js', 'React', 'jQuery', 'Next.js', 'Express.js', 'Angular', 'ASPNET CORE', 'Vue.js', 'ASPNET', 'Flask python']\n",
      "['Visual Studio Code', 'Visual Studio', 'Intellij IDEA', 'Notepad++', 'Vim', 'Android Studio', 'PyCharm', 'JupyterLab', 'Neovim', 'Sublime Text']\n"
     ]
    }
   ],
   "source": [
    "# Get the top 10 languages - this excludes things like SQL\n",
    "languages_for_analysis = [lang for lang in temp_list_of_languages if lang.lower() in languages][:10]\n",
    "web_frameworks_for_analysis = [w_frame for w_frame in temp_list_of_web_frameworks][:10]\n",
    "ides_for_analysis = [ide for ide in temp_list_of_ides][:10]\n",
    "\n",
    "print(languages_for_analysis)\n",
    "print(web_frameworks_for_analysis)\n",
    "print(ides_for_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdca7049-bec0-44f8-8942-c426b4315ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit_data(url: str):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for comments to load\n",
    "    try:\n",
    "        comments = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[id=\"-post-rtjson-content\"]')))\n",
    "    \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        paragraphs = soup.select('div[id=\"-post-rtjson-content\"] p')\n",
    "    \n",
    "        texts = [p.get_text(strip=True) for p in paragraphs]\n",
    "\n",
    "        driver.close()\n",
    "    \n",
    "        return texts\n",
    "    except:\n",
    "        driver.close()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3b129f-ab27-4bc9-b3d2-fb98ed7214bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_hackernews_data(url: str):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for comments to load\n",
    "    try:\n",
    "        \n",
    "        comments = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.comment div.commtext.c00')))\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        paragraphs = soup.select('div.comment div.commtext.c00')     \n",
    "        \n",
    "        texts = [p.get_text(strip=True) for p in paragraphs]\n",
    "\n",
    "        driver.close()\n",
    "    \n",
    "        return texts\n",
    "    except:\n",
    "        driver.close()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b398fc-032f-4766-a480-ead9d27e2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search\n",
    "# Reddit\n",
    "# Hacknews i.e. Hackernews\n",
    "\n",
    "languages = {}\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for lang in languages_for_analysis:\n",
    "    # # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + lang + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in languages:\n",
    "            languages[lang] = data\n",
    "        else:\n",
    "            languages[lang] += data\n",
    "\n",
    "    query =  \"Opinion on \" + lang + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in languages:\n",
    "            languages[lang] = data\n",
    "        else:\n",
    "            languages[lang] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70142863-5e39-4be4-bdea-3594af531d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information for the web frameworks\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for w_frame in web_frameworks_for_analysis:\n",
    "    # # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + w_frame + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in web_frameworks:\n",
    "            web_frameworks[w_frame] = data\n",
    "        else:\n",
    "            web_frameworks[w_frame] += data\n",
    "\n",
    "    query =  \"Opinion on \" + w_frame + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in web_frameworks:\n",
    "            web_frameworks[w_frame] = data\n",
    "        else:\n",
    "            web_frameworks[w_frame] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ee9aa05-b5b3-4696-a109-332158047a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information for the integrated development environment\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for ide in ides_for_analysis:\n",
    "    # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + ide + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in development_environment:\n",
    "            development_environment[ide] = data\n",
    "        else:\n",
    "            development_environment[ide] += data\n",
    "\n",
    "    query =  \"Opinion on \" + ide + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in development_environment:\n",
    "            development_environment[ide] = data\n",
    "        else:\n",
    "            development_environment[ide] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22b010a9-758d-4b49-a081-5c1d7eacfa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information for the integrated development environment\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for dev in temp_list_of_tools:\n",
    "    # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + dev + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in tools:\n",
    "            tools[dev] = data\n",
    "        else:\n",
    "            tools[dev] += data\n",
    "\n",
    "    query =  \"Opinion on \" + dev + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in tools:\n",
    "            tools[dev] = data\n",
    "        else:\n",
    "            tools[dev] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf2c3036-01ea-44e8-b542-30bc5d9da324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the text corpus to txt files\n",
    "# Languages\n",
    "for key, value in languages.items():\n",
    "    try:\n",
    "        with open(f\"./languages/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue\n",
    "\n",
    "# Development environment\n",
    "for key, value in development_environment.items():\n",
    "    try:\n",
    "        with open(f\"./ide/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue\n",
    "\n",
    "# Development tools\n",
    "for key, value in tools.items():\n",
    "    try:\n",
    "        with open(f\"./tools/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue\n",
    "\n",
    "# Web Frameworks\n",
    "for key, value in web_frameworks.items():\n",
    "    try:\n",
    "        with open(f\"./web-framework/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c9b34b-627d-4f34-a4e9-6b27e3094ceb",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50d1d82-fb0b-4075-9772-f530a21a6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_files = [f for f in os.listdir('./languages/') \n",
    "         if os.path.isfile(os.path.join('./languages/', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfda9641-de9c-4d15-b4cc-060fd2293bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Programming_language</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Java</td>\n",
       "      <td>but java also is catching up with modern featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Programming_language                                           Comments\n",
       "count                    10                                                 10\n",
       "unique                   10                                                 10\n",
       "top                    Java  but java also is catching up with modern featu...\n",
       "freq                      1                                                  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data = pd.DataFrame(columns=['Programming_language', 'Comments'])\n",
    "\n",
    "for idx, file_name in enumerate(language_files):\n",
    "    file_text = \"\"\n",
    "    with open(f\"./languages/{file_name}\", 'r') as file:\n",
    "        file_text = file.read()\n",
    "    \n",
    "    programming_language = file_name.replace('.txt', '')\n",
    "    scraped_data.loc[idx] = {\n",
    "        \"Programming_language\" : programming_language,\n",
    "        \"Comments\" : file_text\n",
    "    }\n",
    "\n",
    "scraped_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b26eb4-af92-4ba6-ad42-f62823ae9431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java has a total corpus of 66859 words\n",
      "Rust has a total corpus of 91473 words\n",
      "JavaScript has a total corpus of 57585 words\n",
      "PowerShell has a total corpus of 22634 words\n",
      "Python has a total corpus of 66986 words\n",
      "Cpp has a total corpus of 102126 words\n",
      "PHP has a total corpus of 94804 words\n",
      "TypeScript has a total corpus of 68149 words\n",
      "Kotlin has a total corpus of 32613 words\n",
      "CSharp has a total corpus of 42993 words\n"
     ]
    }
   ],
   "source": [
    "scraped_data['comments_length'] = 0\n",
    "\n",
    "# Looking at collected corpura\n",
    "for n in range(0, len(scraped_data)):\n",
    "    msg = \"\"\"{Programming_language} has a total corpus of {total} words\"\"\".format(\n",
    "        Programming_language = scraped_data[\"Programming_language\"][n],\n",
    "        total = len(scraped_data[\"Comments\"][n].split(' '))\n",
    "    )\n",
    "\n",
    "    scraped_data.loc[n, 'comments_length'] = len(scraped_data[\"Comments\"][n].split(' '))\n",
    "    \n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa62ae-da57-4a95-8d37-c56f4ec569ed",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9804cc23-88e8-41b9-9107-2a80836286d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jacquesthurling/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def get_freq(corpus):\n",
    "    corpus = re.sub(r'\\.|\\>|\\,|\\\"|\\\"|\\\"|\\—|\\–|\\-|\\?|\\!|\\:|\\;|\\(|\\)', '', corpus)\n",
    "\n",
    "    corpus_lst = corpus.lower().split()\n",
    "\n",
    "    unique_words_full = set(corpus_lst)\n",
    "\n",
    "    unique_words = [word for word in unique_words_full if not word in stopwords.words('english')]\n",
    "\n",
    "    word_freqs = pd.DataFrame(columns=['Word', 'Frequency'])\n",
    "\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        word_freq_pair = {'Word':word, 'Frequency':corpus_lst.count(word)}\n",
    "        word_freqs.loc[idx] = word_freq_pair\n",
    "\n",
    "    return word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b72b8-868d-4bd7-a9c4-9bbab0587598",
   "metadata": {},
   "source": [
    "We can use the above function to get the word frequencies of each corpus and assess if we would like to treat any of the most frequent words as stop words\n",
    "\n",
    "(taken from the example project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84e0de-efbd-4f6c-8644-90ed8c370144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java: Unique words 7757\n",
      "Rust: Unique words 9788\n"
     ]
    }
   ],
   "source": [
    "language_frames = []\n",
    "\n",
    "for n in range(0, len(scraped_data)):\n",
    "    programming = scraped_data[\"Programming_language\"][n]\n",
    "    comments = scraped_data[\"Comments\"][n]\n",
    "\n",
    "    comment_word_freq = get_freq(comments)\n",
    "\n",
    "    print(f\"{programming}: Unique words {comment_word_freq.index.max()+1}\")\n",
    "    language_frames.append(comment_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496687db-ca14-4768-9795-6198f8567fe4",
   "metadata": {},
   "source": [
    "As we can see the amount of unique words is quite high for the different programming languages, this would correlate quite nicely, since we are looking at differing opinions from different platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae1ebc-a3be-417e-9f0d-ba8599532dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "\n",
    "# Get the common top words for all programming languages\n",
    "for n in range(0, len(language_frames)):\n",
    "    top_words += list((language_frames[n].sort_values('Frequency', ascending=False).head(300))['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e965122-70d4-4c43-a5f2-af455427c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b2a0f7-d5e1-4757-8a05-c5c450a2f9cf",
   "metadata": {},
   "source": [
    "I didn't create custom stopwords, since I am interested specifically about the person commenting, I will not remove the first-person singular pronous, as this will have an affect on the sentiment to determine if the person has negative or positive sentiment about the programming language being looked at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1556277-7998-4c10-b6ae-fddb4db32e4c",
   "metadata": {},
   "source": [
    "### Filter Corpus of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8278a11-1036-4a6b-a680-037c9bdf5db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def remove_stopwords(corpus):\n",
    "    corpus = corpus.lower()\n",
    "    \n",
    "    # Remove certain punctuation using regex. This will stop 'said' and 'said.' appearing as different words\n",
    "    # Do not remove apostrophes\n",
    "    corpus = re.sub(r'\\.|\\>|\\,|\\\"|\\\"|\\\"|\\—|\\–|\\-|\\?|\\!|\\:|\\;|\\(|\\)', '', corpus)\n",
    "    \n",
    "    # Tokenize the corpus - this creates a list of all the words in the corpus \n",
    "    tokens = word_tokenize(corpus)\n",
    "    \n",
    "    # As well as stopwords, we want to remove tokens with only punctuation, or suffixes separated from words\n",
    "    punct = [\"'\", \"''\", '``', '(', ')', '%', '&', '...', '…', \"‘\", \"’\"]\n",
    "    suffixes = [\"'s\", \"n't\", \"'ve\", \"'ll\", \"'re\", \"'d\"]\n",
    "    remove_words = punct + suffixes + custom_stopwords + stopwords.words('english')\n",
    "    \n",
    "    # Create a new list with stop words removed from the corpus\n",
    "    filtered_tokens = [word for word in tokens if not word in remove_words]\n",
    " \n",
    "    # Reconstruct corpus as a string\n",
    "    filtered_corpus = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92292ba3-d471-4b49-8713-cd58c8e45cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data['filtered_comments'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c12b8-038b-43c6-a4c2-1a6c1d0f6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, len(scraped_data)):\n",
    "    scraped_data.loc[n, 'filtered_comments'] = remove_stopwords(scraped_data[\"Comments\"][n])\n",
    "\n",
    "# Show that we have the filtered values\n",
    "scraped_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96e869-9648-48d2-97d8-b5cb5f9b4d49",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33db8fd-a968-4e1b-8eeb-245ac11cfef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_corpus(corpus):\n",
    "    tokens = word_tokenize(corpus)\n",
    "\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45ad7e-ea58-462b-9e14-cd5abf417648",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data['lemmatized_comments'] = ''\n",
    "\n",
    "for n in range(0, len(scraped_data)):\n",
    "    scraped_data.loc[n, 'lemmatized_comments'] = remove_stopwords(scraped_data[\"Comments\"][n])\n",
    "\n",
    "# Show that we have the lemmatized values\n",
    "scraped_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e14bbf-e2ed-46b3-add1-e6ddaebf4565",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b06ca-d4e5-4290-a733-3c3b9341d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download VADER (Valence Aware Dictionary and sEntiment Reasoner) lexicon, a sentiment analysis tool\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Create an instance of the nltk sentiment analyser\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f430daa-c10e-4826-8b59-e917cb30b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data['neg_sentiment'] = ''\n",
    "scraped_data['neu_sentiment'] = ''\n",
    "scraped_data['pos_sentiment'] = ''\n",
    "scraped_data['comp_sentiment'] = ''\n",
    "\n",
    "for n in range(0, len(scraped_data)):\n",
    "    sen_score = analyser.polarity_scores(scraped_data[\"Comments\"][n])\n",
    "    scraped_data.loc[n, 'neg_sentiment'] = sen_score['neg']\n",
    "    scraped_data.loc[n, 'neu_sentiment'] = sen_score['neu']\n",
    "    scraped_data.loc[n, 'pos_sentiment'] = sen_score['pos']\n",
    "    scraped_data.loc[n, 'comp_sentiment'] = sen_score['compound']\n",
    "\n",
    "# Show that we have the filtered values\n",
    "scraped_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea9095-1828-4c28-b8e0-3022bec94aef",
   "metadata": {},
   "source": [
    "### Exploritory Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca24b1-95e2-4c2d-aa8a-e53f8d6a734f",
   "metadata": {},
   "source": [
    "Images used when creating wordclouds for the programming languages being discussed:\n",
    " - C++ logo\n",
    " - C# logo\n",
    " - JavaScript logo\n",
    " - Kotlin logo\n",
    " - Rust logo\n",
    " - Powershell logo\n",
    " - Python logo\n",
    " - TypeScript logo\n",
    "\n",
    "On the note of using the logo, the logo will be the backdrop of the wordcloud's colour, however the masking isn't being used for this particular instance. If there is a failure, we use the C++ logo for the backdrop for the wordcloud, since the logo is mostly gray, which provides a neutral black wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4d00c-91ad-4b65-8364-c8554eda42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_language_name_to_file(name: str):\n",
    "    match name.lower():\n",
    "        case \"c#\" : return f\"csharp-logo.png\"\n",
    "        case _ : return f\"{name}-logo.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aaa1e7-74f2-4b60-aee8-e9c6a58c27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,10, figsize=(12, 3))\n",
    "\n",
    "# # A function to create a wordcloud given a corpus and an image to use a mask\n",
    "def create_word_cloud(corpus, maskimage, size):\n",
    "    mask = np.array(Image.open(maskimage))\n",
    "\n",
    "    word_cloud = WordCloud(background_color=\"white\", mask=mask).generate(corpus)\n",
    "    return word_cloud\n",
    "\n",
    "def generate_colour(maskimage):\n",
    "    mask = np.array(Image.open(maskimage))\n",
    "    \n",
    "    return ImageColorGenerator(mask)\n",
    "\n",
    "# Going over the  \n",
    "for n in range(0, len(scraped_data)):\n",
    "    try:\n",
    "        image_name = map_language_name_to_file(scraped_data[\"Programming_language\"][n].lower())\n",
    "    \n",
    "        colours = generate_colour(f'./language-logos/{image_name}')\n",
    "        \n",
    "        axes[n].imshow(create_word_cloud(scraped_data[\"filtered_comments\"][n], f'./language-logos/{image_name}', [20,20])\n",
    "                       .recolor(color_func=colours), \n",
    "                       interpolation='bilinear')\n",
    "        axes[n].axis('off')\n",
    "    except:\n",
    "        colours = generate_colour(f'./language-logos/cpp-logo.png')\n",
    "        \n",
    "        axes[n].imshow(create_word_cloud(scraped_data[\"filtered_comments\"][n], f'./language-logos/cpp-logo.png', [20,20])\n",
    "                       .recolor(color_func=colours), \n",
    "                       interpolation='bilinear')\n",
    "        axes[n].axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4deb84b-29a1-4584-ba2c-bb66c4ef25ec",
   "metadata": {},
   "source": [
    "Create wordclouds for all the languages that are being analysed to show the most prominent words used when discussing the programming language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe55aa07-807a-4a73-8c57-e6d2a51f85c3",
   "metadata": {},
   "source": [
    "### Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cad10b-ceee-45a2-a4a5-f9dc8fdaa363",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_palette = sns.blend_palette([\n",
    "    \"#000080\",  # Navy Blue\n",
    "    \"#C8A2C8\",  # Lilac\n",
    "    \"#FF7F50\",  # Coral\n",
    "    \"#006F00\",  # Dark Green\n",
    "    \"#FFE900\",  # Bright Yellow\n",
    "    \"#E30022\",  # Cadmium Red\n",
    "    \"#29AB87\",  # Jungle Green\n",
    "    \"#E0B0FF\",  # Mauve\n",
    "    \"#8A2BE2\",  # Blue Violet\n",
    "    \"#5D2F27\"   # Chestnut Brown\n",
    "], 10)\n",
    "\n",
    "pub_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab46aa-92fd-487d-b90c-8be08868a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gcf().set_size_inches(10, 5)\n",
    "\n",
    "ax = sns.barplot(x='Programming_language', y='comments_length', hue='Programming_language', data=scraped_data, palette=pub_palette)\n",
    "\n",
    "ax.set(xlabel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037fd5c2-75e4-4f54-8c0d-34c5c91e1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = pd.melt(scraped_data, id_vars =['Programming_language'], value_vars=['neg_sentiment','neu_sentiment','pos_sentiment'])\n",
    "\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29115d1c-aaf7-47ff-a703-2a37c53c2537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot for negative, neutral and positive sentiment scores\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "\n",
    "ax = sns.barplot(x='variable', y='value', data=sentiments, hue='Programming_language',\n",
    "                 palette=pub_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b774b-f14d-4bc7-b5fe-3416bf39d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sentiment to show what is more represented\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "\n",
    "order = sentiments.groupby('variable')['value'].mean().sort_values().index\n",
    "\n",
    "# Create the sorted barplot\n",
    "ax = sns.barplot(\n",
    "    x='variable', \n",
    "    y='value', \n",
    "    data=sentiments,\n",
    "    hue='Programming_language',\n",
    "    palette=pub_palette,\n",
    "    order=order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051e65a-d4c0-486d-a035-f8aabeb5c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = pd.melt(scraped_data, id_vars =['Programming_language'], value_vars=['neg_sentiment'])\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "\n",
    "order = sentiments.sort_values('value')\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x='variable', \n",
    "    y='value', \n",
    "    data=order,\n",
    "    hue='Programming_language',\n",
    "    palette='rocket',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39635c3-9514-421b-bbd6-d2c686e31b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = pd.melt(scraped_data, id_vars =['Programming_language'], value_vars=['pos_sentiment'])\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "\n",
    "order = sentiments.sort_values('value')\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x='variable', \n",
    "    y='value', \n",
    "    data=order,\n",
    "    hue='Programming_language',\n",
    "    palette='rocket',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efc1c7-84f6-4c83-8da0-660d9f32fe6d",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a424a10c-c801-45e8-9cf4-23aa9cd796b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(columns=['Programming_language', 'comment_list'])\n",
    "\n",
    "print(language_files)\n",
    "\n",
    "for idx, file_name in enumerate(language_files):\n",
    "    with open(f\"./languages/{file_name}\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # Remove newline characters\n",
    "        lines = [line.strip() for line in lines]\n",
    "\n",
    "        programming_language = file_name.replace('.txt', '')\n",
    "        tfidf.loc[idx] = {\n",
    "            \"Programming_language\" : programming_language,\n",
    "            \"comment_list\" : [remove_stopwords(line) for line in lines]\n",
    "        }\n",
    "\n",
    "len(tfidf['comment_list'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efca115-dd17-4f49-a51b-f8f1117b24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(tfidf['comment_list'][0])\n",
    "\n",
    "# Using PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c='blue', edgecolor='k', s=50)\n",
    "plt.title('PCA of TF-IDF Matrix')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ced76d-64d7-497e-b36e-a7f1b6e2d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TF-IDF scores to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), \n",
    "                       columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Calculate mean TF-IDF scores\n",
    "mean_tfidf = tfidf_df.mean().sort_values(ascending=False)\n",
    "\n",
    "# Plot top terms\n",
    "plt.figure(figsize=(12, 6))\n",
    "mean_tfidf.head(10).plot(kind='bar')\n",
    "plt.title('Top 10 Terms by TF-IDF Score')\n",
    "plt.xlabel('Terms')\n",
    "plt.ylabel('TF-IDF Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd9b5e-f261-4674-94bb-b4095a1cb417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create similarity matrix\n",
    "similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix, \n",
    "            annot=True, \n",
    "            cmap='YlOrRd')\n",
    "plt.title('Document Similarity Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34764e9-07c7-4522-a546-d05b9e00f01a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
