{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692e25c2-5048-4816-b5a8-8c0dc496a8e0",
   "metadata": {},
   "source": [
    "## Initial Import of my Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a4f47a-f862-4cc5-b4b9-31fa299083b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OS lib\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Prepare for OCR to get information from the initial images\n",
    "import easyocr\n",
    "\n",
    "# Prepare for crawling\n",
    "from googlesearch import search\n",
    "\n",
    "# Prepare language data\n",
    "import csv\n",
    "\n",
    "# Webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Analysis\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from PIL import Image\n",
    "\n",
    "# Show all matplotlib graphs inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set all graphs to a seaborn style with a grey background grid which makes reading graphs easier\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34558a-b3e2-446f-95be-06b705506dd9",
   "metadata": {},
   "source": [
    "## Collection of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6a265e0-ae33-4690-9885-e1860dea5cad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "languages = {}\n",
    "async_tools = {}\n",
    "ai_tools = {}\n",
    "cloud_tools = {}\n",
    "development_environment = {}\n",
    "frameworks = {}\n",
    "web_frameworks = {}\n",
    "sync_tools = {}\n",
    "tools = {}\n",
    "\n",
    "with open(\"languages.csv\", \"r\") as file:\n",
    "    language_dict = csv.DictReader(file)\n",
    "\n",
    "    for row in language_dict:\n",
    "        if row[\"type\"] == \"pl\":\n",
    "            key_id = row[\"pldb_id\"]\n",
    "            languages[key_id] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99157b1d-7654-4b34-a335-f7e25ecaad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('./images/') \n",
    "         if os.path.isfile(os.path.join('./images/', f))]\n",
    "\n",
    "# Show list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c3336c-68d3-4a47-8bff-6351e3ecc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnesesarry_info(text:str):\n",
    "        if '%' in text:\n",
    "            return None\n",
    "            \n",
    "        match text:\n",
    "            case 'Source: surveystackoverflow.co/2024' : return None\n",
    "            case 'Source: survey stackoverflow.co/2024' : return None\n",
    "            case text if 'Most' in text: return None\n",
    "            case text if 'popular' in text : return None\n",
    "            case text if 'Respondents' in text : return None\n",
    "            case 'Web frameworks and technologies': return None\n",
    "            case 'Developer': return None\n",
    "            case 'Survey' : return None\n",
    "            case 'Integrated development environment' : return None\n",
    "            case 'Data licensed under Open Database License (ODbL)' : return None\n",
    "            case '2024' : return None\n",
    "                \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482e6f6d-f4dd-46ab-8e7f-74b0d0f3b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_languages = []\n",
    "\n",
    "def process_languages(text):\n",
    "    if text is None or'%' in text or len(text.split()) > 1:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'JS' : temp_list_of_languages.append('JavaScript')\n",
    "        case 'C+-': temp_list_of_languages.append('Cpp')\n",
    "        case 'PY' : temp_list_of_languages.append('Python')\n",
    "        case 'TS' : temp_list_of_languages.append('TypeScript')\n",
    "        case 'C#' : temp_list_of_languages.append('CSharp')\n",
    "        case '2024' : return\n",
    "        case _ : temp_list_of_languages.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d0e4c00-b23d-4228-8493-ae5a77ad0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_web_frameworks = []\n",
    "\n",
    "def process_web_frameworks(text):\n",
    "    if text is None or'%' in text:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'Node:js' : temp_list_of_web_frameworks.append('Node.js')\n",
    "        case 'Express' : temp_list_of_web_frameworks.append('Express.js')\n",
    "        case 'Next js': temp_list_of_web_frameworks.append('Next.js')\n",
    "        case 'Vuejs': temp_list_of_web_frameworks.append('Vue.js')\n",
    "        case 'Flask': temp_list_of_web_frameworks.append('Flask python')\n",
    "        case '2024' : return\n",
    "        case _ : temp_list_of_web_frameworks.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003e2a0f-aceb-44be-9eba-e6760f9d9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_ides = []\n",
    "\n",
    "def process_ides(text):\n",
    "    if text is None or'%' in text:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'Notepad+ +' : temp_list_of_ides.append('Notepad++')\n",
    "        case 'Jupyter NotebooklJupyterLab' : temp_list_of_ides.append('JupyterLab')\n",
    "        case _ : temp_list_of_ides.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b73de9d5-7332-48cf-b81e-70cf54353c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list_of_tools = []\n",
    "\n",
    "def process_tools(text):\n",
    "    if text is None or'%' in text:\n",
    "        return\n",
    "\n",
    "    match text:\n",
    "        case 'Other tools' : return\n",
    "        case _ : temp_list_of_tools.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c214ea3d-c5c4-4ff8-98f0-0c63e713ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = easyocr.Reader([\"en\"])\n",
    "\n",
    "files = [f for f in os.listdir('./images/') \n",
    "         if os.path.isfile(os.path.join('./images/', f))]\n",
    "\n",
    "for file in files:\n",
    "    result = reader.readtext(\"./images/\" + file)\n",
    "    \n",
    "    # Fix data given from the image - only extract the programming languages\n",
    "    for (bbox, text, prob) in result:\n",
    "        match file:\n",
    "            case \"popular-languages.png\": process_languages(remove_unnesesarry_info(text))\n",
    "            case \"popular-web-framework.png\": process_web_frameworks(remove_unnesesarry_info(text))\n",
    "            case \"popular-development-environment.png\": process_ides(remove_unnesesarry_info(text))\n",
    "            case \"popular-tools.png\": process_tools(remove_unnesesarry_info(text))\n",
    "\n",
    "# Clear memory\n",
    "del reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "921278b2-cf19-429b-ac8d-685e1606e8f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JavaScript', 'Python', 'TypeScript', 'Java', 'CSharp', 'Cpp', 'PHP', 'PowerShell', 'Rust', 'Kotlin']\n",
      "['Node.js', 'React', 'jQuery', 'Next.js', 'Express.js', 'Angular', 'ASPNET CORE', 'Vue.js', 'ASPNET', 'Flask python']\n",
      "['Visual Studio Code', 'Visual Studio', 'Intellij IDEA', 'Notepad++', 'Vim', 'Android Studio', 'PyCharm', 'JupyterLab', 'Neovim', 'Sublime Text']\n",
      "['Docker', 'npm', 'Pip', 'Homebrew', 'Make', 'Vite', 'Kubernetes', 'Yarn', 'Webpack', 'NuGet']\n"
     ]
    }
   ],
   "source": [
    "# Get the top 10 languages - this excludes things like SQL\n",
    "languages_for_analysis = [lang for lang in temp_list_of_languages if lang.lower() in languages][:10]\n",
    "web_frameworks_for_analysis = [w_frame for w_frame in temp_list_of_web_frameworks][:10]\n",
    "ides_for_analysis = [ide for ide in temp_list_of_ides][:10]\n",
    "tools_for_analysis = [tool for tool in temp_list_of_tools][:10]\n",
    "\n",
    "print(languages_for_analysis)\n",
    "print(web_frameworks_for_analysis)\n",
    "print(ides_for_analysis)\n",
    "print(tools_for_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdca7049-bec0-44f8-8942-c426b4315ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit_data(url: str):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for comments to load\n",
    "    try:\n",
    "        comments = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[id=\"-post-rtjson-content\"]')))\n",
    "    \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        paragraphs = soup.select('div[id=\"-post-rtjson-content\"] p')\n",
    "    \n",
    "        texts = [p.get_text(strip=True) for p in paragraphs]\n",
    "\n",
    "        driver.close()\n",
    "    \n",
    "        return texts\n",
    "    except:\n",
    "        driver.close()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3b129f-ab27-4bc9-b3d2-fb98ed7214bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_hackernews_data(url: str):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for comments to load\n",
    "    try:\n",
    "        \n",
    "        comments = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.comment div.commtext.c00')))\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        paragraphs = soup.select('div.comment div.commtext.c00')     \n",
    "        \n",
    "        texts = [p.get_text(strip=True) for p in paragraphs]\n",
    "\n",
    "        driver.close()\n",
    "    \n",
    "        return texts\n",
    "    except:\n",
    "        driver.close()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b398fc-032f-4766-a480-ead9d27e2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search\n",
    "# Reddit\n",
    "# Hacknews i.e. Hackernews\n",
    "\n",
    "languages = {}\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for lang in languages_for_analysis:\n",
    "    # # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + lang + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in languages:\n",
    "            languages[lang] = data\n",
    "        else:\n",
    "            languages[lang] += data\n",
    "\n",
    "    query =  \"Opinion on \" + lang + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in languages:\n",
    "            languages[lang] = data\n",
    "        else:\n",
    "            languages[lang] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7091f-ce4a-4390-ab13-163e2c4e3839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70142863-5e39-4be4-bdea-3594af531d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information for the web frameworks\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for w_frame in web_frameworks_for_analysis:\n",
    "    # # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + w_frame + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in web_frameworks:\n",
    "            web_frameworks[w_frame] = data\n",
    "        else:\n",
    "            web_frameworks[w_frame] += data\n",
    "\n",
    "    query =  \"Opinion on \" + w_frame + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in web_frameworks:\n",
    "            web_frameworks[w_frame] = data\n",
    "        else:\n",
    "            web_frameworks[w_frame] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ee9aa05-b5b3-4696-a109-332158047a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information for the integrated development environment\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for ide in ides_for_analysis:\n",
    "    # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + ide + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in development_environment:\n",
    "            development_environment[ide] = data\n",
    "        else:\n",
    "            development_environment[ide] += data\n",
    "\n",
    "    query =  \"Opinion on \" + ide + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in development_environment:\n",
    "            development_environment[ide] = data\n",
    "        else:\n",
    "            development_environment[ide] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22b010a9-758d-4b49-a081-5c1d7eacfa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the information for the integrated development environment\n",
    "\n",
    "# Get a wide range of opinions from developers\n",
    "for dev in temp_list_of_tools:\n",
    "    # Looking for developer sentiment on the given technology\n",
    "    query =  \"Opinion on \" + dev + \" :site reddit\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_reddit_data(result)\n",
    "\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in tools:\n",
    "            tools[dev] = data\n",
    "        else:\n",
    "            tools[dev] += data\n",
    "\n",
    "    query =  \"Opinion on \" + dev + \" :site hacknews\"\n",
    "    search_results = search(query, num_results=5)\n",
    "\n",
    "    for result in search_results:\n",
    "        data = scrape_hackernews_data(result)\n",
    "        # Skip the data that can't be extracted\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if lang not in tools:\n",
    "            tools[dev] = data\n",
    "        else:\n",
    "            tools[dev] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf2c3036-01ea-44e8-b542-30bc5d9da324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the text corpus to txt files\n",
    "# Languages\n",
    "for key, value in languages.items():\n",
    "    try:\n",
    "        with open(f\"./languages/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue\n",
    "\n",
    "# Development environment\n",
    "for key, value in development_environment.items():\n",
    "    try:\n",
    "        with open(f\"./ide/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue\n",
    "\n",
    "# Development tools\n",
    "for key, value in tools.items():\n",
    "    try:\n",
    "        with open(f\"./tools/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue\n",
    "\n",
    "# Web Frameworks\n",
    "for key, value in web_frameworks.items():\n",
    "    try:\n",
    "        with open(f\"./web-framework/{key}.txt\", 'w') as file:\n",
    "            for paragraph in value:\n",
    "                file.write(paragraph + \"\\n\")\n",
    "    except:\n",
    "        print(\"Failed to save \" + key + \" file\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c9b34b-627d-4f34-a4e9-6b27e3094ceb",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50d1d82-fb0b-4075-9772-f530a21a6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_files = [f for f in os.listdir('./languages/') \n",
    "         if os.path.isfile(os.path.join('./languages/', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e393c078-aeb8-4d1a-926c-1b3bf808a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = pd.DataFrame(columns=['Programming_language', 'Comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfda9641-de9c-4d15-b4cc-060fd2293bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Programming_language</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Java</td>\n",
       "      <td>but java also is catching up with modern featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Programming_language                                           Comments\n",
       "count                    10                                                 10\n",
       "unique                   10                                                 10\n",
       "top                    Java  but java also is catching up with modern featu...\n",
       "freq                      1                                                  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, file_name in enumerate(language_files):\n",
    "    file_text = \"\"\n",
    "    with open(f\"./languages/{file_name}\", 'r') as file:\n",
    "        file_text = file.read()\n",
    "    \n",
    "    programming_language = file_name.replace('.txt', '')\n",
    "    scraped_data.loc[idx] = {\n",
    "        \"Programming_language\" : programming_language,\n",
    "        \"Comments\" : file_text\n",
    "    }\n",
    "\n",
    "scraped_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b26eb4-af92-4ba6-ad42-f62823ae9431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java has a total corpus of 66859 words\n",
      "Rust has a total corpus of 91473 words\n",
      "JavaScript has a total corpus of 57585 words\n",
      "PowerShell has a total corpus of 22634 words\n",
      "Python has a total corpus of 66986 words\n",
      "Cpp has a total corpus of 102126 words\n",
      "PHP has a total corpus of 94804 words\n",
      "TypeScript has a total corpus of 68149 words\n",
      "Kotlin has a total corpus of 32613 words\n",
      "CSharp has a total corpus of 42993 words\n"
     ]
    }
   ],
   "source": [
    "# Looking at collected corpura\n",
    "for n in range(0, len(scraped_data)):\n",
    "    msg = \"\"\"{Programming_language} has a total corpus of {total} words\"\"\".format(\n",
    "        Programming_language = scraped_data[\"Programming_language\"][n],\n",
    "        total = len(scraped_data[\"Comments\"][n].split(' '))\n",
    "    )\n",
    "\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa62ae-da57-4a95-8d37-c56f4ec569ed",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9804cc23-88e8-41b9-9107-2a80836286d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jacquesthurling/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def get_freq(corpus):\n",
    "    corpus = re.sub(r'\\.|\\>|\\,|\\\"|\\\"|\\\"|\\—|\\–|\\-|\\?|\\!|\\:|\\;|\\(|\\)', '', corpus)\n",
    "\n",
    "    corpus_lst = corpus.lower().split()\n",
    "\n",
    "    unique_words_full = set(corpus_lst)\n",
    "\n",
    "    unique_words = [word for word in unique_words_full if not word in stopwords.words('english')]\n",
    "\n",
    "    word_freqs = pd.DataFrame(columns=['Word', 'Frequency'])\n",
    "\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        word_freq_pair = {'Word':word, 'Frequency':corpus_lst.count(word)}\n",
    "        word_freqs.loc[idx] = word_freq_pair\n",
    "\n",
    "    return word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b72b8-868d-4bd7-a9c4-9bbab0587598",
   "metadata": {},
   "source": [
    "We can use the above function to get the word frequencies of each corpus and assess if we would like to treat any of the most frequent words as stop words\n",
    "\n",
    "(taken from the example project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e84e0de-efbd-4f6c-8644-90ed8c370144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java: Unique words 7757\n",
      "Rust: Unique words 9788\n",
      "JavaScript: Unique words 7418\n",
      "PowerShell: Unique words 3976\n",
      "Python: Unique words 7658\n",
      "Cpp: Unique words 10310\n",
      "PHP: Unique words 9627\n",
      "TypeScript: Unique words 7326\n",
      "Kotlin: Unique words 4527\n",
      "CSharp: Unique words 5868\n"
     ]
    }
   ],
   "source": [
    "language_frames = []\n",
    "\n",
    "for n in range(0, len(scraped_data)):\n",
    "    programming = scraped_data[\"Programming_language\"][n]\n",
    "    comments = scraped_data[\"Comments\"][n]\n",
    "\n",
    "    comment_word_freq = get_freq(comments)\n",
    "\n",
    "    print(f\"{programming}: Unique words {comment_word_freq.index.max()+1}\")\n",
    "    language_frames.append(comment_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496687db-ca14-4768-9795-6198f8567fe4",
   "metadata": {},
   "source": [
    "As we can see the amount of unique words is quite high for the different programming languages, this would correlate quite nicely, since we are looking at differing opinions from different platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62ae1ebc-a3be-417e-9f0d-ba8599532dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "\n",
    "# Get the common top words for all programming languages\n",
    "for n in range(0, len(language_frames)):\n",
    "    top_words += list((language_frames[n].sort_values('Frequency', ascending=False).head(300))['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b91e1d-485b-4bbe-946b-a246d4defc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae28af0-10ad-4ca4-9821-3b5bb388080f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e965122-70d4-4c43-a5f2-af455427c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b2a0f7-d5e1-4757-8a05-c5c450a2f9cf",
   "metadata": {},
   "source": [
    "I didn't create custom stopwords, since I am interested specifically about the person commenting, I will not remove the first-person singular pronous, as this will have an affect on the sentiment to determine if the person has negative or positive sentiment about the programming language being looked at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1556277-7998-4c10-b6ae-fddb4db32e4c",
   "metadata": {},
   "source": [
    "### Filter Corpus of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8278a11-1036-4a6b-a680-037c9bdf5db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jacquesthurling/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jacquesthurling/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def remove_stopwords(corpus):\n",
    "    corpus = corpus.lower()\n",
    "    \n",
    "    # Remove certain punctuation using regex. This will stop 'said' and 'said.' appearing as different words\n",
    "    # Do not remove apostrophes\n",
    "    corpus = re.sub(r'\\.|\\>|\\,|\\\"|\\\"|\\\"|\\—|\\–|\\-|\\?|\\!|\\:|\\;|\\(|\\)', '', corpus)\n",
    "    \n",
    "    # Tokenize the corpus - this creates a list of all the words in the corpus \n",
    "    tokens = word_tokenize(corpus)\n",
    "    \n",
    "    # As well as stopwords, we want to remove tokens with only punctuation, or suffixes separated from words\n",
    "    punct = [\"'\", \"''\", '``', '(', ')', '%', '&', '...', '…', \"‘\", \"’\"]\n",
    "    suffixes = [\"'s\", \"n't\", \"'ve\", \"'ll\", \"'re\", \"'d\"]\n",
    "    remove_words = punct + suffixes + custom_stopwords + stopwords.words('english')\n",
    "    \n",
    "    # Create a new list with stop words removed from the corpus\n",
    "    filtered_tokens = [word for word in tokens if not word in remove_words]\n",
    " \n",
    "    # Reconstruct corpus as a string\n",
    "    filtered_corpus = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92292ba3-d471-4b49-8713-cd58c8e45cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data['filtered_comments'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f6c12b8-038b-43c6-a4c2-1a6c1d0f6d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Programming_language</th>\n",
       "      <th>Comments</th>\n",
       "      <th>filtered_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Java</td>\n",
       "      <td>but java also is catching up with modern featu...</td>\n",
       "      <td>java also catching modern features really refo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Programming_language  \\\n",
       "count                    10   \n",
       "unique                   10   \n",
       "top                    Java   \n",
       "freq                      1   \n",
       "\n",
       "                                                 Comments  \\\n",
       "count                                                  10   \n",
       "unique                                                 10   \n",
       "top     but java also is catching up with modern featu...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                        filtered_comments  \n",
       "count                                                  10  \n",
       "unique                                                 10  \n",
       "top     java also catching modern features really refo...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for n in range(0, len(scraped_data)):\n",
    "    scraped_data.loc[n, 'filtered_comments'] = remove_stopwords(scraped_data[\"Comments\"][n])\n",
    "\n",
    "# Show that we have the filtered values\n",
    "scraped_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96e869-9648-48d2-97d8-b5cb5f9b4d49",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c33db8fd-a968-4e1b-8eeb-245ac11cfef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_corpus(corpus):\n",
    "    tokens = word_tokenize(corpus)\n",
    "\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d45ad7e-ea58-462b-9e14-cd5abf417648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Programming_language</th>\n",
       "      <th>Comments</th>\n",
       "      <th>filtered_comments</th>\n",
       "      <th>lemmatized_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Java</td>\n",
       "      <td>but java also is catching up with modern featu...</td>\n",
       "      <td>java also catching modern features really refo...</td>\n",
       "      <td>java also catching modern features really refo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Programming_language  \\\n",
       "count                    10   \n",
       "unique                   10   \n",
       "top                    Java   \n",
       "freq                      1   \n",
       "\n",
       "                                                 Comments  \\\n",
       "count                                                  10   \n",
       "unique                                                 10   \n",
       "top     but java also is catching up with modern featu...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                        filtered_comments  \\\n",
       "count                                                  10   \n",
       "unique                                                 10   \n",
       "top     java also catching modern features really refo...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                      lemmatized_comments  \n",
       "count                                                  10  \n",
       "unique                                                 10  \n",
       "top     java also catching modern features really refo...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data['lemmatized_comments'] = ''\n",
    "\n",
    "for n in range(0, len(scraped_data)):\n",
    "    scraped_data.loc[n, 'lemmatized_comments'] = remove_stopwords(scraped_data[\"Comments\"][n])\n",
    "\n",
    "# Show that we have the lemmatized values\n",
    "scraped_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953bf96-6057-4bc1-b555-6fbbcd77fa72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
