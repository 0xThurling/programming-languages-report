Questions and help posts are off-topic for this subreddit. User/cpp_questions.r/cscareerquestions. or StackOverflow instead.
Employers looking forC++developers or individuals looking for work must use the designated "C++ Jobs" thread pinned at the top of the subreddit.
If you want to share a personal project written in C++ use the designated "Show and Tell" post pinned at the top of the subreddit.This doesnotapply to C++ libraries or other projects that would be of interest to general C++ programmers.
Posts about other languages (e.g. Carbon, Rust, Val, etc.) are off-topicunlessthere is substantial content directed at C++ programmers writing C++. That is, either posts about interop from the C++ side, or posts about interesting ideasfromanother language that can be appliedtoC++.
C++ has about 40 years of accumulated cruft and backwards-compat requirements to overcome, which is the main source of the dislike.    Newer languages dont have the same amount of tech debt.
The upside is that if you're learning now and writing new code, you can write really clear code that rivals other languages.
C and C++ are fairly old languages, and a lot of existing systems and libraries are built on them. Many have legacy code from before the internet got real. Once the internet got going security vulnerabilities became a very big problem. C/C++ didn't really account for that when it was originally made so there was and still is a never ending supply of security vulnerabilities. These ultimately got attributed to the language itself because of how freely you can mess with memory.
Most languages in the last couple of decades have been trying to solve two main issues with programming. How to protect memory and how to make it thread-safe. Other languages have made some innovative advances in these areas, but usually at a heavy cost to performance (C/C++ has maintained the performance so people keep going back to it).
Rust's main argument is that is has added those innovations, but maintained the performance.
Most of the ideology people develop is caused by how slow the industry is to switch from one language to another, and running into the problem you have. You can't use a new language if there are no libraries written for it, and no one will write libraries if there are no libraries (So its hard to build momentum).
So I think I can answer the state of affairs pretty comprehensively.
The reason people get fed up with C++ is because it requires a comparatively massive amount of cognitive complexity and manual effort to write correct code due to entirely preventable flaws that have been around for decades.
C++ has spent those decades putting backwards compatibility ahead of developer experience, and people did in fact get fed up with it and created Java, JavaScript, C#, Go, etc.
So it's not quite correct to look at this as C++ being this implacable incumbent. It has lost massive amounts of market share to other languages that exploded in popularity with web applications that traded off maximum theoretical performance to be much safer or easier to use. C++ used to be a high-level general-purpose language that you built regular applications and UIs with (see Win32, MFC) with C or even assembly being the low-level options. It's now perceived as a "low-level systems" language because the number of people in tech also exploded after its heyday, and most people don't remember that era.
Tooling
The vanilla tooling in Rust is stupidly better. Like, there's not even a contest. With Rust, building, packaging, testing, documentation, formatting just work with little to no cognitive overhead. Sure, tools theoreticallyexistin C++, but they require set-up, often require customization, and flat-out aren't anywhere near as good. With cmake, you're learning a seemingly endlessly verbose completely different language just to build your project. With cargo, it's essentially one line to pull in an extra library.
And when it comes to static analysis, a lot of the things you would even lint for or check for with C++ don't even exist as a problem in non-unsafe Rust; so while more sophisticated and mature options may exist for C++, it's much less likely that you'd need to set something like them up in Rust anyway.
Code Confidence
Again, it's stupidly in favor of Rust. An intermediate Rust developer can safely work in a codebase and the compiler will stop them from doing something dumb. Hell, a beginning Rust developer can probably work in a codebase, they'll just be frustratingly slow due to having to constantly recompile stuff in a feedback loop with the compiler until they've internalized what they need to.
Conversely a C++expertcannot write code that is as strict as Rust. It's just not even theoretically possible with the language. Even when C++ has an analogous construct (eg std::optional compared to Rust's Option), in C++ you have to explicitly opt-in to safety, whereas Rust you have to explicitly opt-out. std::optional will happily let you dereference the option and dowhatever(probably blow up the program in an uncontrolled crash) if you forget to check it, whereas Rust will outright refuse to compile and attempt to execute a controlled shutdown even if you bypass that.
In other words, with C++, you have to already know what you're not supposed to do. Thus all the recent safety constructs aren't effective for beginners, who don't know what they're not supposed to do and probably don't know about them, and aren't as useful for experts, who already know best practices.
On top of that, C++ also has a massive swath of the language and past best practices that's been superseded by newer constructs and practices. But since the old stuff got there first, it's easier and more obvious to use. So, once again, the least experienced or most reckless developers are more likely to reach for the worst possible option, while the most experienced or most disciplined people, who are less likely to need it, will reach for the best possible option.
And on top of that, there are also a ton of developers who resist or flat-out refuse to adopt modern constructs of C++, because they have business-critical deadliness and managers more worried about those than whether the C++ community agrees with their internal coding guidelines.
Thus when I see something like Bjarne Stroustrup make an assertion like modern C++ is good enough for anything he cares about, it feels out-of-touch because to what seems like most actual developers, "C++" still means "C++03", and all of those modern constructs he's factoring into his risk assessment, effectively don't exist for most people using the language.
Learning Curve
I've seen people claim that C++ is easier to get started in than Rust. The caveat to this is, someone who's become baseline fluent in Rust is using the equivalent of the safe, modern constructs and only had to learn a subset of what they'd have to learn in C++ before they'd get to those.
Thus, yes, you can initially start coding in C++ faster that Rust, because the compiler is less likely to grab your hand to stop you from doing something that would be dangerous if you start doing multithreading, or try to blow off errors or edge cases. But in a practical setting, you have to learn and acquire the discipline to address a lot of these in C++ anyway for any serious codebase that uses C++. After all, people tend to choose anything but C++ unless they really need performance, and performance (today) entails memory-safety and multithreading.
Compiler messages
C++ compilers dumping a slew of pages of unusably complex template errors is a meme, and it feels like a ton of C++ errors are either useless save for the line number, or are only comprehensible if you can mentally map what the compiler is saying into something completely different that's what's actually happening.
Clang is, I think, better about this than GCC, but Rust is pretty close to best-in-class here if not the best-in-class.
Code Reviews and Conventions
As a consequence of the improved tooling, increased strictness, and having the advantage of learning from past software development, Rust code tends to be a lot more consistent and easier to review than C++.
In C++, it seems like every team has its own "style" or "conventions" guide. This is how many spaces you should put around arithmetic, these are the constructs we're using and not using, and so on and so forth.
In Rust, a lot of this is just done automatically by tooling.
Consequently, Rust reviews tend to be able to focus on higher level concepts like the functionality or code design, and less on spot checking pedantic things that could blow up everything, like whether somebody remembered to lock the right mutex every time they modified a variable, or if they freed a pointer on every codepath an error could occur.
Cohesiveness
Though I think this is getting better in recent editions of C++, there are a number of things in the STL that seem like blatant oversights that trip you up. For instance, std::hash didn't support std::tuple or std::pair. std::to_string only supported basic types. Etc etc.The failures of the STL have also prompted a lot of stopgap replacements; consequently there's a lot of software that now uses some proprietary equivalent to the STL that's technical debt now.
Rust is not immune to this, but its standard library tends to be more consistent about all modules supporting features, and it also tries to mitigate the issue by keeping the standard library smaller and relying on its tooling to let people use popular third-party libraries as a de facto standard rather than adding everything into an increasingly complex standard library.
Thus the issue of cohesiveness might grow slower than C++, since "n" is smaller in the "O(n^2)" problem of determining whether each standard library item works with every other standard library item.
Ecosystem
Here C++ finally has a major advantage. There are far more libraries around for it than Rust. Whereas there are domains that effectively don't exist or only started getting fleshed out in Rust.
That being said, the tooling has a massive effect on the style of the ecosystems. With Rust, libraries tend to be small and do one thing really well. In C++, things tend towards superlibraries (boost, folly, Qt) that are essentially an entire suite of things with an opinionated philosophy specific to that library. The C++ libraries do try to be compatible with the STL, but the dearth of libraries mean its uncommon for them to be able to standardize on some third-party solution to something not in the STL; they simply each replicate that functionality in their own way.
Backwards compatibility
C++ is obviously more compatible with legacy C++ than Rust.Well, at least most of the time. Historically at least, C++ didn't lock down a name mangling standard, so shared libraries compiled with MSVC, GCC, or clang weren't necessarily compatible with each other, which forced projects to restrict themselves to a C API anyway (which is what you can expose with Rust as well).
In any case, C++'s massive ecosystem and need to commit to backwards compatibility makes it harder to solve a lot of these problems than it is with Rust, because there are a lot more stakeholders, and thus a lot more likelihood that someone will be seriously inconvenienced or even face an existential crisis if C++ were to mandate that they address all the technical debt in their codebase to keep pace.
Also, Python tried a breaking upgrade and it was a huge hassle.
Functional Style
This is, in some ways, more stylistic, but it has a practical impact on code confidence.Rust allows and leans in favor of a more functional coding style than C++. It really tries to avoid "side effects" and represent things using types. This can be a different style of thinking (eg an iterator chain vs a for loop) that people are more amenable to, but it also has the effect of encoding a function's behavior into its input and output types, and therefore changing the implementation in a way that changes how the function interacts with things externally is more likely to generate an easy-to-identify explicit error rather than silently corrupting data at runtime.
Rust's iterators are also stupidly better than C++'s, making it far more practical to write things to operate on a stream of items in a modular fashion. Whereas in C++ iterators tend to be so convoluted to use that people usually just don't use them. There are third-party libraries that can do something functionally similar to Rust's iterators, but the syntax isn't so great and figuring out how to use them (with C++'s permissiveness, poor error messages, and lack of consistent documentation) can be a total nightmare.
Performance / Efficiency
The way that Rust is styled, as well as the more modern constructs that are awarded first-class status, tends to mean code relies less on nominally runtime behavior. For instance, there's less need to put something in a shared_ptr as a precautionary measure when the compiler is checking its lifetime for you. Rust also has superior aliasing information.
But there are also areas that involve optimizations that Rust lacks because the language is so new, like stack efficiency. Reading various benchmarks and a research paper or two has given me the impression that idiomatic Rust code tends to fall somewhere between C and C++, but closer to C++, and that odds are the domain-specific algorithms involved are likely to define which is most performant.
Community
The Rust community, at least historically, has been known for being helpful and inclusive. While I can see how you're getting the vibes that you are, both the language and the community have put a lot of effort into making C/++ code interoperate with Rust.
The vibe I've generally gotten has been that Rust developers are more likely to say "C++ is around to stay" and worried about not feeding into stores of the Rust Evangelism Strike Force, whereas C++ developers tend to be more actively deprecating, critical, or frustrated with C++.
Is C++ worse?
The answer, as with many things, is "it depends".
However, to give an answer to the question I think you're asking, Rust has the advantage of decades of hindsight that it leverages effectively and greatly reduced baggage in the form of both an enormously reduced quantity legacy code, and the extant code is leveraging those decades of hindsight to make it easier to upgrade without breaking its correctness. So if the implied question is, "Is Rust designed better?", I'd say the honest answer is yeah. Even C++ celebrities would do it differently if they had to do it today (see: cppfront, or various Bjarne Stroupstrup comments).
But if the question is more along the lines of "which language will end up becoming more pervasive in the long run", I think it depends a lot on how Rust handles the same backwards compatibility issue that C++ has struggled with.
And what happens with AI. If someone develops an approach to reliably transcode and verify code from one language to another, and even fix compiler errors, porting large codebases from C++ to Rust become much more practical than they are now, and would have correctness and possibly performance benefits. However, it also introduces the possibility of upgrading legacy C++ code to modern C++ code to realize some correctness benefits while keeping the language more familiar to the original dev team.
Specific Problem
Because I'm starting a side project that most likely will depend on PcapPlusPlus, since I cannot see any other alternative that even starts to resemble the quality and maturity of this C++ library. I don't see any other logical alternative to this C++ library, yet the hate for C++ from all directions makes me question this choice nevertheless... and frankly it's making me a bit mad, because I just don't see how Rust's value proposition favours general purpose development.
I looked at what the Rust libraries would be for this, and it looks like `pcap` (which probably doesn't support as many backends as PcapPlusPlus) and `pnet_packet` which is a bit inconvenient to have as a separate library.
It seems like I've seen packet analysis done in Python a lot too (since usually it's more of n ad hoc troubleshooting thing, or IO is the largest performance hit).
But to me it looks like your specific use case just doesn't have libraries that are as mature as C++. If you wanted to do multithreaded processing on truly massive amounts of packets, then I think you'd start to see the benefits of iterators, rayon, and compiletime checking against data races.
And if you were looking into providing this as a library that needs to just work for people to include in their projects, then I think you'd see the benefits to the tooling.
Ok here's my 2 cents. c++ is great if you know what you're--
Segmentation fault (core dumped)
I actually just started learning C++ today, I would definitely say its not too complicated and most people over react.
Give it another 6 months
As others have pointed out, C++ is old enough to have plenty of people who've been burnt by C++ in the past. Rust is new enough that no one has built enough software with it yet to hate it.
In my experience, two things have really held back C++ in the past or lead to criticism.
First, C++'s comparitively low parsability makes it hard to build good tools. C++ is a complex language, and the C preprocessor makes code exponentially harder to parse. Where are the right headers? Who knows? Java, by comparison, is remarkably easier to parse and has a standardized intermediate format, so tools like annotation parsers are something a reasonable person can consider. Is it really a fair comparison? Probably not. But it's still a comparison that people make. Tools for C++ just aren't as helpful as for other languages, in part because C++ is just harder for anyone (compilers included) to understand.
Second, the fact that package managers for C++ (apart from OS-level tools) are relatively new is a big impediment. I've encountered too many projects where the owners will tell me "Just use this specific version of Ubuntu or it won't build at all" or where they have incredibly complicated scripts to make sure that they get the exactly right but old version of some dependency. They did all of that because Conan and vcpkg weren't around earlier. This really isn't an indictment of the language, but people still take it out on C++ when package management is practically a first-release requirement for new programming languages.
Bjarne Stroustrup (C++ creator): "There are only two kinds of languages: the ones people complain about and the ones nobody uses."
I would like to see a C+ language, something in between C and C++.
I agree. I use C because it's simpler, and executable sizes tend to be smaller. There's an issue with language complexity and implicit work. I've also found that search and replace works better without classes / method names. There's more context to chew on.
Honestly, the biggest thing they could do is clean up macros, without crippling what's possible. I use macros to implement reflection on C structs. Managing a full implementation for C++ is much harder - templates, private members, multiple inheritance, etc.
I wish C with Classes was C with Proper Strings and Vectors instead. And then just left it at that.
I say be a good programmer in any language. I've coded C++ professionally and it was fine... But that was a while ago.
Last time I looked at some of the newer features of C++11, I don't recall being entirely impressed. The error messages were still massive and sometimes impenetrable. The language is certainly old, and when old languages are forced in modern directions, it's not always a great fit. I feel the same way about Java.
Looking at either a modern language (e.g. Go, Rust, Swift) or an old language that stayed true to its roots (e.g. C, Scheme), everything just feels cleaner to me.
This is another reason I'd like C to stay as old-school as possible. When they add classes to C, we'll know it's jumped the shark. :-)
All my new personal projects that might have been in C++ are now in Rust.
Yeah, the error messages are ridiculous.
They’re like fake movie error messages.
Any relation to beej.us?
edit:
Nevermind, you are.  Imagine a middle-aged programmer reacting like a teenager to meeting a boy-band. Can I have your autograph? :D
It's just a another language. Use it when you need it.
C++ is a near-superset of C, so the question is really, is there anything I find useful in C++? And the answer is yes, there are tons of ideas in C++ that are extremely verbose or impossible to express in C.constexprcomes to mind as something that C is incapable of imitating at the moment.
Morever, when writing most code I do not want to care about memory management, I do not want to care about resizing buffers, I do not want pointers to appear anywhere if I can help it. Modern C++ is all about value semantics and the rule of zero, and these are a huge boon over most C code bases.
Ardent C developers find Modern C++-isms like template metaprogramming (and especially SFINAE) crazy and unreadable, but that is mostly an unwillingness to learn something new. Not to mention these constructs are used chiefly in library development, not application code.
I like C++, I prefer writing most application code in C++ if I can help it, because I can write the same application in a tenth as many lines. That doesn't mean I don't love C, I do. When working in an environment where I need the ABI simplicity (or the implementation simplicity) of C, I don't hesitate to reach for it. But that is the niche C fills.
I think it's best to consider C++ to be an entirely different language with no relation to C.  The way the language is used differs so much from C, it's barely recognisable.  And idiomatic C code has long ceased to be idiomatic C++ code.
Also beware of anybody talking about “C/C++.”  Use of this term usually indicates a lack of proficiency in both C and C++.
The main issue that I have with it, is that C++ compilers don't let you choose which language features to include/exclude. As soon as I switch to a C++ compiler, I have no hard way to control what gets into my codebase. I have to trust code reviews and design documents to ensure that some features don't get used in my codebase. That's just not good enough.
I also hate the culture of the C++ community. My experience with C++ projects and colleagues is that most C++ programmers are quite mediocre. Yet, online forums will jump on you for daring to not being an expert on some obscure C++ feature or syntactical point. And don't you dare claim that you don't like or want to use a given feature, ot they'll send the wolves after you.
I never copied the link to learncpp.com from old reddit to new. I can't remember why, I do remember I didn't want to just use a text block like on old reddit, which was probably the only thing available at the time this community was founded. New reddit, at least, has additional side-bar facilities and I was trying to organize accordingly. I've got a backlog of things I'd like to do, including an FAQ...
Learning C++ from scratch isn't about building stratified layers of foundations - the lines aren't so clear. The myriad of topics you need to teach yourself is a web, a mesh. There is a difference between learning C++ the syntax, C++ the language, C++ the developer, C++ the engineer, and computer science. It takes a certain critical mass before you get moments of insight into the bigger picture, and those insights are what are critical - you're just following along until you get those.
But you have to learn it all eventually, because you're going to see it all eventually. You can never really understand avectororstringuntil you understand classes, but classes are typically considered an advanced topic. Catch-22? Perhaps it would be easier to learn the mathematical underpinnings of C++ first - that classes have their roots in set theory. But we don't teach computer science quite like that, no commonly accessible source does.
I don't have much to say about online resources. What I can say is that comprehensive books seem to publish faster than comprehensive websites. I think it's because there is more focused labor and editing that goes into something as tangible and permanent as a book, that is published and sold, than whatever motivates website publishers. We both expect and demand more of something you're going to fork money over for. If I had to give you advice, it would be both based off my experience, and conservative, because I know it works - as it did for me, and I want to achieve through you the highest chance of success, for what my advice is worth.
But also libraries rent their books out for free. </s>
What do you think about learncpp.com?
I think it teaches things in the wrong order.
E.g. references shouldn't be taught as convenient pointers, they should be taughtfirstand then pointers should be introduced as inconvenient references.
It teachescinbeforestring+getlinewhich is why so many beginners get nonsense problems where their user input didn't do as expected.
It introduces arrays before vectors.
Not that all of those shouldn't be taught, and I totally get the desire to e.g. explain arrays before vectors so you can explain vectors by explaining how it treats arrays.
But it sets your students up for failure.
It's hard to explain the pedagogy of it, but:
Youcanlearn how to use a vector without understanding intricately how a vector works (templates, allocators, RAII, fundamentals of arrays, etc./)
Youcannotlearn how to use dynamic arrays without understanding the heap, pointers, sizeof, etc./ -
So if you start by teaching people arrays, and they're struggling with that, and you then introduce vectors, they're going to take their (wrong, struggling) ideas about arrays and apply them to vectors and also misunderstand how vectors work.
But if you do it the other way around, then, once theydoknow how to use a vector, and are confident writing a program that has a collection of things e.g. names and addresses, you can continue to the under-the-hood how-does-this-actually-work, look-how-much-you-have-to-remember isn't it great how the vector automates it for you?
Does the student risk all kinds of UB if they're using strings and vectors without understanding how char* and arrays work?
for sure
But they're students! They're gonna get UB with char* and arrays too! And more, probably, because there's more you have to understand to correctly use char* and array than there is with string and vector.
I agree. They've made improvements, like moving std::string nearer to the beginning, and listen to and implement feedback from readers, but the structure isn't great for beginners, but if you already know programming, I feel like it's pretty decent, especially since it's free. I don't think resources like this should necessarily be shunned or ignored. I bought into the whole "books only" route over 2 years ago, and it burned me the hell out. 2 years later, I'm quite enjoying myself with this website. The quiz questions aren't too challenging, but still enjoyable. I was overwhelmed With Bjarne's 1000+ page book, with tons of quizzes, and burned myself out, and abandoned C++ completely because I didn't want to use a website because everyone sort of demonizes them.
I've heard opinions like yours before. However, I was taught C first. I personally like starting from the low level. In college, I was taught how to usemalloc/freeto manage a dynamic 2D array. Then hearing aboutstd::vectorthere was no mystery about how the thing works - I've already implemented bits and pieces that are at the heart of its functionality.
Preface: Not a professional software developer.
www.learcpp.comcertainly is the best online tutorial out there and it certainly is better than some books. If you just look at all the contents, it might as well be a book. Just because something isnt availible in printed form doesnt mean its any worse or better.
I would agree with you that the order of some teaching isnt great.
If it were up to me, I would teach usage ofstd::vectorbefore explaining raw arrays. I can see why it is done this way, because pointers bring fundamental understanding, but I dont think the order adds any value. You can understand what astd::vectordoes and use it without knowing about pointers.
I was rather happy when an introduction tostd::stringwas added to the section on fundamental types (before C strings) That is good enough for most cases.
cplusplus.com's tutorial,
Is literally stuck on c++98. I dont think anything else needs to be said about it.
Finally I would still say that the C++ Primer gives you more information not only about C++ but programming than learncpp does. Its also about 5 times as long. Turns out good resources are hard to create and "expensive" as a result.
s learncpp still considered "bad"
I think this sentiment (at least in the online communities I frequent) has changed years ago.
I had to message the moderatos onr/learnprogrammingto get it removed from the list of discouraged resources. It simply was put up there years ago ( like more than 5yrs) and noone ever bothered to check in on it again (as is so often the case with online resources).
Maybe we should put in a "petition" to add it to isocpp's website and remove cplusplus.com in its stead.
Is learncpp still considered "bad", or have people's opinions on it changed?
As far as I'm aware, it's never been considered bad, at least not in any communities I've been part of. It's the single most commonly recommended resource on this subreddit, at least.
but can't find it being recommended by (m)any reputable sources/professional programmers.
I'm a professional software developer who has recommended it to dozens of people looking for quality, free resources. It's certainly not perfect, but it's a lot better than most of what's out there.
I recommend it, it's generally up to date and covers enough topics to give a beginner a good start.
But why choose? It's free. If you were going to get a book you can do that too, no additional charge.
I'm not a fan of the, here is how you do X (next chapter), just joking, here is the correct way to do X, approach. Compared to any Stroustrup's book, learncpp is still pretty shoddy. Personally I would avoid it.
I made a career in high frequency trading and recently switched over to music software, C++ is king in both of those fields with no close second. You can add video games, VFX, CAD software, embedded, etc. to that. C++ isn’t going anywhere anytime soon. But even if it was, technologies come and go but the understanding you gain by learning C++ about how hardware and software interact at the lowest level, will always be useful as a programmer.
But even if it was, technologies come and go but the understanding you gain by learning C++ about how hardware and software interact at the lowest level, will always be useful as a programmer.
This is the most important part to me.
Domain knowledge(systems programming, in this case) is typically more valuable than expertise in a specific language.
Yes.
Once you learn C++ you will conquer every programming landscape, whatever happens in your career.
I see a lot of of people saying its an old language
C++ is 40 years old. It being old shouldn't discourage you. C++ gets updates, has large commercial support and a lot of people are still using; this should tell you it's a very successful language. Compare that to a lot of younger languages that nobody uses anymore.
its very hard, and has omplex syntax
It's a complex language as a whole but not actually that hard for the most part. Most programmers only need the general stuff that already has very similar counterparts in other languages so learning those in either language is often enough to move on to other ones with minor modifications. The really hard parts are specific to C++, but you might never need to touch them in your career. I know people who don't write templates, only use existing ones or written by someone else. And that's fine, not everyone needs to master the tools they are using.
wanted to know if it’s something I should pursue aside from college
Do you want to or need to? Learning C++ is probably a good way to secure a job. I also find it fun to learn its intricate parts.
You wrote this post on a browser written in c++, from an OS ui probably written in c++, reading news about AI using frameworks written in c++, playing games written in c++, and on and on... c++ is not old, is foundation.
If we were in 2010, I would say that yeah, C++ is old and not changing. If we were in 2015, I would say maybe, the language is not getting modern fast enough.
Today in 2024 I can tell you with confidence that the C++ committee is kicking ass in getting C++ as a modern language and they are also planning on making it safer and to create a scripting equivalent.
It’s definitely still worth to learn!
C++20 is much different than C++ in the 1980s. They may be built on the same foundation and they may look similar, but decades of updates have drastically changed the way programmers use C++ both in terms of coding practices and applications. It’s not old, it’s established, and the language continues to be improved upon with every revision.
Yes it’s hard and has complex syntax. It’s not too bad — the fundamentals are similar to C until you start adding all the different optional features. If you want to pursue this then don’t look all the way up the mountain, just focus on the path in front of you. C++ is way too complex for anyone to fully know every last piece of it.
The purpose of C++ is that it runs very quickly (in ideal circumstances with competent engineers) since it’s a compile-time language and it works well with hardware since it’s low-level. You’ll see it forever because it fills a niche for tight timing and low overhead that languages like JS and Python can’t. It’s used more often in fields like embedded, HFT, game engines/game dev, automotive, defense, medical devices, etc.
C++ is still one of the most important language
its an old language [...] My only experience in this field is that I know a bit of Python
Python is 35 years old lol. C++ is going to outlive most languages. Don't worry about the age of a language.
C++ is a very important language, and a great 2nd or 3rd language to learn IMO, but when you're learning something you should start with small simple steps and build up from there. You wouldn't start someone learning to drive in a formula 1 car just because it's the best / fastest car.
I may get downvoted here but C++ isn't great for a beginner because:
it's such a big language: you won't learn 5% of it in a year
there's so many gotchas
the build systems are complex and basically a programming language themselves
the ide / debugging / docs experience is awful compared to modern languages, and you need these most as a new dev.
Normally I recommend C# as a first language as there's really good editor support and error messages. It builds with a simpledotnet buildand it's a strongly typed general purpose programming language like C++. You can see immediate results in your progress.
If you want to specifically learn for a type of job:
Game Development: C#/C++Machine Learning: PythonHigh Frequency Trading: C++Embedded: C/C++Web: JavaScript + nodeHighest employment chance: JavaScript + ReactHighest paid job: Python+PhD in MLEasiest to learn: PythonAnything else: Whatever you enjoy
This is a C++ sub, do you really think you'll get unbiased answers ?
My two cents : I have worked on C++, Java and Python in my decade long career, having spent max time on C++. I'm so glad I started my career working on hardcore C++ since I got to understand how things work close to the hardware. Post that, learning other languages and understanding how to use them effectively became a breeze. Also, it is really helpful since a lot of Python extensions are actually written in C/C++. I really think learning C++ made me a much better engineer and those who work exclusively in Java/Python only don't really have an in depth understanding of internals. You will encounter C/C++ whenever you're peeking under the hood of any low level system (e.g. Linux kernel, ML libraries, Python interpretor or the JVM).
That being said, the reality is that C++ is nowadays used almost exclusively for applications where you need to extract the maximum performance from the hardware - OS kernels, CAD, computer graphics, video games, HPC, systems programming, etc. Everywhere else where speed of development and safety matters more (e.g. enterprise applications), safer languages like Java and Python are used. Python is also very useful for AI/ML applications and general automation. From just a job  perspective, you will probably have more options for Java/Python than for C++. However, generally good companies are language agnostic and it is considered that if you're good in C++ you can pick up any language very easily.
So, all in all, I would totally encourage you to learn C++ since it will make you a better engineer but as a new grad it would help if you had some exposure to Java and Python as well.
I am a bona fide C++ hater, but that's just because I resent working on it for professional code that needs to work reliably with coworkers I don't entirely trust. For personal projects it's one of the most satisfying coding contexts I've ever worked and it is indeed a blast to have such fine-grained control over execution.I abandoned the language about a decade ago and I don't see myself looking back. My projects these days need long-term reliability more than anything and rust + cd/ci hits a sweet spot. That said, I do miss the thrill of designing and executing a program that runs in a certain way exactly as I intended and knowing that it was my expertise and insight that allowed this execution. Would I want to work with someone that was driven by that? Hell no! But it is personally a joy I won't forget.Other inexcusable pain points: the build systems and package management is an absolute nightmare; the pre-processor feels like a sadistic joke; the syntax is horrible; there's so much cruft in the runtime you need years of experience to not machine-gun your foot off by using the most obvious tool at your disposal. But in a sense this just increases the joy of shipping a working executable with all your cleverness and blood and tears wrapped with a bow.
Kudos for one of the most relatable descriptions of C++ I've read.I did a couple years writing C/C++ professionally, and I hope to not go back to that. Too many hours debugging other people's code, suffering vague integration issues, and just trying to get the build system spaghetti to run.
Work should be fun too, not just personal projects. I've noticed that over the last 10 years, the tech world has seemed to lose this idea. I blame this on the large tech companies who seem to take the best people and put them in an adult prison. Granted a prison with good food and soft chairs.Even startups seem to be less fun. It seems the VC world foments a narrow window of thought with a lot of copy cat startups doing the same thing. Fads, lots of fads.I use C++ for every new project because I know if I need to build something myself, I can do it. Using frameworks all the time sucks because frameworks never work the way you want. Especially if it deals with the core of the project.If you can build it yourself, then do it, otherwise use a framework. And if you can build it yourself, C++ is a great tool because it can do everything and doesn't get in your way in all the important ways. Meaning, I can express my mental model directly in C++, where a language like Rust might complain I'm doing an unsafe thing. Sometimes I think in pointers damned and pointers are a great thing!Lets bring fun back! Not just in our personal lives, but work too!
Maybe they are hired to resolve complex issues beyond the FAQ of a framework, because they know how the underpinnings of the framework work. Many frameworks and libraries are built in C++. Without curiosity and sense of adventure, what are you? Another clone of framework x?
If I was ever given a choice then I'd tell you. And I have done "resolving complex issues beyond the FAQ of a framework" many, MANY times.Almost as if country and environment have anything to do with it, more than merit.
Making software is more than coding. It's about both solving and finding problems. In my experience, boring people make uninteresting products and lack imagination to solve the hard ones. Having fun is crucial for creativity and loosening the mind to now what you "should do" but what you "could do".You get paid to solve problems not write software, and having fun is critical to solving problems. If you are bored, stressed, and frustrated,  what makes you think your mind would be in a good place to solve problems?
And you addressed exactly zero existential concerns.Assume I had no choice of the programming jobs I was taking in the last years. Now try to fit your framework of mind in it.Present me the result as a textual reply here?Hint: what I felt and how I felt... nobody cared.
I am blaming the companies, not the workers. It's not your responsibility to make sure you have the environment to do your job well. That's the job of the company. There are a lot of bad companies out there and that's my point. They don't care what you feel and whether you have fun and that's to their detriment and yours.
It might be to their detriment, but I suffered much more than I could detect that they suffered so I am keeping a reserved judgement on that one. But even if you are 100% correct (a real possibility) then it still doesn't matter much; I suffered and I still do.Environment and even geographical area count. A lot. Even if I worked remotely for the last 14 years. Turns out USA companies really don't want people outside of USA, especially recently.
Yes, despite what outsiders think, programming is a tough gig. Even in the US, if you look at wages, they have been stagnant since the dot com bubble collapsed if you account for inflation. People are overworked and as recently shown, job security is not really there. Outside the US it's often worse.I'm sorry you suffered.
I still suffer, to this day. Gets more and more difficult as you age.Hopefully I wisened up and I think I started doing the right steps but it's like blowing against the wind; ultimately you'll succeed but... yeah, nevermind. Don't want to complain. I can muscle through it. Not like I have any other meaningful choice either. It's a victory or death situation (not literally but almost).Thanks for your sympathy. <3Hope I didn't come across as too much of an arse. I am simply way too jaded and burned out.
Why is that baffling?If I couldn't have fun at work (of this exact nature) - I'd quit, I'd be bored, my brain would be constantly itching.These activities are no more a waste of time than reading up on a topic or doing a tutorial or anything that furthers your skillset. A boss that objects to that wouldn't last long with me.
Because you'd be broke and freezing outside.See, you're privileged as hell if this doesn't even cross your mind.
There are options between "freezing to death" and "brainlessly working on other people's ideas while suffering inside". I've personally quit jobs paying more than double of my current salary, because I felt I'm wasting my life on boring, repetitive work just to make someone else rich.
This article seems kinda confused. It makes a lot of points, but I'm struggling to extract why that means C++ is more/less fun.The simultaneous description of modern C++ as "extremely high quality" and yet pervasive legacy bs, horrible tooling, etc is confusing. They say unique_ptr is great yet they "really hate RAII". Many of the discussed topics are largely irrelevant or incorrect. Metaprogramming used to be everywhere but now isn't (what?? Take a look at concepts)? C++ has the best graphics libraries (surely not true)? Installing Python is harder than C++? These seems like odd obsessions of the author, with little relation to the argument.I think the author just finds C++ fun for some complex combination of personal factors, which is fine. Perhaps partly due to a "hacker" type personality. Some of the most fun I've had with programming has been C++, due to its performance and technicality. At the same time, C++ is in practice crap for "real" software development where your own short-term enjoyment is not paramount (for a billion reasons we all know). C++ is a deeply conflicted language, scarred by decades of legacy and politics. Ignoring the reality of C++ is the biggest mistake of those who discuss it. My attitude towards C++ these days is of tiredness. I don't want to jump through the hoops C++ has thrown at me for the past many years. We don't need to beat an already long dead horse. It's ok to let C++ be what it is.
> I think the author just finds C++ fun for some complex combination of personal factors, which is fine. Perhaps partly due to a "hacker" type personality.I think that sums up the point of the article just fine.To me at least, it's not so much that it's confused as that it's him being excited about having fun and doing his best to share his joy. Which means that, sure, it's not a proper essay at all, it's not really trying to convince you to agree so much as saying "if you find this relatable without me trying to actively convince you, maybe you'd find it fun too."I think given you've been mugged by reality in commercial work to the point of tiredness, your not finding it to make sense at all is completely understandable, though.I'm currently having a blast writing javascript of all things, and I imagine if I wrote up a similar blog post explaining why it'd come across just as badly to somebody burned out on the last decade of commercial javascript too.But I'm glad he's having fun, and I'm glad he decided to share the joy.
I just spent 3 days to statically link a third party dependency to my C++ project, in a way that works with linux/windows/macos. The package wasn't available on conan/vcpkg, it was just a github repo with a weird combination of Makefile/cmake file. While I'm far from a cmake/cpp expert, this is a non-issue with most modern languages: you just pip install, cargo add, npm install, go get etc. You can read front-to-cover books about every details of C++ semantics and it'll still be a PITA to work on real world projects
The javascript toolchain for dependencies and build is as arcane as the sloppiest Makefile.It's amazing that we ended up with the javascript equivalente of autotools.
Created an account just to say: I am a 10-year JavaScript veteran who knows the ecosystem at least as well as any sane person could possibly be expected to, and this is 100% correct. When I tell people that JS build tooling is the second-worst headache to C/C++ build tooling, they laugh, but stick a toe outside the happy path of `npm create <some-framework-project> && npm start` and you will know what pain feels like.I did get some similar vibes trying to work with Gradle and Maven for a project a while back, but I don't really live in that world so it's hard to say whether my experience was typical or just symptomatic of my inexperience.
And it'll continue to get worse because JS devs love coming up with their own abstractions over everything and piling complexity. You don't write JS, you write TS. You don't write HTML, you write JSX. You don't write CSS, you write Tailwind. And of course, React has its own compiler as of recently. Now you have to figure out in what order to run these parsers, bundle everything together including your hundreds of dependencies, minimize, obfuscate, tree shake, and whatever else fairy dust magic you want to sprinkle on top. Meanwhile, the default build tool of choice changes about once every 2 years, and now you also have a choice of runtime - node vs bun vs deno. Can't wait to see what the next revolutionary idea would be to contribute to this madness.
You forgot to mention the 3 or 4 different TypeScript compilers!I do have to admit at least some of the pain is self-inflicted on my part. I _want_ those nice abstractions, so I will write a damn Rollup or Babel plugin if it means I can use Sass stylesheets in my TypeScript+JSX components that compile to vanilla Custom Elements (so I don't have to depend on a huge framework runtime).But as nice as the Babel/Rollup/Vite plugin APIs are, when you start doing stuff like that you then have to deal with all the deep-in-the-weeds bullshit that comes with it, like the fact that Webpack and Rollup and Node.js all have subtly different (and mostly poorly documented) module resolution rules and that they all differ wildly from the official ES-module spec that the standards committee finally shipped like 20 years too late, so trying to get your unit-testing framework and your bundle toolchain and your browser to all agree about how to digest the mess you've made becomes this giant clusterfucktastrophe that makes you question every life decision and formative event that led you to the moment you thought this might be a neat idea.But yeah just like build a React app or whatever and you'll probably be fine.
There are those of us out there that have a beautiful experience with JavaScript. Vanilla JS, web components, a lightweight rendering library like lit-html, class props as reactive state...NodeJS + express + vanilla web components is the most graceful and productive stack I've ever used in 30+ years of development.
Good luck trying to use these tools with anything other than the blessed path of node on the backend.The JS ecosystem absolutely has the problem of "too many compilers", and of course as I'm typing this there's someone out there writing a "one compiler to unify them all".
What was particularly difficult about it? Sometimes modifying someone else's code to build statically or on another platform is tricky. Linking is generally just include dir, lib path, and lib though.
The library was duckdb (https://duckdb.org/docs/installation/), and here are some of the issues I ran into:- trying to compile against the prebuilt duckdb_static.so file got me a ton of undefined reference error, I asked about this on their discord and it seems to be a knwon issue, there are additional dependencies that are not part of the release so I had to build it myself- the library uses CMake but it contains a Makefile and they recommend using that to build it- It seems like on windows, if you create a dynamic library you have to add __declspec(dllexport) before each function. DuckDB has a #ifdef DUCKDB_STATIC_BUILD to toggle/disable it which isn't documented anywhere, you have to read the codeI'm sure someone will tell me that this is very standard and shouldn't take me 3 days to figure it out, but with basic knowledge of cmake and build systems that's a bit of a pain, and there's no other language where you have to do that
> trying to compile against the prebuilt duckdb_static.soTypically you statically link against.a object library..so are shared objects intended for dynamical linking only. Does duckdb provide something like duckdb.a ?
That was a typo, the file was duckdb_static.a
Ha, I used duckdb once and ran into a memory corruption issue through normal usage. It was a known issue also surfaced by a fuzzer. One of the devs referred to the fuzzer as the fusser in the issue about it and I stopped using the project immediately. Best of luck with that one!
I stopped using it because it was in a memory unsafe language, had readily triggered crashes, and because a developer seemed to security as a chore.
CMake, the defacto if not standard build system, when correctly setup, does all this for you.Your argument is along the lines of "the maintainer of X rust package didn't setup cargo correctly and it's impossible to include in my project, rust is a horrible language". You can absolutely make an argument that the lack of a standard build system is a pain point, and well it's C++ you get to be a special snowflake and use a makefile instead of CMake but then the onus is on you to provide correct documentation.
In the case of duckdb they provide proper packages for pip, cargo, npm, go get, and the only one that seems like a PITA to use is the C/C++ dependencies, even though the package is written is C++, so I think this tells something about C++ dependency management
Seeing as it's Christmas Eve and I has nothing better to do...Duckdb own documentation says the C++ API is internal and you should use the C API[1]The makefile also has a 'bundle-library' option which seems to be exactly what you were looking for, it generates a statically linked library which is what the Golang package is using, probably others too but that's the one I checked first.This is purely a documentation problem, the build system does what is needed it seems. Create an issue, or better yet, add the necessary documentation...1.https://duckdb.org/docs/api/cpp
I used the C api but I did miss the 'bundle-library' option, will look into that
I definitely don't blame you for missing it.I found it by looking at the go package because they claimed they are statically linking there.
You seem to be agreeing with me then, weird take to do it while sounding like you disagree...As I said, you can make the argument C++ build systems are no good. However the argument originally made is that C++ the language caused these issues when in reality the package maintainer hasn't put in the effort to properly document and fix known issues in their chosen build system for C++.The fact that they in fact have built out proper packages for pip, cargo, npm and go tells me they have the necessary expertise on the team and made an active choice to not do the same for C++. Create an issue with the maintainer.
The argument originally made was "You can read front-to-cover books about every details of C++ semantics and it'll still be a PITA to work on real world projects"Which is not disagreeing with you just because you are explaining/disputing the reason it was a PITA.
That statment was qualified by:> While I'm far from a cmake/cpp expert, this is a non-issue with most modern languages: you just pip install, cargo add, npm install, go get etc.Which I also find fundamentally wrong because most real world projects have a C/C++ dependency which needs to be built and is built from one of those languages build systems by calling into cmake / autotools + make.Just because you called apt-get or any other way to pull dependencies doesn't remove the fact that they need to be built and likely involve a C compiler.
I have seen way more projects with very broken CMake integration than rust packages with broken cargo setup.CMakeisa pain.
> the maintainer of X rust package didn't setup cargo correctly and it's impossible to include in my projectThat basically never happens, which is the point.
The main difference being that Cargo is both for distributing and building locally AND it’s the blessed way of building all Rust projects. This means that as long as you have a copy of the codebase, it’s going to be using Cargo & you can add it with Cargo regardless of it being published formally to crates.io (the copy can even be remote since you can build from a git reference directly). Python packaging requires more work although there’s typically no build step so you can probably just __import__ the path and move on. JS is closer to Rust in that package.json is the standard way to manage components and you can import them even if they’re not published via the npm registry.Pretending the situation isn’t very different for C++ is completely ignoring the standard committee’s complete abdication of standardizing a build system (which by the way if they’re not going to do, why are they bothering to standardize anything?)
> But to flip this around, let's say you want to use this third party in some other "modern" language how would this become better?. Clearly if someone hadn't done the work to package in vcpkg/conan you can expect it wouldn't exist in pip/cargo/npm either. So if you had a bad time in C++, you would have a dramatically worse time in other languages.Err, no?In case of Rust usually there is really no work that's necessary to package it and upload. You just type `cargo publish` and you're done (after filling out metadata in Cargo.toml). Even if someone wouldn't upload it to crates.io as a Rust crate you can just add a single line to your Cargo.toml:package_name = { git = "https://github.com/user/repo", rev = "revision" }and you're done, you can use it as a dependency.
I think the difference is that you can pip install a github url and it will work, it will even work if the upstream has done nothing to package it.I think the issue could be broadly defined as how difficult it is how difficult it is to take someone else's code and use it as part of your project. For C/C++ the answer can be completely trivial to a massive pain in the ass.
Oh I dunno.  With cmake it really is pretty straight forward.  You can pull from VCS, a tarball, even use a git submodule.If it's another Cmake project, that's the happy path.  Easy peasy.  If it's not you still have the ability to run ./configure && make or evoke whatever incantation is in the dependency's requirements.  I'm not the hugest fan of cmake, but I've worked with it quite a bit and it's a much more pleasant story than other languages I've used a lot like python, JavaScript, OCaml (which I love), etc.I do find rust and go dependency management a little simpler due to the prescribed nature of them, but once you get off the happy path, the flexibility of C land is tough to beat.  It sort of matches the language stories themselves;  you have more control and have to do a bit more yourself.  It's all about trade offs.
How often do folks venture off the happy path with Rust dependencies? Personally I’ve never once found myself in that situation.
I've done it before to integrate rust into polyglot build systems. There's a surprisingly long history of build systems trying and failing to implement rust builds without the "happy path" of simply wrapping cargo. As far as I know no one's ever succeeded.
I periodically work on c++ instead of C and each time it follows a similar pattern where I learn about some new c++ trick, think it will make things better, write my code, hit a compiler error, then spend the rest of the day learning why I can't do what I want.  Granted, I usually am stuck at c++14, and some of the issues are fixed in future versions but still...I really want to love C++.  It gives me a more powerful C that theoretically should improve my output but in the end it carries so much cognitive baggage that I usually end up writing "C with classes" and move on.
> then spend the rest of the day learning why I can't do what I wantThere's a point when learning's fun, I think the OP is still there.I wrote a bunch of realtime C++ in 2003, hated it. But last year, I wrote most of my code in C++ and liked it finally.Lambda and auto was the tipping point where I stopped hating it. Some templates thrown in, but mostly to avoid writing a lot of branches for variants of the same function.With lambdas I could now write the way I was initially taught programming, mostly by Lispy teachers channeling SICP.Didn't hate the allocations either with unique_ptr.
If you haven’t already look up ‘if constexpr’ in cpp17 and newer. It lets you have compile time branches in a single function.
Also requires, which initially looks batshit insane but is in fact really quite cool:https://www.think-cell.com/en/career/devblog/if-constexpr-re...
> Unlike requires requires and requires { requires }, which are perfectly reasonable C++ code, requires requires { requires } is completely silly.Normal day in cpp land...
How on earth did C++ brainrot lead them to think that reusing that keyword was anything but a nightmare? The very first line of the article> Probably the two most useful features added to C++20 are requires and requiressaid without a trace of irony ... the whole thing reads like parody!I keep hearing that C++20 or whatever is getting so great but if so why would they specify such a dumpster fire of syntax???
"Lispy teachers channeling SICP" made new audibly laugh
> There's a point when learning's fun, I think the OP is still there.What a horrible idea that one might no longer be there.
You hiring for positions where I can have fun programming?
Never hired anyone. But also never worked in any place where programming was not fun for me. But also never touched any Java or C++ job.
I see a lot of bad code from people who have just learned about some feature and want to use it. Don't do that. Think of how to write your code, and then, if a feature solves a problem, use it. Problem->Solution, not the other way around.E.g. C++ templates are generally pretty awful, but sometimes, compile-time duck typing, or automatically adding padding based on sizeof(), etc, is very useful.
This describes my C++ experience precisely. Initially I was a C# programmer learning C for embedded projects and it went exactly like this.C++ has a lot of really neat features that sure look powerful if your application aligns precisely. It seems likeeverytime I try a new feature, what I want to do is always an edge case that doesn't work in my situation. I try for a few hours/days before I give up and just write it in C.
My biggest disappointment with C++ is that the standard libraries are completely unsuitable for use in embedded systems where you have to control when, where and how many memory allocations occur.  This is particularly important when the system level design choice is to perform all allocations at system startup, which is a common design pattern for high performance systems with real time characteristics.  A high speed messaging system I worked on ran into this all the time.  We couldn't use the standard implementations of things like heaps, hashes or queues because they don't have a way of making memory allocations occur at startup.  It was quite common to have to re-implement those data structures when adding a new feature, as the stl or boost implementations were not suitable for this design pattern.
Have you tried override the allocators and destructors of your classes so you can do all allocations at startup?
C++, like most technology, works better if you put in the work to learn about it.(Admittedly there are languages like python and ruby that buck this trend.)
Sure, but the effort/reward ratio bears some consideration.  I've put a lot more effort into learning C++ than any other programming language but I'd still say it's just the language I'm maybe 4th most proficient in.
> C++, like most technology, works better if you put in the work to learn about it.There's also the impact of software entropy: if someone has little to no experience developing software and has to grow it by adding features and fixing bugs, over time their contribution to the project invariably results in degrading it beyond the point they can possibly salvage it.At that point, they blame the tools.
Programming languages are a means to an end.  I'd rather have my cognition going towards solving the actual problem and not worrying about implementation details.
Python has a lot of footguns (and special __ methods), and insane, alien, scoping rules. And lots and lots of syntax, as well.It is not easy to learn (I have no idea why it's considered a beginner friendly language). It benefits from putting in work to learn it. It's not as bad as c++, but it is not a shining example of how to get it right either.
You are forgetting these teeny tiny teensy little things called "return of investment" and the ratio of energy expended vs. the results achieved.
Pythondoeswork better if you put in that kind of work. Otherwise you'll get bit by the way mutable default arguments work[0] (or never learn to use it to your advantage), or by late binding of closures[1] (and maybe you'll pick up the awful habit ofexploiting the confusing early binding of default argumentsto make the `lambda`s that you constructed in a loop work properly[2]). Or you won't get the big picture of the descriptor protocol[3] and how method lookup[4] is related to the implementation of properties[5]. Or you won't get the fancy metaclass[6] thing that your favourite framework is expecting you to treat as an opaque abstraction[7], or how and when you might use a class decorator[8] instead of a metaclass to do that kind of metaprogramming.[0]:https://stackoverflow.com/questions/1132941[1]:https://stackoverflow.com/questions/2295290[2]:https://stackoverflow.com/questions/3431676[3]:https://docs.python.org/3/howto/descriptor.html[4]:https://eev.ee/blog/2013/03/03/the-controller-pattern-is-awf...(yes, this was partly an excuse to get one of Zed Shaw's critics [9] into the discussion)[5]:https://stackoverflow.com/questions/3798835[6]:https://stackoverflow.com/questions/100003[7]: e.g.https://medium.com/@miguel.amezola/demystifying-python-metac...- I didn't have a good link for this and didn't spend a long time searching, but this seems okay[8]:https://stackoverflow.com/questions/681953[9]:https://eev.ee/blog/2016/11/23/a-rebuttal-for-python-3/
they’re still bad for generating any significant body of code.They’re a little bit better about deciphering errors.They’ll still bullshit* you and send you on wild goose chases.*hallucinate if you prefer
> They’ll still bullshit you and send you on wild goose chasesAnd confidently at that. It can't seem to find the backbone to say no to me either.If I say something like "wait, X doesn't seem to make sense, isn't it actually Y and Z?" it will agree and reformulate the answer as if Y and Z were correct just to placate me. I usually use the LLM to learn new things, I don't don't actually know if Y and Z apply.
C++ really is a blast and I think the complaints people have about it really depend on context. Lots of C++ devs hate that language but imo misdirect their hate. They actually work on legacy products built by people who were never that great at writing software. This happened to me with Rust actually where I was thrust into an existing rust project at the company I work for. It's an "old" codebase with many different and conflicting conventions within it (different error handling types that don't compose well, bad class abstractions, lots of unsafe block when not needed, etc). The project was  the most miserable project I've ever worked on and it's easy for me to blame Rust, but it's really just bad software development. There are warts in C++, but a lot of the pain people run into is just that they are working on crap code and it would be crap in any language.
In every other language (except C), basically every time you write v[x]=y you aren't inviting the possibility of arbitary memory corruption. The C++ 'std::optional' type makes calling '*v' undefined behaviour if 'v' is empty. The whole point of optional is to store things that might be empty, so why put undefined behaviour on the most common operation on an optional when it's empty (which is going to be common in practice, that's the point!)The problem I have with C++ (and I've written a lot), is every programmer involved in your project has to be 100% perfect, 100% of the time.I know you can run memory checkers, and STL debug modes, but unless you are running these in release as well (is anyone doing that?), then you are then counting on your testsuite hitting every weird thing a silly, or malicious, user might do.Given how important software is nowadays, and how Rust seems close to the performance of C++, is it really worth using a language where so many of the basic fundamental features are incredibly hard to use safely? They keep making it worse, calling an empty std::function threw an assert, the new std::copyable_function has made this undefined behaviour instead!
No, I'm talking about std::optional<T>. It's a type designed to store an object of type T, or nothing. You use '*v' to get the value out (if one is present).It was originally discussed as a way of avoid null pointers, for the common case where they are representing the possibility of having a value. However, it has exactly the same UB problems as a null pointer, so isn't really any safer.I'm going to be honest, saying that memory corruption in C++ is 'almost never a problem' just doesn't match with my experience, of working on many large codebases, games, and seeing remote security holes caused by buffer overflows and memory corruption occurring in every OS and large software program ever written.Also, that 'penalty' really does seem to be tiny, as far as I can tell. Rust pays it (you can use unsafe to disable it, but most programs use unsafe very sparingly), and as far as I've seen benchmarks, the introduced overhead is tiny, a couple of percent at most.
It was originally discussed as a way of avoid null pointers,I don't think this is true. I don't think it has anything to do with null pointers, it is a standard way to return a type that might not be constructed/valid.I think you might be confusing the fact that you can convert std::optional to a bool and do if(opt){ opt.value(); }You might also be confusing it for the technique of passing an address into a function that takes a pointer as an argument, where the function can check the pointer for being null before it puts a value into the pointer's address.I'm going to be honest, saying that memory corruption in C++ is 'almost never a problem' just doesn't match with my experience,In modern C++ it is easy to avoid putting yourself in situations where you are calculating arbitrary indices outside of data structures. Whether people do this is another story. It would (or should) crash your program in any other language anyway.Also, that 'penalty' really does seem to be tinyThe penalty is not tiny unless it is elided all together, which would happen in the same iteration scenarios that in C++ wouldn't be doing raw indexes anyway.
>I don't think this is true. I don't think it has anything to do with null pointersThe paper that proposed std::optional literally uses examples involving null pointers as one of the use cases that std::optional is intended to replace:https://isocpp.org/files/papers/N3672.htmlHere is an updated paper which is intended to fix some flaws with std::optional and literally mentions additional use cases of nullptr that the original proposal did not address but the extended proposal does:https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p29...I am going to be somewhat blunt, but if you're not familiar with how some of the use cases for std::optional is as a replacement for using raw pointers, along with some of your comments about how C++ treats undefined behavior, as if it's just results in an exception, suggests you may not have a rigorous enough understanding of the language to speak so assertively about the topic.
I am going to be somewhat blunt, but if you're not familiar with how some of the use cases for std::optional is as a replacement for using raw pointersI never said there wasn't a use case, I said it wasn't specifically about protecting you from them. If you put a null pointer in and reference the value directly, it doesn't save you.If you don't understand the context of the thread start arguments over other people's simplified examples. I'm not going to write huge paragraphs to try to avoid someone's off topic  criticisms, I'm just telling someone their problems can be avoided.
Yes you did and I literally quoted it, here it is, your words:"I don't think this is true. I don't thinkit has anything to do with null pointers, it is a standard way to return a type that might not be constructed/valid."This was in response to:"It was originally discussed as a way of avoid null pointers,"I have provided you with the actual papers that proposed std::optional<T> and they clearly specify that a use case is to eliminate the use of null pointers as sentinel values.
It's a template, you can put whatever you want in it including a pointer. All your example shows is that you can bind a pointer to a reference and it will wrap it but won't access the pointer automatically.I'm not sure what your point here is other than to try to mince words and argue. It's a standard way to put two values together and what people were probably already doing with structs. You can put pointers into a vector too, but that doesn't mean it's all about pointers.
I'm pretty sure the issue that the parent commenter is referring to isn't about wrapping a pointer type in an optional, but wrapping a _non-pointer_ type in an optional, and then trying to access the value inside. std::optional literally provides a dereference operator operator[1] which contains the following documentation:> The behavior is undefined ifthis does not contain a value.The equivalent to this in Rust isn't `Option::unwrap`, which will (safely) panic if the value isn't present; the equivalent is `Option::unwrap_unchecked`, which can't be invoked without manually marking the code as unsafe. I've been writing Rust professionally for a bit over five years and personally for almost ten, and I can definitively say that I've never used that method a single time. I can't say with any amount of certainty whether I've accidentally used the deference operator on an optional type in C++ despite writing at least a couple orders of magnitude less C++ code because it's not something that's going to stick out; the deference operator gets used quite often and wouldn't necessarily be noticeable on a variable that wasn't declared nearby, and the compiler isn't going to complain because it's considered entirely valid to do that.[1]: Fromhttps://en.cppreference.com/w/cpp/utility/optional/operator
Your basis seems to be no-one is ever going to write bad code, anywhere, ever, and invoke undefined behavior. That doesn’t seem reasonable.Also, an unwrap isn’t perfect, but it’s much better than UB. It asserts. No memory corruption, no leaking all your user’s data, no massive fines.The equivalent to C++ would be an unchecked unwrap in an unsafe code block, and that would throw up flags during review in any Rust codebase.
An unchecked dereference should also throw up flags during review in a C/C++ codebase. I didn't assume that nobody would make mistakes. My argument has always been that you use a language like C++ where needed. Most of your code should be in a GC language. Going in with that mentality, even if I wrote that code in Rust, I'm exporting a C API, which means I may as well have written the code in C++ and spend some more time in code review.EDIT: an unwrap that crashes in a panic is a dos condition. In severity this might be worse or better depending where it happens.Both are programmer error, both should be caught in review, both aren't checked by the compiler.
> "Rust isn't designed for efficiency"Citation needed, because Graydon Hoare the original Rust creator (who has not been involved with Rust development for quite a long time) wrote about how the Rust that exists is not like the original one he was designing:- "Tail calls [..] I got argued into not having them because the project in general got argued into the position of "compete to win with C++ on performance" and so I wound up writing a sad post rejecting them which is one of the saddest things ever written on the subject. It remains true with Rust's priorities today"- "Performance: A lot of people in the Rust community think "zero cost abstraction" is a core promise of the language. I would never have pitched this and still, personally, don't think it's good. It's a C++ idea and one that I think unnecessarily constrains the design space. I think most abstractions come with costs and tradeoffs, and I would have traded lots and lots of small constant performancee costs for simpler or more robust versions of many abstractions. The resulting language would have been slower. It would have stayed in the "compiled PLs with decent memory access patterns" niche of the PL shootout, but probably be at best somewhere in the band of the results holding Ada and Pascal."https://graydon2.dreamwidth.org/307291.html
The fact that by default array access is bounds checked in Rust and by default it isn't in C++ disproves that.I think you would have a hard time convincing the C++ standards committee to put a checked container in the standard, maybe now with the negative publicity maybe but definitely not before.I'm guessing it would be impossible to get an unchecked container into the rust stdlib.
https://doc.rust-lang.org/std/vec/struct.Vec.html#method.get...
My point isn't that people aren't going to write bugs in Rust; my point is that people _will_ write bugs in literally any language, and bugs that cause panics in Rust are not going to expose the same level of vulnerability as bugs in C++ that cause UB.There clearly are people who think that UB isn't as dangerous as I do, so if that's where you stand, I guess it is just a "fundamental difference of opinion". If you actually believe that you (or anyone else) is capable of being careful enough that you aren't going to accidentally write code that causes undefined behavior, then I don't think you're wrong as much as delusional.
It's okay to admit you were wrong.
I think if you had something real to say here you would have done it already.
I just want to pick out one thing:It would (or should) crash your program in any other language anyway.To me there is a massive difference between "there is a way a malicious user can trigger an assert, which will crash your server / webbrowser", and "there is a way a malicious user can use memory corruption to take over your server / webbrowser".And bounds-checking penalties are small. I can tell they are small, because Rust doesn't have any kind of clever 'global way' of getting rid of them, and I've only noticed the cost showing up in profiles twice. In those cases I did very carefully verify my code, then turn off the bounds checking -- but that's fine, I'm not saying we should never bounds check, just we should do it by default. Do you have any evidence the costs are expensive?
If you profile looping through an array of pixels in linear memory order vs bounds checking every access it should be more expensive due to the branching.As someone else mentioned you can boundary check access if you want to in C++ anyway and it's built in to a vector.My point here is that it isn't really a big problem with C++, you can get what you want.
Iterating over an array in Rust does not involve bounds checking every access since Rust guarantees immutability to the container over the course of the iteration. Hence all that's needed is to access the size of the array at the start of the iteration and then it is safe to iterate over the entire array.Note that this is not something C++ is able to do, and C++ has notoriously subtle iterator invalidation rules as a consequence.
They asked for evidence of bounds checks being expensive, please try to keep the context in mind.Note that this is not something C++ is able to doOr you could just not mutate the container.
> "They asked for evidence of bounds checks being expensive"They said "bounds-checking penalties are small" not that it was free. You've described a situation where bounds checking happens and declared that it "should be" expensive because it happens. You haven't given evidence that it's expensive.
I didn't give direct evidence, that's true.You can profile it yourself, but it is the difference between direct access to cached memory (from prefetching) and two comparisons plus two branches then the memory access.To someone who doesn't care about performance having something trivial become multiple times the cost might be acceptable (people use scripting languages for a reason), but if your time is spent looping through large amounts of data, that extra cost is a problem.
It's the optimization that C++ is unable to perform, not the immutability.
Are you talking about optimizing out bounds checking that isn't happening in the first place?
If you declare using an invalidated iterator as UB, the compiler can optimize as if the container was effectively immutable during the loop.
I've been wondering how much you'd pay if you just required that out of bounds accesses be a no-op with an option of calling an error handler. De-reference a null or out of bounds pointer you get zero. Write out of bounds, does nothing.int a = v[-100]; // yolo a is set to 0I really suspect that unlike the RISC mental model compiler writers think in terms of a superscalar processor would barely be slowed down. That is if the compiler doesn't delete the checks because it can prove they aren't needed.
My understanding is that the cost that is mentioned comes from the branch introduced by checking if the dereference is in-bounds (and the optimizations these checks allegedly prohibit), not from what happens if an OOB access is detected
Where I feel that breaks down while probably leet code would be faster. But high quality code tends to have checks to make sure operations are safe. So you got branches anyways in real code.
You have to do a very similar operation to has_value() .value() with Rust's optional though... How is opt.ok_or("Value not found.")?; so different from the C++ type?
It seems like people think dereferencing an unengaged optional results in a crash or exception, but ironically you are actually less likely to get a crash from an optional than you would from dereferencing an invalid pointer. This snippet of code, for example, is not going to cause a crash in most cases, it will simply return a garbage and unpredictable value:auto v = std::optional<int>();
    std::cout << *v << std::endl;While both are undefined behavior, you are actually more likely to get a predictable crash from the below code than the above:int* v = nullptr;
    std::cout << *v << std::endl;I leave it to the reader to reflect on the absurdity of this.
What is the desired behavior? I see at least 3 options: panic (abort at runtime predictably), compiler error (force handling both Some&Nothing cases [needs language support otherwise annoying], exceptions (annoying to handle properly). There is too much undefined behavior already.Perhaps, 3 types could exist for 3 options.
One is undefined behavior which may manifest as entirely unpredictable side-effects. The other has semantics that are well specified and predictable. Also while what you wrote is technically valid, it's not really idiomatic to write it the way you did in Rust, you usually write it as:if let Some(value) = some_optional {
    }At which point value is the "dereferenced" value. Or you can take it by reference if you don't want to consume the value.
I prefer ok_or for optionals and map_err for for results in Rust. I believe the way you proposed ends up with very deeply nested code which I try to avoid.
`std::optional<T>`'s style is more akin to usingif x.is_some() {
    let x_value = unsafe { x.unwrap_unchecked() };
  }everywhere.
You can pay the penalty and get safe container access, too. AFAIK, C++ standard containers provide both bounds checked (.at()) and unchecked ([]) element retrieval.
Now we are getting down to a philosophical issue (but I think an important one).In Rust, they made the safe way of writing things easy (just write v[x]), and the unsafe one hard (wrap your code in 'unsafe'). C++ is the opposite, it's always more code, and less standard (I don't think I've seen a single C++ tutorial, or book, use .at() as standard rather than []), to do bounds checked.So, you can write safe code in both, and unsafe code in both, but they clearly have a default they push you towards. While I think C++'s was fine 20 years ago,  I feel nowadays languages should push developers to write safer code wherever possible, and while sometimes you need an escape hatch (I've used unsafe in a very hot inner loop in Rust a couple of times), safer defaults are better.
I really can't resonate with this. The type system is obnoxious (and you don't even get proper memory safety out of it), the "most vexing parse" truly is vexatious, there's all the weirdness inherited from C like pointer decay (and then building new solutions like std::array on top while being unable to get rid of old ways of doing things), and of course the continued reliance on a preprocessor to simulate an actual module import system (and the corresponding implications for how code for a class is organized, having to hack around with the pimpl idiom etc....)Essentially, the warts are too large and numerous for me to find any inner beauty in it any more.
> and of course the continued reliance on a preprocessor to simulate an actual module import systemC++ modules are supposedly meant to save the day. Although only recently have the big three compilers (GCC, Clang, MSVC) reached some form of parity when compiling modules.
I mean, this is all anecdotal, but I've only ran into the most vexing parse a few times, I rarely had to use the pimpl idiom, and the header file stuff... okay that isn't great. I've been a c++ dev for 10 years and I actually worked on an extremely old codebase and was involved in modernizing it is to C++11. Maybe I'm too C++ brained, but all those things just aren't that bad? There is no wartless language and you just deal with the warts of the language you're in and it's your responsibility to learn the details of the language.
The header stuff is pretty bad. The rest are strange choices of stuff to complain about. The vexing parse was never a big deal and these days with bracket initialization (which only rarely you can't use) even less. The pimpl idiom: not sure why it's really a problem. Plus you have many choices for type erasure in c++.
>The vexing parse was never a big dealI definitely got bit by it multiple times.>The pimpl idiom: not sure why it's really a problem.Because it's even more boilerplate and adds indirection in a place where you might have been painstakingly trying to avoid it (where the first indirection costs you all your cache coherency). C++ lets you go out of your way to avoid objects having to carry any overhead for virtual dispatch if you aren't going to use it; but then you might have to choose between rerouting everything through a smart pointeranywayor suffering inordinately long compile times. Nothing to do with type erasure (if I'm thinking clearly, anyway).
I can deal with all the warts but I'm not really having a blast.
True - but it’s much easier to make an incomprehensible mess in more complex languages. Whereas blub language projects are pretty easy to decipher regardless of the state of the codebase.
I refer you to the (admittedly dated, but also quite funny) C++ FQAhttps://yosefk.com/c++fqa/
This 1000%, stop blaming the tools!
How can we make better tools if we don't blame tools?
Eh, I'm reminded of the old "programming languages as weapons" comic, with the one that I still remember is JavaScript was a sword without a hilt, with the blade being the "good part" and in place of the hilt, another blade labelled the "bad part".You can blame devs instead of tools all day long, but you can't deny that there are things about the tools that hold the developers back.
I’m glad someone is having fun in C++. Personally, after >20 years, I have to be done with C++. It’s just a mess.If I really need the low-level control, I can wrangle C (warts and all), otherwise Rust, Python, etc just make me happier.
Agree. C++ is here to stay though. Too much code written in that language. It's certainly a great skill to have, and there are super lucrative jobs too.For me, I feel the language just go in the way and is way too complex. Sure, I know the discourse: "modern C++" is great, you don't need to learn about older. versions of C++, things are getting simpler with each new version.The problem is that the codebase you get to work on aren't modern C++. They use every possible feature ever released and it's just hard and full of pitfalls.I suppose people who have only work on C++ in their projects for years can develop some level of expertise and are immune to the complexity. But really, not a great language...
One of the things that has made C++ more tolerable over time is that it became easy to seamlessly replace most of it with an implementation that works the way you think it should and with most behaviors being fully defined -- the core data types, standard library, etc. Particularly with C++20 and later, alternative foundations feel quite native and expressive.Most new C++ projects I see lean into this quite heavily. The ease with which you can build a systems-level DSL for your application is a strength of C++.
There's a parallel here with modern applications perl as compared to the crawling horrors people perpetrated in CGI scripts during the dot com boom.Also present day javascript (the language; the ecosystem is another matter ;) compared to the 'var' and IIFEs-for-scoping era.
I take a few features from C++ in my C++/SDL2 Ultima spinoff project -- never completed sadly. Class, std::function, std::unordered_map, std::string, std::unique_ptr<type> are the only ones that I could recall.I can't imagine reading other people's code though, unless it's in a similar style. I did find that the most difficult part is to get past the programming patterns (such as Visitor pattern) as I don't write large programs professionally.I wish C++ stopped at C++/11. The committee seems to want to include everything into it at the moment. Maybe C++ is sort of ULTRA programming language that supports every niche programming style.
FWIW, C++20 is amuchtidier and more usable language than C++11. The difference between C++11 and C++20 is almost as large as C++11 and legacy C++.
Thanks, I briefly read the changes on cppreference. It's a good list (although I don't get many of the points as I'm an amateur) but I think my small project doesn't benefit a lot.
What is your Ultima spinoff project?
Ah that was my pet project to create an engine that runs a game similar to Ultima 1-3. I even used the Ultima 4 sprite sheet. Never completed it though.
C++ is an absolute mess.But it's a fun mess, and I like writing in it :) Sometimes, that's important.My relationship with, say, Rust is much colder: I respect Rust, I use Rust where I think it makes sense (especially in e.g professional contexts where I often can't defend the use of C++), and I think what it's doing is important. But I find it tedious to write, despite having a fair amount of Rust projects under my belt at this point. It's better in key ways, but not asenjoyable, for me.
Rust is fun for me but I keep it high level with Rc and built-in data structures. It's fun thinking about variables as little boxes of fixed size that values get moved into or out of. It's so different than almost any other language where stuff lies often god knows where and is referenced ad nauseam. Although that can be fun too if the language treats mutability as exception rather than a rule.
Is Rust not meant to offer the same kind of control and "low-level-ness" as C? Like, can't you cast integers to pointer explicitly if you break the `unsafe` seal (and thereby e.g. do memory-mapped IO to control hardware)?
> I'm glad someone is having fun in C++. Personally, after >20 years, I have to be done with C++. It’s just a mess.I've spent a couple of decades working with C++, and the truth of the matter is that, as much as it pains the bandwagony types, C++ became great to work with once C++11 was rolled out. The problem is that teams need to port their projects to >C++11 and that takes an awful lot of work (replace raw pointers with smart ones, rearchitect projects to shed away C-style constructs, etc) which some product managers are not eager to take.I firmly believe that the bulk of this irrational hatred towards C++ comes from naive developers who try to rationalize away their lack of experience by blaming everything on the tool. On top of this, cargo-cult mentality also plays a role.What makes this quite obvious is the fact that the solution presented to all problems always comes in the form of major rewrites, even with experimental and unproven features and technologies. Writing everything again is always the solution to everything. Everyone before them knows nothing whereas these messiah, cursed with being the only ones who see the one true answer, always have a quick and easy answer to everything which always boils down to rewriting everything from scratch with the flavor of the month. Always.Weird, huh?
> C++ became great to work with once C++11 was rolled out. The problem is that teams need to port their projects to >C++11The problem is the C++ that's not great to work with is still there, and there's nothing preventing the rest of the world from using it; there are always going to be naive developers with a lack of experience who don't know how to use the tool. For this reason, all the code that's possible to write in C++ will be written, that includes the unsafe code.It's not enough to have a safe, nice, modern, subset of C++ that everyone "should" use. If developers have the option to use the warty, sharp, foot-gun infested version of C++ they will, and they will gift that code to the rest of us in the form of readily exploitable software.This is why organizations like CISA are suggesting developers move to other languages that take a stricter posture on memory safety:https://www.cisa.gov/news-events/news/urgent-need-memory-saf...> companies should investigate memory safe programming languages. Most modern programming languages other than C/C++ are already memory safe. Memory safe programming languages manage the computer’s memory so the programmer cannot introduce memory safety vulnerabilities. Compared to other available mitigations that require constant upkeep – either in the form of developing new defenses, sifting through vulnerability scans, or human labor – no work has to be done once code is written in a memory safe programming language to keep it memory safe.
> The problem is the C++ that's not great to work with is still there, and there's nothing preventing the rest of the world from using it;That's precisely why all this criticism is actually thinly veiled naive inexperient developers blaming the tools.  Selling full rewrites as solutions to the problems they created is a telltale sign. As they are lacking the experience and know-how to fix the mess, they succumb to the junior dev disease of believing deleting everything and starting from scratch is a solution to all of life's problems and inconveniences.
> naive inexperient developers blaming the toolsThat's not the problem. It's naive inexperienced developersusingthe tools. Most developers have to maintain code they didn't write themselves. One can learn all the C++ best practices in the world, but it won't protect you fromother people. That's why languages with strong restrictions and constraints that force safety and correctness are needed. With such languages, naive inexperienced developers won't be able get anything to compile. We won't have to deal with their mistakes as they'll never be able to ship them. Any experienced developer would surely want this.A rewrite is not pointless if you are rewriting into a language with additional guarantees. You are checking for and proving the absence of certain classes of software flaws by doing so.
> With such languages, naive inexperienced developers won't be able get anything to compile.Hey dude, I can’t get this thing to compile?Just wrap all your variables in Arc; that’s what I always do.
I don’t see how juniors, who want to rewrite things, are the reason why C++ has 3 freaking ways to initialize a variable.
There’s a few more than three.https://leanpub.com/cppinitbookhttps://www.reddit.com/r/ProgrammerHumor/comments/8nn4fw/for...
> C++ became great to work with once C++11 was rolled out.Have Yossi Kreinin's objections (https://yosefk.com/c++fqa/defective.html) been addressed yet? In particular, can I reuse source code from another file without a text preprocessor yet? Can I change a class' private members without recompilation, or am I still stuck with indirecting that behind a "pImpl" pointer in the class declaration in the header file? (Being able to use a smart pointer for that is not addressing the problem.) Are compiler error messages resulting from type mismatches reasonably short (without third-party tools like STLFilt) yet (never mind whether they're informative or pleasant to decipher)?I know that "some parts of the FQA are not up to date with C++11 [and onward]", but I haven't heard of these problems being fixed.
> Have Yossi Kreinin's objections (https://yosefk.com/c++fqa/defective.html) been addressed yet?A cursory read of that list is enough to see it's a list of complaints fueled by a mix of ignorance and disingenuity.For example, the first entry complaining about "no compile time encapsulation" is somehow completely ignorant and oblivious to very basic things such as the pimpl idiom. I mean, this thing is notorious in the way it allows Qt to remain binary compatible across even major version bumps. And yet, "This makes C++ interfaces very unstable"?The list reads like pure nonsense, to be quite honest. At some point the author gripes over the lack of garbage collection. Which language lawyers know very well that until somewhat recently C++ standards actually had provisions to explicitly support it, but whose support was removed because no one bothered with it.Is this your best reference?
Yossi is aware of the pImpl idiom and refers to it explicitly in section 16.21 of the FQA. It adds the overhead of indirection; in particular, it means that even when you don't use polymorphism and were able to avoid the cost of a vtable, you still don't get to have an array of instance data contiguously in memory. And it's still something you have to do manually; you don't get it by default. It seems clear to me that this simply doesn't meet Yossi's standard for "compile time encapsulation".>At some point the author gripes over the lack of garbage collection. Which language lawyers know very well that until somewhat recently C++ standards actually had provisions to explicitly support it, but whose support was removed because no one bothered with it.Other people not caring about garbage collection doesn't mean it's a missing feature. It's clear why operator overloading in particular would benefit from the ability to make temporaries without worrying about the memory they allocate. (Of course, this was written in an era with a much poorer ecosystem of "smart pointers".)>Is this your best reference?It's not as good of a reference as I remember it being, I suppose. Ithasbeen a long time. But what I've seen of C++ in the interim, bit by bit, has generally made me less inclined to pick it up again, not more. The complexity just doesn't seem justified.
Beware of C23 and later.C is basically going into "we want C++ but without OOP and with templates done via _Generic".Also LLVM and GCC aren't going to be rewritten from C++ into Rust anytime soon.
>Beware of C23 and later.Beware of what? C23 fixed a number of issues. Sure, there are some oddballs (QChar and auto, mostly), but overall I think C23 is an improvement.
Those C89 hardliners have a different point of view.Also there is the whole point Objective-C and C++ are much better than those improvements will ever be.
> Those C89 hardliners have a different point of view.C89 was only a thing because Microsoft somehow decided to drag it's feet and prevented msvc from supporting anything beyond c89 for a long, long time. Only in 2020 did they manage to finally do something about it.https://devblogs.microsoft.com/cppblog/c11-and-c17-standard-...
Because for Microsoft C belonged into the past, it was about time to move into C++.https://herbsutter.com/2012/05/03/reader-qa-what-about-vc-an...They only changed of point of view after Satya, and the whole Microsoft <3 Linux and Microsoft <3 FOSS pivot.And in any case, blaming Microsoft doesn't really work out, as many of those folks don't even care Windows exists, only UNIX/POSIX platforms.
> Also LLVM and GCC aren't going to be rewritten from C++ into Rust anytime soon.I'm afraid you underestimate the will power of rustaceans to find literally anything to rewrite.
I doubt Cranelift will ever match the only project that has beaten the contribution level of Linux kernel.
I imagine most C programmers are still using C99 or older anyway, particularly in the embedded space.
I'm not sure this is still true. I'm just a hobbyist but esp-idf, for example, supports C++23.
esp-idf is still on GCC 11, but most of the features of C23 are in GCC 13.
C99 ornewer. You gotta have standard fixed size types like int16_t.
Not in the embedded space.  When you've worked on embedded systems long enough, you learn that you have to accept the compiler that the vendor provided you with, and you adapt your codebase to the limitations of that compiler.  This means working around code generation bugs, adding #ifdefs to define typedefs for things like int16_t if they don't exist.That said, things are a lot better than they were 15 years ago, and the mainstream ARM  compilers used today are leagues better than the barely functional cobbled together ports from the early '00s.  ARM64 is a tier 1 platform, and there are enough users of the platform that the extended QA team that embedded developers were unintentionally part of in the past is no longer really the case (at least when it comes to the compiler).However, there are still truly obscure architectures (mostly 8 bit and 16 bit) that continue to have oddball compilers.  And you accept and deal with that insanity because it means you're only paying $0.01 for that microcontroller to blink an LED when some kind of an event occurs.
What part of C23 makes you believe this?
Everything that was taken from C++, and _Generic.And the ongoing discussion for lambdas that didn't make it into C23, but still on the table for C2y.Meanwhile, zero progress in what actually matters, proper strings and arrays, that aren't a  continuous source of memory  corruption bugs.
_Generic is from C11 though. For array I wonder people don't use them more. After all, array access can easily be bounded-checked in contrast to pointers. I essentially eliminated all my spatial memory bugs this way.
Same. I wish someone made a better C++ from a blank slate. And no it's not Rust.
This is a long rant that covers a lot of ground, a lot of which will inevitably be ignored because the letters "C++" trigger people, myself included sometimes. (Skip ahead to "It's Not All Puppies and Butterflies" for some of the complaints.) The author is really impressed by C++11, as am I, after purposely ignoring C++ for the better part of twenty years.I appreciate the shout outs to some packages and libraries to play with, although I still often find it a pain to incorporate other libraries into my projects. (Single-file headers, anyone?) I'm intrigued by FTXUI.And boy, howdy, he's right: cppreference.com is amazing.  Python's documentation is pretty good, but I've never seen anything as good as cppreference.com.
cppreference.com is very, very good at beingreference, as in the name.Python's documentation is scattershot and incomplete in many places, and lacks a consistent copy-editing style - but it offers good coverage of all kinds of documentation (per the Diataxis framework), not just reference. The people writing that documentation explicitly take that framework into consideration and use it to look for ways to improve. (But it's still a volunteer effort that works basically the same way the code development does, following open-source principles, so.)
The article filled me on a lot of things I didn't know about C++ because I learnt it at school and college, but soon moved to Python/JavaScript for day job. I have been itching to "get closer to the system" for a while now, and learning Rust on the side hasn't been easy. This article gave me hope that, I might be able to do that if I refreshed C++. Hello CMake… or I should probably say, Meson.
If you're starting a new project with a talented team that knows how to use modern C++ well, I agree that C++ is great! It's a pleasant and powerful language, delivers great performance and (while complicated) is straightforward to debug and optimize.I had the privilege of working on a codebase that was about 5 years old, written to C++11 standards or later, and I thoroughly enjoyed it, for many of the reasons expressed in the article.If you are working on an older codebase that has evolved over 20-30 years, or one that has not been maintained by talented people, you will have a very different experience.
I started my life as a dev with PHP, learning it on the job to set up a custom Wordpress theme with plugins for special datatypes (eg STCs).When I was 21, they offered to pay for my tuition so I went back for a CoE degree (later switching to CS). The moment I touched C++ - on my very first day in their “intro to programming” class - I fell in love. The type system, debugging, and ecosystem was fantastic compared to PHP 7 that I was using at the time. With my manager's permission, I went back to some of our existing intranet apps and built a C++-based API for them that our PHP would call over a web-socket, so that I could leverage the type system (and performance). Those systems are still in daily use, company wide. Maintenance hasn't been a problem. And that’s despite running in a really weird environment (IBM Pase for i).Now, after using Rust and Axum, I'd vastly prefer the Rust ecosystem for creating that type of API. But C++ deserves credit. It really is a great language, especially if you’re coming from web languages like (non-TS) JavaScript or PHP.
> If you are working on an older codebase that has evolved over 20-30 years, or one that has not been maintained by talented people, you will have a very different experience.I'm trying to think of any other language that could fit that description and not also be a terrible experience. In fact, I'd prefer C++ to many other possibilities I can think of (C, Java, Fortran). The exception would be C#, but I'm not sure if it's old enough to fit.
Fair point, although I think it's true that C++ gives you more rope to hang yourself with, and if I had to choose, I'd rather maintain a crusty old codebase written in Java than one written in C++.
The issue is that the JVM has started deprecating features which old codebase rely on.I don't work with it, but even I know about the JVM no longer supporting sandboxing or being able to kill threads programmatically.C++ doesn't have this issue because you can still deploy code from an old toolchain onto a modern OS.
You can and always have been able to .destroy() threads in java, but doing so breaks the memory model in  ways that make it clear that you should never do it. My guess is that if you're kill heavy in c++, then you've also run into bugs waiting on locks of threads you've now just murdered. That's why there's a ton of great abstractions for how to "kill work running somewhere else" that doesn't leave your application in a potentially unstable state.Java 9+ sure did kill off a lot of things that weren't particularly ideal. Most normal devs who don't write libraries probably can't even name a single feature removed (corba, script interpreters, some jmx cruft, etc). Most of what was culled still exists in libraries that could easily be added back with project imports. Maybe Java applets are super dead now? I don't believe any modern browsers still support native plugins, but maybe there are some niche individuals bemoaning the loss of java web start.As for the "can still deploy code" comment, the same applies to java. Look around, and sadly you see very old releases of java still in use today. My guess is that if I fired up a java 1.0 compiler and JVM today, it would run on my PC (poorly).Before criticising something, please consider being well enough informed to warrant the comment.
I want to run untrusted code in the JVM, and limit its access to IO and compute so it can't phone home or mess up my system. This used to work with the Security Manager plus being able to kill threads.Now, based on the new thread killing implementation, a malicious thread can just catch the exception and keep running, and on top of that the Security Manager is being removed too.So now my options are, switch to C++ and Lua, or run every sub-component in its own VM and add a bunch of IPC in the middle of my application. Or maybe port it all to JavaScript. Massive headache, performance implications are bad and it breaks continuity in an open source community that's been running since 2001.This is my use case, maybe hello world 101 doesn't have any problems but I'm sure anyone doing more complex stuff is going to hit the same kind of issues.
You do have some options in the JVM world. As mentioned, you should still be able to kill threads if you really wanted to, but if you wanted to pivot to JS/LUA, there are still jsh alternatives like rhino in JVM to avoid some of the larger reworks. They are based on their own stack/continuation work of over a decade ago, so not fresh. You may want to look at virtual thread pools through. Since these are green threads and the JVM preempts them aggressively, there may be the abstractions you're able to exploit for your more aggressive culling needs.I don't know about security manager though. I haven't poked at it for a while, but I wasn't aware that they removed it? Maybe you just need to opt I to the JVM access rule to support it. You can always layer your security with a separate class loader, which can prevent child tenants from even seeing protected classes and statics, which is always good security layering if untrusted is your problem.
So that's a fun one. C# the language people know first released in y2k and the 1.0 release was 2002.So it definitely is old enough to fit and it certainly has it's warts from age that show up in long lived projects.
I used to work on a codebase at Microsoft that was classified as a microservice. Pretty much entirely written in C# and it was about 12 years old.
> C++ has built-in regex now and they're pretty damn good tooSadly, `std::regex` is anything but good (poor performance, lack of Unicode support) and should be generally avoided.
The linter used at my work insists on not using it....
Is there something wrong with the specification that causes poor performance?
The underlying issue is that for the matches the interface relies a lot on heap allocations for the individual matches, leading to a lot of allocations of small regions to copy from the original input. Many other libraries provide a lot more control there.In benchmarks std::regexp often is a lot slower compared to other implementations, independent from the standard library implementation of choice.The big upside compared to all others is that it's always available. But if there is a choice alternatives are often better.
It’s ultimately a “won’t break ABI” issue:https://stackoverflow.com/a/70587711
I learned C++ and enjoyed it but never went too deep. I always enjoyed C more. There's also so many S tier codebases to read to learn C better like the original Dune game engine or Unix utilities.Having dabbled a bit with Rust recently I can't see any strong reasons to use C++. The combination of strong functional programing inspired type system with performance seems unbeatable. Almost a perfect language.I'm sure there must be some legacy reasons to use C++ though. Maybe Game Engines, embedded programing, some kind of other legacy tie in?
I switch between C++ and Rust at work. Honestly with modern C++20/23 a lot of the pain points are being fixed mostly by just copying Rust. If you make a new C++  codebase its possible to do it reasonably cleanly. But at this point I don't understand why you would make new software in C++.Here are a bunch of C++ annoyances I can think of. Library/Package management is nonstandard. Headers are code duplication. The standard library changes depending on implementation and is almost undreadable. Weird behavior in the standard. People relying on undefined behavior without realizing. Use before assignment issues. Subtle ownership issues and memory leaking due to bad refcounts. Needing to make everything const instead of by default. Checking for exceptions in everything you call to ensure your code is noexcept. Unreadable errors when working in the standard library. Heavily OO code is basically tech debt. The LSP is not structural like in Rust where the definition is found by checking the AST. Navigation and codebase discovery is slow in C++ because of the poorer LSP.Rust has first class explicit 'nostd' support whereas in C++ you need compiler specific flags to disable the std lib and its hard to make sure you did it right. So the embedded reasoning is silly to me.Rust also has game engines like bevy but they are new. You could hook into godot scripting with Rust if you want. Low level audio is just as easy in Rust and you can do it cross platform with a single crate.In general I think it's just legacy code and hesitancy to change.
Hugeamount of legacy across many dimensions, not just apps written in it 
[1], like number of users, published and available knowledge / resources (books, courses, blogs, articles, videos, software libraries, etc.), high compatibility with another huge language (C), etc.This is just software industry general knowledge, for those who have been there for more than a few years in the field. I am not even a proper beginner in it, because I have never used it much, although I had bought, read and to some extent, understood some classic C++ books, including by the language creator (Bjarne Stroustrup [2]), Scott Meyers [3], and a few others, earlier. I did have a lot of experience using C for many years in production, though, including on a successful commercial product.[1]https://www.stroustrup.com/applications.html[2]:https://www.stroustrup.comhttps://en.m.wikipedia.org/wiki/Bjarne_Stroustrup[3]https://en.m.wikipedia.org/wiki/Scott_Meyers
C++ makes sense when you need low latency, but don't really care about correctnessSo I believe gamedev will stay on C++ the longestGame crashes with segfaults for 0.001% users? So what?
HFT perhaps?
I can see that.On most metrics I've seen Rust is comparable on general speed.Maybe if you're at the level where you've essentially writing portable assembly and are okay with lack of safety. You need to know exactly what is happening within the CPU, maybe on custom hardware.I bet some defense applications would be in this category too, although for my own sense of self preservation I would prefer the Rust type system.
Rust would probably be a good fit for HFT, but as the field is so dominated by C++ is hard for another language to make inroads. Java managed to some extent.I would expect a lot of unsafe though.
Ecosystem effects are definitely important to C++'s dominance in HFT, but it's also a domain where a lot of the guarantees Rust offers just aren't all that relevant. From a security perspective most code always runs in sandboxes accessible to only a select few whitelisted IPs. True, you don't want a segfault while you're in the middle of sending an order to an exchange, but most of those are pretty easily smoked out in simulation testing.
Yes, if you are receiving malicious data from the exchange, getting p0wnd is the least of your concern.
I worked with c++ for many years and it has lots of warts. Slow compilation, lack of decent build system (no cmake isn't that great), mistakes made in the standard library (iostreams), weak cross platform standard libraries were always a headache.
But I always loved the power that <algorithm> and the like provides, although attaching allocators (and other template things) kind of sucks.When I looked at and test drove rust all I saw was heaps of complexity stacked up yet again. Probably didn't help that I smacked headlong into the Arc Mutex and lifetimes mess. I'm really more inclined to go for something like 'zig' which does so much with simple syntax (and awesome comptime) and still gets excellent performance.
> no cmake isn't that greatNor is cmake even really a standard: after thirty-odd years writing C++, I've yet to work on a project which used it!
That's pretty damn impressive! How did you manage to avoid codebases with CMake? Or are you just saying you personally chose not to use it for your own?
It simply hasn't come up: none of the teams I've ever worked on have chosen CMake. I've certainly encountered & compiled libraries which use it, but I've never had any reason to edit a CMakeLists file myself.Of course CMake has only really been a practical tool for 20-ish of those 30-odd years, and "Modern CMake" was only published twelve years ago; it's definitely gained a lot of popularity over the last decade, but the idea that CMake is the default C++ build system, not just one among many, still feels to me like a recent change.
It seems meson is gaining traction and will overpass cmake in near future.
> lack of decent build systemAt one point I was using gulp.js to build my small C++ experiment.
Using c++ after learning rust feels like picking through moldy bread
Using C++ after Rust made me appreciate the latter a lot more. You quickly learn all the footguns that the compiler stops you from doing and is generally a good learning experience.
Plus the Rust compiler actually gives you helpful error messages. C++ compiler errors might as well be in Klingon.
This is what I feel as well. I liken it to using an aimbot in Quake. Turn off the aimbot and you still win because the aimbot trained you how to get headshots. There are many times the Rust compiler told me I couldn't do something I had insisted would be fine, only to ponder and realize in fact what I was doing would cause subtle bugs in an edge case. Rust catches it at compile time, C++ allows you to write the code and sends you a segfault when the subtle edge case occurs in production.
Segfaults in production are thegoodcase. They're when the system recognizes you've made a mistake and stops it from propagating. The bad cases are when the program keeps running but silently does the wrong things.
Yes! This can be a real problem when your data structures are allocated from a memory pool. Since the whole memory region is owned by the program, out-of-bounds writes will either do nothing or silently corrupt your program state.
The good case is that you catch the error in development and it doesn't even get to production.
Elaborate?
>I really hate RAII.I'm baffled by this. Deterministic destruction (necessary and basically sufficient for RAII) was one of the best design decisions in C++, I think. RAII means youcan'tforget to clean up a resource. You don't have to type anything for it to get cleaned up at the right time, it just happens. It's basically what GC promised developers, except for all resource types, not just memory. Your code gets shorterandmore correct.I'm extremely curious to hear how this could ever be bad.
RAII isn't just about the destructor, though.  It's about writing theconstructorin such a way that you can always safely destroy the object.  If you don't write the constructor right, RAII might become something you really hate, because it could blow up on you any time the destructor fires.
I meant that it's wonderful touseRAII classes that others have developed, rather than implement RAII classes for others to use. But now that you mention it:>It's about writing the constructor in such a way that you can always safely destroy the object.I agree, but I see this as a good thing, even though it makes the class slightly harder to implement than it might be if responsibility for getting the object into a state safe for destruction is dumped on (or even shared with) the calling code. The latter approach requires programmer discipline at everyuse-- but you will only implement the class once, and probably use it many times.
> I want you to ask yourself an honest question. When was the last time you actually had fun in programming?Every time I write C code.
For me, C is fun until I hit a certain level of abstraction complexity involving fake homespun vtables and it starts getting harder than it should be to chase down bugs.
Use ASAN:https://en.wikipedia.org/wiki/Code_sanitizer
Everytime I need to write C, I wish I could use C++ instead. The things I miss most are RAII, templates, lambdas, const-correctness, and most importantly, a standard library with container types and algorithms.
What is wrong with const-correctness in C?I think container types and algorithms is a fair point, but if you program C more you should have a go-to library or your own implementation.
> What is wrong with const-correctness in C?One problem is that most C compilers accept implicit conversion between unrelated pointer types by default. If you accidentally pass a `const T*` to a function that takes a `T*`, you only get a warning. In C++ this is always a compiler error.Another thing that I find very annoying (that is more about `const` in general): in C you cannot use `const` variables as compile time constants, instead you always have to use the preprocessor.> but if you program C more you should have a go-to library or your own implementation.Sure, but I still don't like the fact I need to find a third-party library (or roll my own) for the most basic data structures. Also, it's not like generic containers can be trivially implemented in C. They require lots of preprocessor magic and the resulting API will always be much worse than any equivalent C++ implementation, in particular with non-trivial types.
I do not see how this not being an error is a problem. First, you can usually toggle this via a compiler flag. Second, I personally find it very convenient that the compiler does not stop for such things. This gives me more freedom  how I can do my work, e.g. fixing const warning or do some other change first.I do not agree about the APIs being worse than C++.
You can build with -Werror like the rest of us to get a compiler error instead of a warning.As for compiler time constants, you certainly can use them for those, but the compiler will happily identify constants without the const keyword, whose true purpose is to cause a build time error if you try to modify it.Regarding libraries, see my other reply:https://news.ycombinator.com/item?id=42506570Libuutil is a system library that is on any system that uses ZFS, although there are no known external consumers, so if you are interested in being the first, feel free to open bug reports with OpenZFS asking for removed functionality to be restored if you want any of it.
> As for compiler time constants, you certainly can use them for thoseNo, you can't. In C, `const` variables are not constant expressions. This means you can't use them in case labels or to define the size of an array.> Regarding libraries, see my other reply:I know that there are several container libraries for C. I was only complaining that there is no standardized solution for the most basic data structures.
What you want is something is is exclusively for compiler time constants, rather than can merely use compiler time constants. I know I am being pedantic here, but that is an important distinction to make when dealing with computer programming languages, since the descriptions need to be free of ambiguity or there will be problems. What you had meant was unclear to me in your previous remarks. I understand now.
There is the trick: use enum for const. It works most of the time.
Yes, at least that's better than macros.
As for not having a standardized solution, sys/queue.h is the closest you will get to it. It would be nice if we had one, but it has not happened yet.
You should see the libuutil library that Sun made to provide containers and other niceties. Even without it, there is always sys/queue.h.
I agree, I switched from C++ to C and I found it relaxing to be able to just forgot about a million language features and their complicated interactions.I also find you have some experience, know how to build good abstractions and have a set of good data structures, there is no issue with address complex problems in C.
For me I think it was Turbo Pascal and Delphi. And I will be honest, Visual Basic, and very early PHP. And may be even early Java.I still dont think any programming language today capture what we had in the late 80s and 90s. But may be that is just nostalgia.
every time I write C code, I don't want to remember how to implement unordered map
You meanenum { nb = 1024 };
    struct { int k, v; } hash[nb];

    // 0 is an invalid key
    void incr(int k)
    {
      int i = k % nb, j = i;
      do {
        int k2 = hash[i].k;
        if (k2 && k2 != k) {
          i = (i+1) % nb;
          continue;
        }
        hash[i].k = k;
        hash[i].v++;
        return;
      } while (i != j);
      abort();
    }Apologies for the telegraphic variable names and weird control flow.  I wrote this on my cellphone.  Lacking these 15 lines are what keep you from writing C?There's a nice tutorial on hash tables in K&R, and I can also recommend learning about Chris Wellons's "MSI" hash table design, described in C athttps://nullprogram.com/blog/2022/08/08/.  He shows a more full featured hash table in about 30 lines of code, depending on what functionality you need.  It's eminently practical, and usually a lot faster than generic algorithms.That's not an exceptionally simple hash table either.  One night I hurriedly wrote a rather long-winded implementation also on my cellphone (strings, with separate chaining and a djb-style hash function) and it also came to about 30 lines of code:http://canonical.org/~kragen/sw/dev3/justhash.c
In the interest of correctness, I just wrote a quick test of the above code athttp://canonical.org/~kragen/sw/dev3/intcount.c.  Perhaps surprisingly, it works, including the error handling, which is not always the case when you write a bunch of untested C after midnight on your cellphone.  But a hash table is evidently simple enough.
Except, you know what? It fails (corrupting memory!) for negative keys k.
The fundamental problem with C++ is that it has hiding ("abstraction") without safety. That's rare.- C -- No hiding, no safety- Python, Javascript, LISP, other interpreted languages -- hiding with safety- Pascal, Ada, Modula, Rust -- hiding with safety- C++ -- hiding without safety.C++ is, decades late, trying to get to hiding with safety. But there's too much legacy.
Well C has functions. Isn't that the original hiding abstraction?
It is an abstraction, but the safety requirements are neither enforced nor abstracted away.C's type system can't communicate how long pointers are valid for, when and where memory gets freed, when the data may be uninitialized, what are the thread-safety requirements, etc. The programmer needs to know these things, and manually ensure the correct usage.
Come write products on top of Unreal Engine, you will have the opportunity to dive into 20+ MLoC of real time C++ goodness. Make sure it's a multiplayer experience for bonus points, eventual consistency makes everything extra exciting.It gives you an appreciation of just how unlikely we're to ever move away from the stuff, short of an LLM innovation that can digest codebases of that size and do an automated port, which I suppose is not outside of the realm of reality these days.
I can't believe I can agree this much with a C++ article. My thoughts precisely. But whenever _I_ brought up the crazy angle-brackets::madness period in language's history, most devs tended to disagree. I rarely follow fads anyway.But there's a downside to C++ as I still see it now: steep learning curve in tooling. I am used to work on legacy projects with long lifespan (mostly infinite, not that I look at it :) ) and kinda got lost when tried to contribute to a modern project (I'd call it a hipster project): the amount of abstractions in build/management tools was terrifying.CMake, for example, is and abstraction over an abstraction above another abstraction. And they used an abstraction over CMake! It also required Python. And not the one available in my OS.I admit, I'm more used to cursed MSVC projects and simple Makefiles. But when CMake starts its dirty work, it's like a black hole of weird scripting and endless pulling of hopefully compatible dependencies you can hardly control (yeah, where does MSVS store that pulled and precompiled garbage? Oh, shi- it's in %userappdata%??? what about other users? What if I want it on a different drive, like MY project?). CMakeLists.txt has a syntax of it's own, and a really obscure one: is it declarative? Are those commands? When does the order of lines matter? Why are CMakes so incompatible? What are those vars? How is any of that online documentation useful? How do I make things work as I want?? And it still doesn't because some required repos have moved, are offline or became incompatible.The infrastructure surrounding my helloworld.cpp was getting SO immense, but it never built as... in the end I was required to upgrade to Windows 10 or 11, which I'd never do, so... This is the sad non-C++ part of C++ programming that I utterly hate.
I recall not being very effective with C++ for years, and then someone recommended the book Large Scale C++ Software Design, and that was a big unlock.I didn’t use 80% of what’s in the book, but just having a comprehensive way of structuring the code was a massive productivity boost. Looking back, I suspect it was less that it was “the right way”, but just that it was “a way” and most of the benefit was it kept me from overthinking and got me to work.Later with C++11, I kept having this thought, “in Python this would be way less verbose”, and I started writing C++ that looked more like Python, creating whatever helper functions Python would have (mostly simple stuff, string handling, etc).That was one of the most productive seasons of programming I ever had, and I still get tempted to write stuff in C++ that Python is better suited for, just because the benefit of not overthinking is that significant (at least for me).
There's too much wrong in this rant to list...
I think we can start with "C++ is not popular enough to attract the weirdos".Presumes C++ is not popular and also popularity attracts weirdos. If anything, weirdos are attracted to languages that are not popular at all. I remember once I was on a small language project and this guy on the mailing list wouldn't stop going on about how our language had to support vorpal math.
A well written blog post that makes many valid points but comes with a wrong assumption: that making a language creativity-friendly is an absolutely desirable trait.It's a desirable trait for my personal projects, where I may use Haskell, Ruby, Ocaml, Racket or something more exotic.At work, I'd rather use languages that are boring, with well defined and uncreative patterns and practices. Professionally I expect to not be surprised often and I want the smallest group cognitive load possible.Languages that breed too much creativity tend to have a rather short list of killer software (that kind of software that makes it worth learn a specific programming language).
I wonder if Zed started using, or went back to, Perl after getting so worked up about the 2 -> 3 transition. (From what I can tell, he's still complaining about it, and still wrong in many of those complaints, and makes unsubstantiated claims in others.)
While it is a great language, ir would profit from less "lets code C with C++ compiler" attitude.Basically it is like renaming those JavaScript files from .js to .ts, and keep coding as if  nothing else is available as productivity and safety improvements.
I feel like an abused spouse after C++. I now avoid:- Inheritance
- Reference counting
- Threading
- Templates
- Classes if possible
- Hidden memory allocation
- Anything that looks cleverAnytime I use them I get flashbacks to some mangled mess of templated threaded classes with some memory leak that shows up after 3 days.I remember writing C++ and trying to figure out how the design would work between these classes, I would end up with something complicated and not entirely correct. Eventually, I thought, what if I did this in C? What would it look like, 90% it turns out with 90% less design and code (and bugs).
There is no rule that requires "programming in C++ proper" being equal with using 100% of the language standard.There is a middle layer, without having to keep repeating all the security flaws of coding in plain C.
You don't have to use any of that and you still get lots of nice things like range based for loops, STL containers, algorithms, namespaces, and constexpr.
Having read the article, nothing really stands out to me as "C with C++ compiler".It talks about ranges, shared/unique pointers, lambdas... Essentially a lot of things C is lacking. I don't know where exacly the overlap you're insinuating comes from.
Where did I mentioned the article?
Pronoun confusion. The second pronoun is ambiguous.Since we are all “hackers” here, I’ll be pedantic…“While it is a great language…”The “it” pronoun clearly refers to the C++ language, as I’m sure you intended.“…ir would profit from less ‘lets code C with C++ compiler ’ attitude.”The “ir” — presumably a typo for “it” — can refer to the article or C++. Given that this thread is about an article, the second “it” referring to the article is a natural assumption.
It with typo refers to C++.
OK, so I admit I also washed my hands of C++ sometime around 2009 and I am being forced back into for <reasons>, and I had no idea what these auto and lambda keywords were.Can anyone point me to a learning reference that will let me jump the meta programming apocalypse and just get to the good stuff?
The "Back to Basics" videos from cppcon are pretty good, IMO.For lambdas:
Back to Basics: Lambdas - Nicolai Josuttis - CppCon 2021https://www.youtube.com/watch?v=IgNUBw3vcO4Back To Basics: Lambda Expressions - Barbara Geller & Ansel Sermersheim - CppCon 2020https://www.youtube.com/watch?v=ZIPNFcw6V9oFor auto, this one is short and summarizes some of the gotachs:C++ Weekly - Ep 287 - Understanding `auto`https://www.youtube.com/watch?v=tn69TCMdYbQ
jump into learncpp and choose where to start from, goodluck
The problem with any language trying to replace C++ for larger codebases is that it's not half as powerful as C++.I've often cursed in C# because something that could be done trivially in C++ if impossible and causes the dev to create convuluted C# while it could be trivialy done in C++ due to its very expressive language features.Those 0.1% of the time that you need those extreme features are what makes or breaks the language in PRODUCTION.
I've had the exact same experience, but opposite. Tons of things that are trivial in C# take a ton of code in C++ to me. Maybe it's just going from being an expert in one language to a newer language?
This often comes from expecting C# to bejustlike C++, where-as more complex use cases are often expected to be done in a (sometimes completely) different way there. It's a good idea to try not to fight the language and workwiththe way it exposes its features.My experience was just like yours - easy to move between C and C#, or Rust and C#. But attempting C++ implementation was always far more difficult. It was never worth it over just spending extra effort in either alternative.If GP reply author has C#-specific questions I'd be happy to answer or point him or her in the right direction. C# is a language with strong systems programming capabilities but has its own learning curve.
What are some examples of these C++ features you've missed in other languages?
Zig not having operator overloading makes it suck horribly for writing any kind of vector code. If everyone had to write int a = int_add(int_mul(3, 7), 2) etc there would rightly be a riot, but since they're not 3D coders they just don't give a shit. Too bad, Zig looks great.
Sorry, one more thing to add to this: Andrew Kelley is obviously a genius, and his talk introducing Zig[0] is in my top 10 of all time technical presentations, for many reasons. But I really do wish someone close to him with a passion for how coding is in many ways applied mathematics, would ask him to please have broader algebraic support for basic operations like +, -, * and maybe divide, with their basic dataflow characteristics. Optimal speed for complex numbers vs std::complex out of the box would be attractive.I understand his point about not wanting to allow every random C++ feature, but in these cases, it isn't a C++ feature, it's language-level basic algebra.In C++ land, ISPC[1] is often what you use when you want top speed rendering perf on SIMD CPUs, e.g. Moonray project[2]Please, just go ahead and define a nice clean API for vectors and scalars like OpenCL provides on its beautiful reference cards:https://www.khronos.org/files/opencl-1-2-quick-reference-car...[0]https://www.youtube.com/watch?v=Gv2I7qTux7g[1]https://ispc.github.io/[2]https://openmoonray.org/Final edit sorry: in the end I love C++ and have been learning Rust mainly out of curiosity. Avoiding C++ quirks one can have few problems and a great time.
Once, I wanted to write a C# function roughly like this:(T1, ..., Tn) Apply<T1, ..., Tn>((Func<P, T1>, ..., Func<P, Tn>) args)This is not possible in C# because the language doesn't have variadic generics. Instead, I used runtime reflection and wrote something like this:object[] Apply(Func<P, object>[] args)Although it worked, the downside is that the types T1, ..., Tn are no longer statically known, which means that the function's contract has to be written in comments and the caller has to check them manually. In contrast, C++ has variadic templates, which would allow the compiler to check the types automatically.
>I've often cursed in C# because something that could be done trivially in C++e.g?
That claim certainly warrants a serious example.  Perhaps you mean "something ... requiring maximum performance ..."?
A bold statement from Ken Thompson on C++:https://m.youtube.com/watch?v=c-P5R0aMylM
I'm gonna be weird and not engage in language war (I love c++)but, dear author:> +95% of the compiler errorsthis. this is unforgivable+X means "additional X". As in "I have Y and I add +X to it". When you are invited to an event, your invite states you can take your +1 with you (one more person that will come within the same invitation)if you want to say "more than", you use X+ !! as in "95%+". Because you have some X amount, and you add some more to get X+...get it together. you're awesome
I think I will take another look. I recently helped my grandson with his university C++ assignment. It was a lot better than I remember C++ in the 90s.Fun wise though, I get the most fun with dynamic languages that are highly interactive like Forth, Common Lisp and Smalltalk. I don’t have to drop out of the zone to kick the compiler all the time.
> I recently helped my grandson with his university C++ assignment.That is awesome. I hope if my kids have kids i'll be of some technical use to them too!
> Need package management? Check out Conan, Meson's WrapDB, and vcpkg.No, just no. All of them are complete, utter crap that doesn’t hold a candle to languages that were designed with packages in mind. We’re using Conan at work, I’ve been using vcpkg at home and I loathe both.> I mean, do you really think Python's package management is top notch? You do? Why are there like 10 package managers then?Worst Python package manager runs circles around whatever C++ offers.
I don't think we need to pretend Python package management is good to contrast C++, they're both awful for different reasons.At least there's maybe a way out for Python (uv / the various PIPs); C++ doesn't appear to have any kind of plan whatsoever, outside of gesturing in the direction of modules.
what's wrong with vcpkg? looks good to me
Question for pros: I'm a Python dev who recently started diving into C++, hoping to build plugins for DCCs. However, every time I read these threads, I feel as if I'm on the completely wrong path. Positive views like the OP's are rare.While I understand that not everyone gets to work on a John Carmack level codebase, is working on a C++ project really as challenging and unrewarding as it seems? Is the productivity that low, and do every step feel so difficult that they eventually drive developers away?
I've had some good times in C++. But for everything that's been thrown into it, I can't believe we're still dealing with header files. That was one of the greatest things about moving to Swift: no more of that BS.But with SwiftUI, Swift has also become "unfun." SwiftUI and Apple's half-assed, broken observation and "reactive" paradigm have made programming a joyless slog.
C++20 has modules, which replace header files completely (unless you use old libraries which aren't available as modules yet). Compiler support is there, but unfortunately IDEs are lagging. If you use modules with Visual Studio, say goodbye to IntelliSense. Maybe they'll iron out the bugs in a couple years...
C++20 that is not complete in 2025. I don’t know how C++ developers can work in this with a straight face.
You probably have to think this way: Anything added to C++ will only get to be used in for real for real in 10 years minimum.
They needed the most powerful, most flexible module system ever, so it might take decades to really become useable. Adoption has been painfully slow so far, it's insane complexity really doesn't help.
Wow, that's ridiculous.I wonder if Xcode does any better with them. Now that would be something.
C++ has modules for small values of "modules":https://arewemodulesyet.orgIt's a bit tongue in cheek but: "Are we modules yet? Nope. ... Estimated finish by: Wed Sep 20 2541"
You can use modules to structure your own codebase. No more need to write headers and think about how to structure your code in terms of compilation units. But yeah, your link shows that practically none of the popular libraries (except STL) can be imported as modules today.
I did C and C++, then moved on to Objective-C and Swift. I recently switched back to C++, after getting tired of Apple’s shit treatment of developers. I also have no interest in learning SwiftUI.Having to define header files in C++ is pretty annoying after doing Swift for many years.
> I've had some good times in C++. But for everything that's been thrown into it, I can't believe we're still dealing with header files.There is nothing wrong with header files. In fact, there is no such thing as a header file specified in C++. There are only forward declarations and definitions, and how those can be managed by developers.Meaning, any talk about header files is a discussion on software engineering practices, and how developers create or avoid their own problems.
"There are only forward declarations and definitions, and how those can be managed by developers."Why do we need to manage them?
Because nobody has figured out a way to automate the process of generating declarations from definitions.
https://github.com/rogual/csaw
Why do developers in other languages not have to deal with it?
> Why do developers in other languages not have to deal with it?They do, except they don't have the bandwagon effect motivating them to complain about other solved problems.
Enjoy your echo chamber.
> Enjoy your echo chamber.Is that supposed to mean anything at all?
For the love of God when will c++ compilers finally be able to output template errors that aren't completely expanded and are written in terms of the user's typedefs? Most of the time I spend parsing template errors with boost is just to figure out what the hell is being complained about.
Weren’t concepts supposed to fix this? Apparently they made it into the 2020 standard. I haven’t touched the language in many years - did they not help?
Concepts did not actually make it into the standard. I vaguely recall they were cut at the last minute or something.
Concepts are in C++20. I don't know the specifics but it's my understanding that the version we got is stripped down in comparison to the original proposal.
Oh, woops. Maybe they I only updated my belief before 2020. OP is right about one thing, cppreference is great.https://en.cppreference.com/w/cpp/language/constraints
I have found LLMs are a great tool for metaprogramming. I think the template error problem has been wanting for a sufficiently advanced compiler, and that's what I see LLMs as being. ChatGPT has been a great help in debugging programs I've written in C++ templates, both in generating the template code and trying to decipher errors generated, leading to suggestions for the template code rather than the expanded syntax.
Yeah, totally. I find LLMs are very useful for doing stuff with the preprocessor, too. ChatGPT taught me how to use boost preprocessor (BOOST_PP_FOR_EACH_PRODUCT).Still though, I want to see MyMapType::value_type in compiler errors rather than... Well, you know. It's going to contain the type of the key, the type of the value, the type of the allocator, just when all you want to really know is that it's a pair<key, value>, which I think most people know of as My map type::value_type.
I've had a similar situation with Kotlin. I've always been a java developer, and I enjoy using it, but even with the newer features it's just...slow.When I had to script things I chose JavaScript (native JavaScript) since it's way faster to iterate, but I've always missed the static typing (I also know python, but I honestly prefer JavaScript)Until I learned Kotlin. It's been a blast to use, incredible common libraries, streams everywhere, nulls that you can use without issues...I just love it (so much in fact that I'm in the process of switching project from java to Kotlin).When I need to do scripts, like for the advent-of-code, I choose Kotlin.
I also really enjoyed the jump into Kotlin as well! That said, I'm also very happy with recent java versions  trying very hard to close the gap. It does feel like at least half of Java's recent major lifts have been to emulate things done well in other languages (a great attribute for a self-aware language developer). I definitely never appreciated a lot of their newer feature choises like records until I saw them working really well in Kotlin data classes.
The amount of high performance, production grade, massively tested libraries written in C++ is unbeatable. I will be honest here, it's easier to improve C++ security by implementing a compiler that produces safer C++ (like Typescript to Javascript) than rewriting everything in any other language (Rust, Zig, Odin, whatever).I mean, could you estimate the cost ($ and time) it would take to rewrite the best audio framework in any other language? (https://juce.com/).
Zig is great, it’s not aiming to be a replacement for C++ though. One of the awesome things about Zig is its interoperability with C. In that sense it’s more of a Typescript to C than Rust is to C++. I’m still not sure what I think about Rust personally. In my region of the world I suspect it’ll continue to struggle to find any form of adoption as the C++ people seem to have very little interest in trading their decade long experience for Rust. Zig on the other hand is winning the hearts and minds of most C programmers I know.I hope Rust succeeds though. I say this more from a change management perspective than anything else. It’s extremely hard for us to find developers who will primarily work with garbage collected languages but occasionally have to work with either C or C++ when bottle necks appear. Rust makes that much easier, or perhaps less dangerous would be a better term. I’m not sure any of the attempts at making C++ more safe to use is going to really succeed in this regard. Maybe, but I nothing within the C++ community seems to pull in that direction so I doubt it. I’d like to mention that I’m aware that Zig isn’t helpful in this regard either as it’s not memory safe.
Zig's interoperability with C really is a stroke of genius. That and the fact that it tries to focus on simplicity with no hidden control flow. It's a fine line to walk, because there's always a tradeoff between abstracting things away to make the code more readable, and tucking potentially critical flaws away out of sight. But so many of C++'s edges come from that hidden control flow, and quirks around things like initialization, ownership, and even things like assignment that less experienced programmers take for granted (if they've ever thought about it at all). And I say that as someone who uses C++17 both professionally and privately, and enjoys the language.
How many total lines of code do you imagine are in these libraries, compared toall their clients? If rewriting the libraries sounds like an unreasonable amount of work in a world where all that client code exists, doesn't that reflect negatively on the readability of C++?
Agree. Sure there are problems here and there but I think that overall modern C++ is likely the most versatile tool in the "compiled to native, can do anything" family
Every article like this I scan to see if the author had previous C++ experience.  And every article, they do.I will be very impressed and curious if I find a glowing article about C++ from someone who didn’t grow up knowing it as a smaller, simpler language.The C++ community needs enthusiastic converts who didn’t do it back in the 2000s if it’s going to stay relevant.
I learned c++ after c++20 and after several attempts to enjoy rust, c#, go, and C, I always come back to c++ as the most enjoyable language to develop.
Nice!  If you blog at all your perspective would be super interesting to hear.
C++ is fun, but "Rust weirdos" are right. Don't use C++, use Rust. That's all.
The stuff you use for work is not fun. The stuff you use for fun is fun. There, I solved the mystery for you.If you choose technology for work by what is the most fun - you enter a hedonist treadmill. Stop. JS framework insanity lies that way. No cool technology will save you from burnout.
I have the luxury of only writing software for myself, which is almost always Python, but after reading this ill go back and have another look at C++. Did one big project in C++ after uni but then never touched it again.
It takes some getting used to. I wrote it for about 10 years then didn't touch it for 10 years and now I write it full time again. Took a few months to get up speed again and stop shooting myself in the foot.
I plead the 10thhttps://en.m.wikipedia.org/wiki/Greenspun%27s_tenth_rule
The metaprogramming metapocolypse resonated with me.  In the late 1990's the entire software world got "pattern disease".  Yikes. We are still paying the price.
> Well, actually the Rust weirdos will bother you but they're usually too busy making the borrow checker happy to actually get anything done so you can ignore them.Ouch...
Can anyone recommend some good “modern” C++ books in this spirit (writing software for fun, not large enterprises)?Preferably for Linux and/or Windows.
Not sure about what you mean by "fun" but Peter Gottschling'sDiscovering Modern C++ An Intensive Course for Scientists, Engineers and Programmersis pretty good. For a catalog of shiny new C++ features Marius Bancila'sModern C++ Programming Cookbookis comprehensive.
As per usual with c++ it depends. In some situations things are guaranteed to be compile time evaluated. E.g. even in early templates this would need to work:Func<1+2>();Something like this there's no guarantees around:template <A, B>
int Func() {
return A + B;
}For almost all compilers it should constant fold this into a constant but in theory it could end up with an add instruction. Basically we can't second guess the author here because it depends on the specifics.
C++ is great, but I hate .h files.. I'm kind of shocked that some tool to automate writing .h files isnt the the standard.
I’m not for sure auto is an improvement. I know it is required for lambdas and it makes it easier to type out a very verbose type, but it really does reduce code readability.I’ve even seen developers use it instead of bool, which is pretty laughable as the they are the same number of characters.
A verbose enough type - and C++ has plenty of those - is indistinguishable from line noise.There are places where having an explicit type annotation can improve readability, places where it harms readability, places where it doesn’t make much difference one way or another. Giving us the option has been a blessing. All programming calls for good taste, C++ programming calls for it more than most.
> but it really does reduce code readability.How about not specifying the type, and letting the compiler infer it correctly and error out when it cannot - like so many other languages do? And those languages are muchstricterabout types than C++.And autoreducingcode readability? Having to figure out the intricacies of a detailed type to write was a huge barrier, and virtually anyone reading the code with a type involving several nested angle brackets would not bother mentally parsing it anyway.
I think it does reduce readability in some scenarios.For instance:
    const auto& processes = 
    getCurrentlyRunningProcesses();
for (const auto& process: processes) {
  // Ok, what do I do with process now? Is it a pair from a map? A struct from a vector?
  // If it's a pair from a map, is the key the pid, a unique id, something else?
}std::unordered_map<Pid, ProcessData> is more readable than auto here IMO: you don't need to open the definition (or hope your IDE correctly display the type).
Auto is an improvement for C++ only because of its uniquely unergonomic type system and standard library. I'd very much prefer writing something like `iterator<auto>` instead of `auto` or `std::map<lotsofchars>::iterator` and not be told by every linter to change most explicit type declarations to `auto`.
I remember reading something here recently about auto causing some painful and difficult to diagnose bug - I think string was what they thought the type should be (and some implicit cast would have made it a string if the type was specified)... but instead it created a string_view which went on to be used somewhere that accepted both string and string_view and then something tried to use it later but whatever the string_view was pointing to was gone (or something in that vein - I don't recall exactly).
It's probably auto when you wanted auto&.A copy was made instead of a reference. I've been bitten by that.
Is anyone still using the Boost libraries?
Yes! Boost.Asio is still the go-to networking library. (It's also available as a standalone, though.)Some parts have been adopted by the C++ standard library and can thus be considered obsolete, there a still quite a few goodies!
Yes, it gives me free coffee breaks every time I compile my code
Shout-out to the Rust users rolling their eyes in this thread, but keeping their mouth shut out of fear of being labeled as a Rust evangelist or something. I find these discussions hard to take very seriously. Even in this thread some people nodding at Rust are implying that C++ is easily/regularly faster which is really a take, given nearly every single RIIR example/benchmarking I've seen so far.
There were 91 occurrences of the word "rust" in 264 comments so far.
Only 7 instances of “footgun” (or a variation) though. HN is slacking - must be the holidays
I really love how politically incorrect this post is. And how honest and fresh it sounds, though quite a few have been quick to say how wrong he is. They could be right, but I think it misses the point.Yes, it's way too easy to do dumb stuff in C++, when you're tired or not sure what you're doing. Things like holding raw pointers or references to things you shouldn't like std::vector::data(), or questionable reinterpret casts and many other things. The compiler won't stop you, only your experience.But he's right about one thing at least: Programming should be fun!All these layers and rules and concerns about memory safety and security don't offer only advantages. They also have tradeoffs. And it's the same thing with those scrum agile ceremonies. It serves its purposes. But it's also the best invention ever to suck all the joy out of programming.I think that both C and C++ still have that fun feeling going for them. When you know what you want to work on and how to do it and you just start doing it and get into the flow. And if you're careful and do things right, it just works and it's a blast!That's my takeaway from the article. That feeling like you're talking directly to the machine, getting it to show you on screen what you saw in your mind, without anything else getting in your way. Now, that is fun!
Programming should be fun. I don't think that's controversial or politically incorrect. (Or if it is, I don't know why.)A lot of people don't naturally have the kind of fun in C++ that Zed describes, and it seems most people here (including myself) would rather talk about that.
> Programming should be fun.Alan Perlis quote: "I think that it's extraordinarily important that we in computer science keep fun in computing. When it started out, it was an awful lot of fun. Of course, the paying customers got shafted every now and then, and after a while we began to take their complaints seriously. We began to feel as if we really were responsible for the successful, error-free perfect use of these machines. I don't think we are. I think we're responsible for stretching them, setting them off in new directions, and keeping fun in the house. I hope the field of computer science never loses its sense of fun. Above all, I hope we don't become missionaries. Don't feel as if you're Bible salesmen. The world has too many of those already. What you know about computing other people will learn. Don't feel as if the key to successful computing is only in your hands. What's in your hands, I think and hope, is intelligence: the ability to see the machine as more than when you were first led up to it, that you can make it more."- "Quoted in The Structure and Interpretation of Computer Programs by Hal Abelson, Gerald Jay Sussman and Julie Sussman (McGraw-Hill, 2nd edition, 1996)" viahttps://en.wikiquote.org/wiki/Alan_Perlis(Perhaps relevant to this article/overall thread: "Programmers should never be satisfied with languages which permit them to program everything, but to program nothing of interest easily.")
It's controversial because of how he makes fun about the people using and promoting other languages, like Rust. I thought it was hilarious.And in case you haven't noticed, C++ isn't seen in a good light anymore for some years now. There are a lot of loud voices saying its time has past and calling for it to be replaced with something newer and better.It's all subjective anyway, but I resonate with the feeling of fun he describes when doing projects in C++. Feeling productive and being protected from whole classes of bugs common in C++ is all well and good, but he was talking about programming being fun and I do not get that same feeling when programming in other languages.It's perfectly understandable that you don't feel the same way though. I hope you do when programming in your favourite language. Otherwise it becomes just something you do to pay the rent.
> It's controversial because of how he makes fun about the people using and promoting other languages, like Rust. I thought it was hilarious.But doesn't that completely ruin the point of the post? I agree with you that something feeling 'fun' is more personal, and that the criteria of what constitutes fun are up to the user. The author doesn't agree with that - you can either adopt the former point or promote the Right Way of having fun. Those snarky remarks made me put this blog into the second category. When you're so invested in your argument, even a fundamentally harmless post about having fun will get that language wars hit piece subtext.If anything, they seem more like desperate cheap shots than arguments. Other people, the NSA etc dislike unsafe-by-default code? Well, they're just authoritarian anti-fun ideologues! Rust users bring up some of the same criticisms I recall in the last paragraph? Well.. uh... that borrow checker, am I right?
I think he was just making fun of people who inflate the importance of their language and their way too much and it should be taken in that spirit. There are good reasons why all these new languages like Rust and Zig popped up in the last 10 years or so and started getting traction. Obviously a lot of people were unhappy having C/C++ as their only choice for performance focused or system level programming. At least in the games business it still seems to be doing well.But to get back to the point of the article, for fun solo projects, when the 'mood for coding' comes over, I may be biased, but I think C++ is still the best. It's like when building a prototype. You just want to test your idea and see how it looks and play with it and just worry about bugs and program correctness later. While coding it in Rust you'd have to spend extra time determining the correct memory ownership relations and that can break the flow.After programming for 20 years, it doesn't come nearly as often as it used to, but I still get that feeling from time to time.
Zed is definitely the sort of person who can make an uncontroversial point sound controversial, yeah. I think we're in violent agreement here.
I was smiling throughout : )(it is a good smile)
> C++ Is An Absolute BlastNo, it's one of the worst languages I ever used. Tons of footguns and bad design choices everywhere. Too much cognitive load for less benefit than other languages.I'm surprised the article didn't mention <iostream>. The f.fail(), f.eof(), f.flags() are confusing and verbose. Even something as simple as f.read() doesn't return the number of elements read, so you need to make a separate call to f.gcount(). And then there are all the opaque types like std::streamsize, std::mbstate_t, etc., where you have no idea how their sizes relate to language types like int/long/etc. or fixed-width types like int32_t/uint64_t/etc.https://en.cppreference.com/w/cpp/string/char_traitsAnd then there are the redundancies. int x = 0; int x(0); int x{0}; all roughly do the same things but have subtle differences in more advanced use cases. This recent thread (https://codereview.stackexchange.com/questions/294784/c20-ro...) reminded me that `typedef` got replaced by `using`. A while ago, I came up with a long list of near-duplicate features:https://www.nayuki.io/page/near-duplicate-features-of-cplusp...> JavaScript still can't even figure out what a for-loop isECMAScript 6 added the for-of loop, which is the more useful alternative to the for-in loop.> C++ has lambda, and it's not bullshit like Python's lambdaC++ lambdas have a heavier syntax than any other lambda I know of (e.g. Python, Java, JavaScript, Haskell, Rust), because it needs to specify attributes and captures.https://en.cppreference.com/w/cpp/language/lambda> My thinking is C++ is now about as good as any other language out thereNot by a longshot. Instead of C++, I reach for Java if I want fast design time, safe operations, and a more limited set of tools (e.g. not needing to decide how many layers of pointer indirection I want). I reach for Rust if I want the power of C++ without its footguns.Heck, my motto for Rust has always been, "C++ done right". Every time I compare analogous features in C++ and Rust, I find that the Rust version is much better designed. As the simplest example, in Rust it's a compile-time error to use a variable whose value is moved out, whereas in C++ the variable is still usable but has an invalid value. Another example is that Rust has traits but C++ relies on instantiating templates and then "duck-typing" to see if the resulting code can actually compile. And let's not forget nullptr, the trillion-dollar mistake - C++ makes nullptr implicitly part of every pointer(*) type, but Rust bans it by default unless you opt in with Option<T>. Rust has other quality-of-life features such as easily declared tuple types, the unit type instead of void (which makes functional programming easier as you don't have to special-case void), pattern matching and unpacking, methods on primitive types (e.g. 456u32.isqrt() instead of sqrt(456)). I just can't look at C++ seriously when Rust is miles ahead, being more expressive and safer.> The Amazing Comeback of C++11I will agree with this in a limited sense When I write C++ code (because I'm a masochist), I will not tolerate anything less than C++11, because C++03 and C++98 are much, much worse. I'm talking about things like various types, standard library classes/functions, unique_ptr, and move semantics.
I'm mostly very much in agreement with what you've said here but I want to pick on a few things:> Instead of C++, I reach for Java if I want fast design time, safe operations, and a more limited set of tools (e.g. not needing to decide how many layers of pointer indirection I want).I don't think I've ever seen a good reason to prefer Java over C# for anything.> Another example is that Rust has traits but C++ relies on instantiating templates and then "duck-typing" to see if the resulting code can actually compileIs thehttps://en.cppreference.com/w/cpp/header/type_traitsfunctionality not sufficient for what you have in mind?>the unit type instead of void (which makes functional programming easier as you don't have to special-case void)Why would special-casing be necessary? You don't need to say e.g. that mapping a void-returning function produces an empty result; it could just be a compile error. I feel like void returnsshould bea special case and I don't like all the ways `None` is used in Python, because it's one of the few things that blurs an otherwise very strong distinction between statements and expressions, analogously between commands and queries.
> in Rust it's a compile-time error to use a variable whose value is moved out, whereas in C++ the variable is still usable but has an invalid valueC++ does it this way because there are common cases in systems code where doing it the Rust way would literally be unsafe. Not all memory references are visible at compile-time and may exist outside the address space.
Would you mind elaborating more on those common cases? I'm not sure I've heard of destructive moves being less safe than non-destructive moves and I'm not smart enough to figure out what you're talking about in your second sentence.
Shared address space. Some other process or silicon can read or write the object you just moved but doesn’t know you moved it. You need to keep the memory previously occupied by the moved object valid long enough for those references to realize you moved it to prevent corruption.A typical case is high-performance I/O, which uses a lot of DMA. DMA is oblivious to most programming language semantics like lifetimes, ownership, etc and will happily step all over your address space if you aren’t careful.
I'm curious to hear more about this use case. The DMA I do in rust is generally static buffers, because I'm not sure how to pass the borrow checker otherwise. (There are ways). Generally, you set up a static [u8] buffer, and pass its pointer to the hardware that's doing the DMAing. Then magic, then the buffer gets read or written by the hardware. In this scenario, the variables never go out of scope. Am I cheating, and avoiding this issue by using static buffers? If the buffer drops during a DMA transfer, I believe UB happens.I'm suspicious a similar principle happens with memory-mapped flash memory as well, e.g. QSPI.
Thanks for taking the time to elaborate!> Some other process or silicon can read or write the object you just moved but doesn’t know you moved it.That should primarily affect buffers that are inline with the moved object, right? i.e., not static buffers or stuff that's heap-allocated? How common is that scenario? I admittedly generally thought DMA used static buffers, though to be fair I'm not exactly highly experienced in the space.> You need to keep the memory previously occupied by the moved object valid long enough for those references to realize you moved it to prevent corruption.How is this (reliably) handled in C++? I feel there's gotta be more than just hoping the empty object hangs out long enough for the rest of the system to catch on (e.g., moving things around near the end of a scope when the empty object will be destroyed "soon").
This just means that affine types aren't the right tool to model memory that you don't have full control over. Which is true, but also represents a very small subset of overall data. Rust provides you with other tools to handle those kinds of situations.There is a small wart here, which is that (with async Rust) some of these use cases would benefit tremendously from full-fledged linear types, or at least an easy way to run code during async cancellation.The difference between an affine and a linear type is that the ways in which a linear type is consumed are controllable through encapsulation — for example, imagine you have a type which represents a certain amount of money, and you want to statically prevent the money from being dropped on the floor. Affine types don't prevent that statically, but linear types do. You can still have runtime checks though.
I don't think C++ is one of the worst languages; there are very few languages as powerful as C++, that alone makes it one of the best.But, much like love and hate, I also don't think that the opposite of good is always necessarily bad, nor vice-versa. A language can be both good and bad at the same time, in different aspects.C++ is really good (unrestrained freedom, performance, ecosystem), and also really bad (tooling, templates, really hard to debug memory issues).Rust is somewhat less good (less free, slower, puny ecosystem in comparison), but also alotless bad (powerful type system, thread safety, fearless iterators/lambdas, etc).Many of the warts C++ has to carry due to its commitment to compatibility, are fixed in Rust with much better alternatives. A lot of footguns are well encapsulated in Rust's affine-ish types and algebraic data types, while still providing unsafe hatches for when you need them. Defaults really matter.
> No one had to hold a gun to anyone's head to get them to adopt it. How do you rationalize that if your opinion had any substance or merit?We did. It was either C or C++ that were supported by our hardware vendor.> For the sake of argument, I assert exactly the opposite: C++ post-C++11 is the absolute best language ever devised by mankind, bar none. Am I wrong?Absolutely. It is one of the most complex and error prone languages out there.
Lambdas that can borrow from the stack and still return. Absolutely unacceptable feature without a borrow checker.
I was C++ dev for 5 or 6 years, up to the late 2000s.I got another C++ job about 3 years ago but bailed after about a year.I could write a tome about what I dislike but to start with, any language that lacks a working standard built-in string type, is just a hard no for me at this stage in my life.  Life is just too short.The tooling and IDE support is atrocious, no standard dependency management for 3rd party libraries and CMake makes maven look well designed.I tried to pull my knowledge up to date.  Hmmm, we used to have lvalues and rvalues, what's this prvalue thing?Surely cppreference can explain:>a prvalue (“pure” rvalue) is an expression whose evaluation>- computes the value of an operand of a built-in operator (such prvalue has no result object), or>- initializes an object (such prvalue is said to have a result object).> *  The result object may be a variable, an object created by new-expression, a temporary created by temporary materialization, or a member thereof. Note that non-void discarded expressions have a result object (the materialized temporary). Also, every class and array prvalue has a result object except when it is the operand of decltype;*>The following expressions are prvalue expressions:>a literal (except for string literal), such as 42, true or nullptr;>a function call or an overloaded operator expression, whose return type is non-reference, such as str.substr(1, 2), str1 + str2, or it++;>a++ and a--, the built-in post-increment and post-decrement expressions;>a + b, a % b, a & b, a << b, and all other built-in arithmetic expressions;>a && b, a || b, !a, the built-in logical expressions;>a < b, a == b, a >= b, and all other built-in comparison expressions;>&a, the built-in address-of expression;>a.m, the member of object expression, where m is a member enumerator or a non-static member function[2];>p->m, the built-in member of pointer expression, where m is a member enumerator or a non-static member function[2];>a.*mp, the pointer to member of object expression, where mp is a pointer to member function[2];>p->*mp, the built-in pointer to member of pointer expression, where mp is a pointer to member function[2];>a, b, the built-in comma expression, where b is an prvalue;>a ? b : c, the ternary conditional expression for certain b and c (see definition for detail);>a cast expression to non-reference type, such as static_cast<double>(x), std::string{}, or (int)42;>the this pointer;>an enumerator;>a non-type template parameter of a scalar type;>a lambda expression, such as [](int x){ return x * x; };>(since C++11)>a requires-expression, such as requires (T i) { typename T::type; };>a specialization of a concept, such as std::equality_comparable<int>.>(since C++20)>Properties:>Same as rvalue (below).>A prvalue cannot be polymorphic: the dynamic type of the object it denotes is always the type of the expression.>A non-class non-array prvalue cannot be cv-qualified, unless it is materialized in order to be bound to a reference to a cv-qualified type(since C++17). (Note: a function call or cast expression may result in a prvalue of non-class cv-qualified type, but the cv-qualifier is generally immediately stripped out.)>A prvalue cannot have incomplete type (except for type void, see below, or when used in decltype specifier).>A prvalue cannot have abstract class type or an array thereof.Yeah, this language is loads of fun.  I've worked on compilers, interpreters, implemented extended Hindley-Milner type systems, etc. so normally love reading formal language specs but this is just insane.
> any language that lacks a working standard built-in string type, is just a hard no for me at this stage in my life.Um, std::string is a thing...
Yes it's a thing - a worse than useless thing.A wafer thin wrapper around an array of bytes using null termination - a model of "strings" which is effectively a computer technology fossil - nearly 60 years old at this stage.[1]It contains no specified or implied encoding - so there is way to actually interpret the data as characters which I guess wasn't a problem in the age before computer networking - your machine has an encoding built in and that was how you interpreted bytes as characters.A representation that, to save a byte or two at the header, means that determining the length of the string is an O(n) operation.It's an abstraction so leaky, it's hard to see the advantage over const char* and the leaks can never be plugged given it's part of the spec that c_str() must run in constant time.It's basically a dangling pointer generator with no unicode or any sort of internationalization support.But why provide a usable string class (never mind any sort of usable date/timestampe representations) when the language designers can spend years to add a whole new layer of absolutely useless complexity to the language - like concepts, for example.Nah - life is too short.[1]https://en.wikipedia.org/wiki/B_(programming_language)
>But, C++ kept evolving, and the standards committee seemed to realize that if they don't do something they'll become an obscure language only used by a single 10 trillion dollar industry. How pathetic!Which industry is this referring to?
My guess is that it's the game industry, because it's probably worth that much and is almost pure C++.
The game engine industry is pure C++. Most games are written in C# just because of Unity
>shame griftersLove this. This is an old post though right, still talking about C+11?
> It is easy to see that C++ is fit as a general-purpose programming language–adoption by millions is a testament to that.I really wish the std would drop this pretense and focus on C++'s strong point: Continue being the fastest systems language possible.Everywhere in the std lib you can see compromises that require rewriting substantial portions for any real time application.Things like: shared_ptr eagerly using atomics whenever possible; std::string allocating; the lack of built-in faster std::allocator replacements like bump allocators, memory pools, etc; no lockless and wait free concurrency primitives; no architecture-aware thread pools or even architecture descriptions; no IPC primitives; etc.Considering how many C++ developers are working on things like games, high performance server applications, databases, and operating systems, it's just bizarre how inappropriate the standard headers are for these tasks.Even something trivial like casting bytes off the wire to a packed struct is an exercise in frustration due to aliasing rules that should have been encoded in the type system and invisible to the user.
> Continue being the fastest systems language possible.If you want to be pedantic -- intheoryC++ can never be the fastest systems language possible because of the language's rules about aliasing.You'd need to smatter "_restrict" everywhere to skirt this.
This is not pedantic at all. Aliasing issues are really subtle and hinder so many optimizations. It's not clear that there is a way to even fix this without substantially breaking backwards compatibility.
Yeah the number of cases where otherwise really trivial optimizations are prevented because of soundness concerns regarding aliasing is simply enormous.
> If you want to be pedantic -- in theory C++ can never be the fastest systems language possible because of the language's rules about aliasing.True but the only practical competitor is Rust, and they gain some alias information (mut) and lose other aliasing information (type punning is fully allowed in unsafe code all the time).
Not so, unsafe code still has restrictions in Rust due to pointer provenance.  You can override these restrictions but it's very much an explicit operation, the default is that pointers with incompatible provenance will not alias.  This is how Rust models its equivalent to TBAA, and the concept is spreading to C/C++ as well.
> Not so, unsafe code still has restrictions in Rust due to pointer provenance.That's separate from what I'm referring to. In C++ a float* and an int* can never alias. In Rust f32* and u32* are allowed to. Meaning in a situation where whether they can alias can't be established by provenance (e.g. in a separately compiled function where the compiler at compilation time can't tell where the float and int came from) then C++ is able to use types to rule out aliasing, but Rust cannot.
Your specific gripes seem reasonable (I suspect design-by-committee plays a big part), but:> I really wish the std would drop this pretense and focus on C++'s strong point: Continue being the fastest systems language possible.You've essentially described C, not C++. C++ has a different philosophy and makes different trade-offs.C++ is at least still pretty committed to theyou only pay for what you useprinciple. As far as I know RTTI is the only real exception (corrections welcome), but even RTTI can be disabled in many compilers.Some of your gripes with the standard library can be addressed with libraries, perhaps from the Boost project.
> You've essentially described CI'd disagree pretty strongly with that. C is more focused on being relatively simple to implement and backwards compatibility with the past 50 years. (I read a blog post by a C committee member talking about that recently, wish I could find the link)Just look at the garbage fire which is the standard library. qsort. strtok. rand.C++ should (in theory at least) be able to match or surpass C for _any_ performance benchmark, because it simply gives you more tools in your toolbox. For example, C is never going to be able to beat std::sort because it can't monomorphize in the compare function.I'm not saying C++ is perfect either (looking at you unordered_map and regex). But I am saying people look at C with rose colored glasses.
Yeah, I think a degree of complexity is required to achieve being the fastest compiled language.To achieve that, you need stuff like monomorphized templates, actual arrays and slices being distinguished from pointers, well-defined rules on pointers (like Rust's rules about references and mutable references, or strict aliasing in C++), and the freedom to let the compiler reorder structure fields for better alignment and space usage. Runtime polymorphism, like virtual functions in C++, are better as a core language feature rather than something implemented as a struct of function pointers, because that way the compiler can devirtualize some calls.All of these things, I think most C programmers would be against adding to C. And pointer rules are historically controversial; for example, Dennis Ritchie was against adding noalias to C in 1988[1].[1]:https://www.yodaiken.com/2021/03/19/dennis-ritchie-on-alias-...
No, C is not the same as it is less expressive.C++ templates allow for declarative nested inlining into a single compilation unit that is extremely difficult to achieve with C macros.See elsewhere in this thread for discussion of boost, it's not applicable.
In practice the Sufficiently Smart Compiler will have a hard time optimizing all these layers.C macros are way simpler as they are only text. I still use them alot in C++.
As someone who reads C++ disassembly/decompilation for a living... Yeah kinda. It depends.If the target code is full of virtual function calls, yeah it gets gross. But if you need dynamic dispatch, C isn't going to be any better, you'll just be reinventing vtables by hand. Similarly, resource acquisition and release happens in C too, you just have to do it by hand instead of letting the destructor do it for you.One ultra gross thing I see all the time in C++ disassembly though: constantly creating copies of a std::string, using them once for a comparison or something trivial, and then throwing them away. Multiple times in the same function. Its the developers fault, they shouldn't be creating new objects, they should be passing a pointer or a string_view or something. Unfortunately, C++ is copy rather than move by default, so its too easy to do this by accident.Another gross thing? (Not C++ specific) 20 functions in a row that are just a single return instruction. Since functions have to have distinct addresses (that's my understanding at least) the compiler/linker can't easily fold identical functions together. Implementations can optimize as long as everything still works (the "as-if" rule). And I know some compilers do. But in practice, it seems they suck at it. So I get to look at 20 lines of assembly in a row that are just `bl`.
> In this thread: all the people that have never looked at C++ disassembly. Maybe the C++ "language server" of your choice should have a mode where it prints all the constructors destructors copy constructors and what not on top of your code?That tool is called Godbolt at gcc.godbolt.org. This is common enough that Godbolt is a verb with a lot of C++ programmers.
> C++ is at least still pretty committed to the you only pay for what you use principle. As far as I know RTTI is the only real exception (corrections welcome), but even RTTI can be disabled in many compilers.Compile time would also be an exception.
I had a program that had to parse a lot of integers like "12345".  The std way to do it was an order of magnitude slower than writing my own simple parsing code.  I have no idea what the std version was doing, but it was crazy to see my program's execution time (measured in hours) dominated by int parsing.  The handwritten version eliminated that.
You didn't say which std function you were using that was slow. But std::from_chars performs a lot faster than std::stoi et al due to many reasons.
Yeah I don't remember what I originally used.  It might have been stoi but it might have been iostream.
Even there, you can use a subset of C++ and get pretty good compile times. Worse than C, as you're still dragging around the full weight of a C++ compiler, but still.
From my understanding exceptions are also an exception, adding overhead even when not used.
the only pay what you use thing is bullshit. because everything you want to use actually costs something. the point being that a better implementation would give you the same things without a cost...
"only pay for what you use" means that you do not pay for things you do not use. You are absolutely correct that everything has a cost, but that is not what this slogan is about. The "only" is load-bearing.
Why does this have to be part of the standard library? Isn't that the whole point of Boost, that one is not limited to what's in std?
It depends on the contributors, but boost also almost always fails to consider performance implications.For me it mostly looks like a reject pile for a stdlib that I must also reject most of the time.The alignment of incentives between these library authors and people that use C++ for high performance applications is just off.
> For me it mostly looks like a reject pile for a stdlib that I must also reject most of the time.It’s also the opposite: a place where possible candidates for the standard get tried out first.But like you, I rarely use it.
Boost has its uses and in many cases it's an improvement over the standard library, but performance isn't one of its key properties. Some parts of boost are about as fast as they can be, but that's the exception.
I have never once seen Boost seriously considered in performance environments.
Even if somebody put such a thing, or two, in the standard library there would be again someone showing that they managed to come up with a better solution. For their workload. And under constraints that are only valid under their use-case and constraints that only they would know. Not standard library developers. And that's the thing - the premise that "one size fits all" even exists is wrong. I used to think that way as well but generic solutions to high-performance algorithms do not really exist.
It was my understanding that one can easily add their own allocator to any STL container, though as you point out you have to write that allocator yourself
Allocators have an inherently broken design, tying them to types. See this CppCon talk by the (engaging and funny) Andrei Alexandrescu:https://www.youtube.com/watch?v=LIb3L4vKZ7Ustd::allocator is to allocation what std::vector is to vexation (or: designing allocators that don't not-work)
If the allocator is unaware of your type, how do you initialize the underlying object? That is - how do you invoke the default or non-default constructor of type T over the memory region you just allocated?
That's not the allocator's job, is it? It's up to the allocator's user to call one of the several available varieties of placement new on the block of memory allocated.
> That's not the allocator's job, is it?I don't see that point because this is not a technical argument, is it?> It's up to the allocator's user to call one of the several available varieties of placement new on the block of memory allocated.This would have been a point if that's the control you don't have with the allocator interface. But you do.I imagine that you're not suggesting invoking placement-new oneself all over the place whenever one allocates the backing storage?
It is a technical argument. Allocator's responsibility is to allocatememory. Constructing objects in that memory is not (and must not be) its responsibility.> This would have been a point if that's the control you don't have with the allocator interface. But you do.Well, std::allocator::construct() has been removed in C++20. You still have std::allocator_traits::construct but it just calls std::construct_at().> you're not suggesting invoking placement-new oneself all over the place whenever one allocates the backing storageWell, that's what all the standard containers do (vector, list, map, etc). Or you can use std::construct_at() instead but that effectively is a variant of the placement new. Of course, you'd better use std::make_unique()/make_shared() (which normally still devolves to placement new IIRC) instead of manipulating raw pointers and invoking constructors manually.
> one can easily add their own allocator to any STL container,You and I must have a different definition of "easily "
How about this?template<typename T>
  struct Foo {
    using value_type = T;
    Foo() = default;
 
    template<typename U>
    constexpr Foo(const Foo <U>&) noexcept {}
 
    T* allocate(std::size_t n) {
        if (n > std::numeric_limits<std::size_t>::max() / sizeof(T))
          throw std::bad_array_new_length();
        auto p = static_cast<T*>(std::malloc(n * sizeof(T)));
        if (!p) throw std::bad_alloc();
        return p;
    }
 
    void deallocate(T* p, std::size_t n) noexcept {
        std::free(p);
    }
  };And then you go and use it asstd::vector<int, Foo<int>> v;So the complexity is not really in writing the "C++ allocator" but in writing a sufficiently complex memory management logic that will actually make your application/algorithm run faster. And that only you can know given that you're familiar with the memory allocation patterns in your code. C++ allocator is only an interface that allows you to capture that logic and makes it feasible to apply through the code.
If you're in a position where you know you need a different allocator, and you have your own allocator, plugging it in should be relatively easy in comparison.
You can but it's definitely not easy to do correctly.There's also a lack of traits that can describe what's required of a specialized allocator. For example, std::map only needs to allocate 1 object at a time in practice while std::vector needs to allocate a contiguous count of objects.
>std::string allocatingYou mean you want string_view? It still calls strlen when you instantiate it with a literal though.
> What is much less prevalent is a demand from average C++ users for memory safety features; they’re much more concerned about compilation speed. When most C++ developers haven’t adopted tools like Coverity and C++ core guidelines checkers, it is hard to claim that memory safety features substantially improve their lives at least from their point of view.I don’t really agree with this. There’s also a group of developers that want more safety, but don’t feel like setting up (or in the case of Coverity purchasing) additional tooling. Some people just want a decent out of the box experience.
I'll make a similar, but fundamentally different, claim to what you're quoting:  There isn't much demand forhalf assedmemory safety features.  And C++ memory safety features are - nearly by definition - half assed.  They're opt-in (meaning all the third-party and system code you link against doesn't have coverage), slow (meaning you can't use it in production), and tend to catch the trivial bugs rather than the hard-to-find ones that keep you up at night.
Reminds me of "type-safety" features in duck type languages.
That sentence stood out to me as well. I don't think C++ developers is necessarily the best people to ask when discussing whether memory safety is important and urgent to work on or not. A lot of developers are relatively shielded from the consequences of bugs and security vulnerabilities, either by their programs not being exposed to the wild in any major sense or by bureaucracy being in between them and any consequences.At some point, the deficiencies of a language becomes a concern for its end-users rather than its developers.
> the deficiencies of a language becomes a concern for its end-users rather than its developers.I think that this level of professional malpractice is something that can't be solved by language choice.
But the level of damage can be reduced by language choice
Maybe, but surely the better answer is for devs to do better.
That actually seems unrealistic to me. Of course, people will always become better at doing the things they do, and we can always try harder, but many of these issues take time to consider and find during code reviews, and a few just slip by.
That's why tools like Coverity exist, but you have to spend time and money to set those up, meaning it's only done when absolutely necessary (or one dedicated person is really pushing for it).
Choosing a different language is basically free in the beginning, and it will impact which sort of bugs will be caught by default and which won't.
C++ is also massively hurt in this regard by not having a package manager. JS is a very risky language by default, but a few tools just with their default settings will already help massively.
I guess Nix could be considered the missing package manager for C and C++, but it's still niche and definitely not a "default" like pip, npm or cargo.
Yeah that stood out to me too. I guess the problem is that only a small percentage of developers both understand the need for memory safety and are vocal about it. That is very different from the number of users who would benefit from better memory safety.When someone encounters a crash, their actionable item isn’t to go ask the C++ committee for better safety. That’s an unassailable wall for most developers, assuming they even understand what’s happening and that there are better options out there.
C++ Developers aren't complaining for two reasons:1. Most of those who care about memory safety have already left the building.2. Those who are still in the building and care don't speak up because their colleagues don't value it, and many of their colleagues will view them as having lower competence.
YesIt is not about what we want. It is what we needNot real.y "we". I have lost any interest in C++ now. I loved it, in the day. Time has moved on
I think it's less about not wanting memory safety and more that compilation speeds are so time-wastingly abysmal that it's a few orders of magnitude more important to address.The whole deal with interpreted languages was to avoid sitting there for a few minutes every time you make a god damn one letter change, with the unfortunate but usually acceptable trade-off of some execution speed.
I know I'm weird, but as long as I can compile individual units and link them to other already-compiled units then compilation speed is something I don't care about much at all.
That's fine and dandy as long as you don't use C++ with templates and other stuff that needs 90% of the code to be in the headers.Any recompile triggers a lot of recompilation.And no, I can't change the code the other hundred devs are writing in my company.In C++ there is no individual units. In C it's somewhat plausible with some discipline.
yes, that's why I used the disclaimer "as long as I can compile individual units and link them".The problem with C++ templates is accurate, and is one of the several reasons why I avoid using templates in my own C++ code. I don't have that freedom in the code I write for my employer, though.> In C++ there is no individual units.Yes, there are. Those unit boundaries are often blurred by other C++ features, but they do exist.
> In C++ there is no individual unitsThere's a still a notion of the compilation unit, which is the cpp file on which you invoke the compiler. Due to templates being prevalent in C++, included files contain the implementation as well; so any changes you make there lead to re-compiling a lot more compilation units than, for example, in C.
That is kind of restating my point. There might be these translation units, but they're tightly coupled to all the other translation units, making them not very individual IMO.
And the additional trade-off that some bugs are only noticed at runtime when that particular line is executed while it could have been noticed by the compiler of a strongly typed language. Pytype helps but at this point you have a static analyzer that potentially runs as slow as a compiler without the additional performance benefit.
There’s no good reason for type checking to be super slow. I’m no fan of Go, but the language compiles insanely fast while being fully statically typed.As I understand it, C++’s slow compilation comes from the fact that it usually parses all of your header files n times instead of once. This isn’t a problem with static typing. It’s a problem with C++, and to a lesser extent C.
> As I understand it, C++’s slow compilation comes from the fact that it usually parses all of your header files n times instead of once.That's one of the things that can slow compilation down but it's definitely not the only one. It helps that precompiled headers (and maybe modules?) can go a long way towards reducing and possibly eliminating these costs as well.I think some (most?) of the larger remaining costs revolve around template instantiation, especially it impacts link times as well due to the fact that the linker needs to do extra work to eliminate redundant instantiations.
> due to the fact that the linker needs to do extra work to eliminate redundant instantiations.Yeah, I see this as another consequence of C++'s poor compilation model:- Compilation is slow because a template class in your header file gets compiled N times (maybe with precompiled headers). The compiler produces N object files filled with redundant code.- Then the linker is slow because it needs to parse all those object files, and filter out all the redundant code that you just wasted time generating.Its a bad design.
I'm not sure I'd call the design "bad". At the very least it's a product of the design constraints, and I'm not sure there's an obviously better implementation without sacrificing something else. I think separate compilation and monomorphization are the biggest contributors, but I wouldn't be surprised if there was something I was forgetting.Somewhat related, there was some work in Rust about sharing monomorphized generics across crates, but it appears it was not a universal win at the time[0]. I'm not sure if anything has changed since that point, unfortunately, or if something similar could be applied to C++ somehow.[0]:https://github.com/rust-lang/rust/issues/47317#issuecomment-...
> I'm not sure I'd call the design "bad". At the very least it's a product of the design constraints, and I'm not sure there's an obviously better implementation without sacrificing something else.It was a product of the design constraints in the 70s when memory was expensive, and compilers couldn't store a whole program in memory during compilation.The problem C++ has now is that the preprocessor operates on the raw text of a header file (which is a relic from C). This means the same header file can generate totally different source code each time its included in your program. C++ can't change that behaviour without breaking backwards compatibility. So headers get parsed over and over again "just in case" - wasting time producing excess code that just gets stripped back out again by the linker.The way C++ works doesn't make any sense now that memory is so much cheaper. Go, C#, java, rust, zig - basically every compiled language younger than C++ compiles faster than C++ because these languages don't contain C++'s design mistake.Rust doesn't share monomorphized generics across crates, but at least each crate is compiled as a single compilation unit.
This was already the case with languages like Modula-2 and Object Pascal in the 1980's, C++ works that way because it was designed to be a drop-in in UNIX/C without additional requirements.
> As I understand it, C++’s slow compilation comes from the fact that it usually parses all of your header files n times instead of once.Sort of.  The primary issues are:1) The C/C++ grammar is garbage.Note thatevery single modern languagehas grammatical constructs so that you can figure out what is "type" and what is "name" without parsing the universe.  "typedef" makes that damn near impossible in C without parsing the universe, and C++ takes that to a whole new level of special.2) C++ monomorphizationYou basically compile up your tempate for the universe of every type that works, and then you optimize down to the one you actually use.  This means that you can wind up with M*N*O*P versions of a function of which you use only1.  That's alotof extra work that simply gets thrown away.The monomorphization seems to be the biggest compile time problem.  It's why Rust struggles with compile times while something like Zig blazes through things--both of those have modern grammars that don't suck.
1. No, the grammar is not the issue per se. As you say, C has the same problem, and C code invariably compiles dozens of times faster than C++, and both Zig and Rust have modern grammars, but Zig compiles about as quickly as C and Rust is only somewhat faster than C++ (depending on features used).2. This is incorrect. What's happening is that each template instantiation for a new set of template arguments requires reprocessing the template to check types and generate code, and also that is done per translation unit instead of per project. Each distinct template instantiation increases the compilation time a bit, much more than it takes to parse the use itself. That's why it's easy to have a small C++ source that takes several seconds to compile.
> 1. No, the grammar is not the issue per se. As you say, C has the same problem, and C code invariably compiles dozens of times faster than C++, and both Zig and Rust have modern grammars, but Zig compiles about as quickly as C and Rust is only somewhat faster than C++ (depending on features used).Sorry, the C++ grammar isterrible.  There are lots of things where C++ can't figure out whether something is a class or name or template or constructor call until lookingwayfar down the chain.However, you are the first person I think I have ever heard claim that Rust is faster than C++.  Rust isnotoriouslyslow to compile.Zig generally compiles much faster than most C projects I've used.  However, that is difficult to lay at the hands ofCas a lot of those are the build system being obtuse.
>Sorry, the C++ grammar is terrible.When did I say otherwise? What I said was that it's not the main cause of C++'s long compilation times. The grammar causes other problems, such as making it more difficult to write parsers for IDEs, and creating things like the most vexing parse.>However, you are the first person I think I have ever heard claim that Rust is faster than C++. Rust is notoriously slow to compile.It's kind of a mixed bag. Given two projects of similar complexity, one in C++ and one in Rust, the one written in Rust will take longer to compile if organized as a single crate, because right now there's no way to parallelize compilation within a single crate. However, compiling the C++ version will definitely be the larger computational task, and would take longer if done in a single thread. Both contain Turing-complete meta-languages, so both can make compiling a fixed length source take an arbitrarily long time. Rust's type system is more complex, but I think C++'s templates win out on computational load. You're running a little dynamically typed script inside the compiler every time you instantiate a template.
(one of the reasons Go compiles fast is its compiler is really bare bones, comparatively speaking it does very little in the optimization area vs what you would see out of .NET and JVM implementations, not even mentioning GCC or LLVM)
It isn't just that, but there are a number of ways that templates can cause superlinear type checking behavior.
Idk as long as you can develop at speed I don't see why a static analyser that's a few times slower than compiling couldn't run on the latest commit overnight? More as a sonarqube type thing I suppose.
Nevermind the language itself, we need a way to pull compilers and project dependencies, pinned to their specific versions, with a single, ergonomic tool. vcpkg seemed really promising but the fact that they didn't start with library versioning from the get go was a very stupid decision, and nowadays versions are pinned to specific commit hashes rather than actual dependency versions, and libraries that weren't previously versioned cannot be fetched if your project relies on an older pre-vcpkg version of a library and cannot be upgraded for one reason or another. These days I find myself struggling more and more with proper dev setup where I have an up-to-date compiler on any machine I want to build the code along with its libraries without running into a rabbit hole of compiler and library issues.
Have you looked into Bazel?https://bazel.build/start/cppIt's not always easy to use, but the focus on reproducible builds and caching is really nice.Edit: I guess it doesn't fetch a specific compiler for you by default, but you could probably ship your toolchain somewhere and pin it in your WORKSPACE file.
Don't forget buck2 as well.
In the olden days, we always checked the compiler, libraries, and essential build tools being used into version control along with the code. That way you could always be sure that you could compile the code.This stopped working so well with Windows, where you usually can't just copy executables out of version control and run them (you have to run an installer instead), but it still works pretty well for the Unices.
In critical embedded development they snapshot the entire VM containing the OS, compilers, entire tool chain, and everything else.
vcpkg was dead in the water because they altered source packages. Completely unacceptable in many situations.
I also find vcpkg extremely fragile.Not in that pinning versions or the APIs or the actual contents of the packages we rely on, so much that every time we update a (rather small) set of dependencies there's nearly always some weirdness around vcpkg itself or the builds.Just the last week we updated to a newer tag and building openssl failed setting up the nasm build dependency, claiming it already existed.Which it did - there was a "nasm" folder in the tools directory it was trying to install it in, from the last time it was installed presumably and somehow got it's internal state messed up. But this caused a fatal error. I eventually worked around it by deleting the nasm directory from every build machine and letting it reinstall exactly the same package again.But the time before there was also a "random" build error, claiming that xz wasn't installed, despite the log showing it was just installed as a dependency in the line above. I guess this was fixed upstream, as the "workaround" was to use a tag from a month or so previous, but now seems to be fixed.Perhaps I'm using it wrong, perhaps you should "always" completely blast away the global vcpkg folder (and any vcpkg stuff cached in build directories from it's cmake integration) every time you touch it. But it's still time and effort for something that probably should be seamless.
vcpkg are forced to alter source packages because in the majority of cases upstream doesn't have working cross platform builds. They have weird broken CMakeLists.txt, incomplete compiler support, weird build incompatibilities with other libraries etc.
Until then there is Nix/Guix and Bazel/Buck.
Kind of a weird paper.1. It uses formalism to try to deny that there is some competition for brain-share and number-of-users among programming languages, or rather around the communities around programming languages.2. It ignores how C++ has, multiple times, semi-reinvented itself rather than "being C++", spurred by features, idioms or use patterns in other languages, and even managed to mostly "eat their lunch", for better or worse (e.g. the D language).3. A call for "aiming for coherence", while ignoring how it sometimes contradiction with other principles, such as: "What you don’t use, you don’t pay for" (the zero-overhead rule); and failure to argue even for a balance.4. "In committee, we frequently spend time on things that only a small number of people care about." <- but these are sometimes extremely important things, relevant to many, and in the future perhaps most programmers; and a small number of people care about them because most people aren't aware of their current or future importance. This is especially poignant where those things will only become important if the committee adopts them, and otherwise can be argued retroactively to never have been worthy of any discussion.etc. etc.
C++ isn't C++.  It is every incarnation of the language, every language subset defined in a code standard (for a project or an organization), over decades.Sure you can write virgin code in C++ and choose some "modern" incarnation of the language or some subset thereof. But there exists billions of lines of legacy that most of us have to deal with.  Daily or occasionally. Usually without the option of rewriting.Then there's the tooling.  The horrific, antiquated build process which we try to automate with tools that just make things even more complex. The lack of a practical standard library that says "you know, those things you often have to do, we should actually offer those".  It is neither helpful nor useful that C++ doesn't have room for HTTP, JSON, basic networking abstractions etc.I would not recommend a beginner learn C++ today.  I'd recommend Rust or Go. Most people are going to be more productive AND produce more robust code in fewer years. One can be annoyed by that statement, but you would have an uphill battle claiming that it is wrong.I don't think C++ is worth the investment for a new programmer. Dealing with C++ just gets more complex the more changes we make to it and the more we evolve it.  C++ isn't just one spec - it is the aggregate of all specs and practices that have existed because that's what you risk ending up working on in real life.It's a bit much history.From the perspective of the programmer I think it is a much better idea to put some energy behind a fresh start. And try to remember the mistakes that were made.  Right now, for C++ developers, Rust looks like the closest thing to a fresh start. But it may be that Go is going to make a lot of C++ programmers a lot happier too since not all C++ code lives in constrained environments or actually has to deliver bare metal performance.I don't think the C++ community evenwantsC++ to turn into something that is comparable to Rust or Go.  Doing that to C++ would require throwing things out and creating something very different.
I like that this doc at least says that C++ is unergonomic. I'm always saying that good way to learn language patterns and idioms is to look into standard libraries implementations.And when you look into C++ libraries/stdlib, they often look like they're written in another language entirely. This is not normal.
> I'm always saying that good way to learn language patterns and idioms is to look into standard libraries implementations.Why would that be true? When you write a library, you are writing code to coverall possibleuses; everything within the scope of your library should at least be considered, even if you personally have no need of that particular bit of functionality. But when you write aprogramit only has to do one thing, so of course it's going to be simpler. To me, it seems obvious that (good) library code will be very different from (good) application code.(I've used libraries that were written like applications, but they were bad libraries; I was constantly fighting the fact that the library author wrote only for their own use-case, and didn't consider any other.)
>> I'm always saying that good way to learn language patterns and idioms is to look into standard libraries implementations.
>
>Why would that be true?This isn't always true, and can't be generally expected, but I think itistrue in the case of Go's standard libraries. These are high quality, and are one of my top examples of good,realsource code to study (along with the DOOM and Quake source code). A nice touch in Go's library documentation is that you can click any API element (type, function, etc.) to be brought directly to its source code.Yes, there are differences between writinglibrariesand writingprograms. But there is value in studying well-written source code, even when it has concerns and requirements that differ from yours. You can adapt what you learn to your needs.
I hear you. But I’ve also learned a lot about how to write idiomatic rust from scrolling through rust’s standard library. You’re right - because it’s written to support lots of programs, it sure is packed with a lot of functions I’ll probably never use. But it’s still quite beautiful and readable. Much more so than C++.Here’s std::vec:https://doc.rust-lang.org/src/alloc/vec/mod.rs.htmlI tried to read C++’s vec class when I was learning C++ and I was confused and lost. I agree with the GP post - it feels like another language.
I'm not very familiar with rust, but isn't the point to mostly avoid `unsafe` and write safe code? Strange that basic std vec functions like insert and remove call `unsafe` - isn't this an example where you don't want to be like the std lib?
At the end of the day, rust is still a systems language. Lots of useful things require unsafe, including most data structure implementations and FFI - including syscalls.I see rust's safety guarantees like having a good static type system. Static type systems don't claim to prevent all bugs, but they do catch an awful lot of bugs at compile time in practice. Rust's default safety with opt-out unsafe blocks work the same way. Despite what some zealots would have you believe, the point isn't to make every single line of code "safe". Good rust code still uses unsafe code - for example your binary includes unsafe code whenever you use Vec or Box from std. But all unsafe code blocks are explicitly called out as such, tested thoroughly (eg with miri) and usually constrained to a small part of your program. You can think about it as, C or C++ programs are 100% unsafe. Rust programs are usually only ~2% unsafe or so. That makes a huge difference in practice.Unsafe code can also usually be encapsulated in safe wrappers. (Eg std::io::File wraps the unsafe call to open(), and std::Vec wraps some raw pointer operations). Unsafe also doesn't turn off the borrow checker. The only difference is unsafe blocks allow you to dereference pointers, call unsafe functions, and a few other things like that.
> Strange that basic std vec functions like insert and remove call `unsafe`The standard library has more unsafe than most programs, because "data structures that need a lot of unsafe" has historically been an argumentforbeing put in the standard library, because that way they'll be reviewed very carefully by experts.
I think you're right, and perhaps I think the underlying issue uncovered by OP might be: the lack of a single core set of features to achieve a sense of mastery with, that feeling like you've understood an implementation stdlib might have given.(I will admit I've felt profoundly stupid when I go from "how hard could it be to implement std::optional/reference counted pointers/etc., anyway?", to say GNU's implementation of it in libc++.)What the feeling of security that new features giveth, having to work with large idiosyncratic codebases and a wide variety of toolchains taketh.
> However, I don’t want to give the impression we should say no to all proposals. There are plenty of opportunities to improve our user’s lives through proposals. Here are a few concrete examples: [all 3 are just library additions that exist outside the standard already]This may not have been the intent, but the way I read it, it's saying that language evolution is now pointless and only library additions can be considered. That would be a pretty dire situation for C++ overall.Never mind that this means many language issues will never be fixed, it would also mean that implementations face the burden of yet more standard library additions which quickly fall behind external libraries because of the added burden of ABI lockin, and now multiplied by several vendors vs just one third party library.
I agree with everything written here - C++ is never going to be Rust, or any other language, but it can at least become better than it is now, in some common-sense ways.In my view, C++'s biggest problem is its design-by-committee structure, leading to a lack of pragmatism - in other words, perfect is the enemy of good. Pragmatism is what you see when you look at the standard libraries of languages like Python and Java: are they perfect? Far from it. But they prioritize being at least good enough for a good amount of situations, over being perfect. Which ends up being far more useful for regular people. And being useful for regular people is what makes a programming language successful.But design by committee means if your proposal isn't perfect, it will get shot down. So the language remains bad for everyone's use cases, rather than at least becoming somewhat good for some people's use cases.
C++ has major flaws that cannot be rectified without serious breaking changes. With that said, Herb has been experimenting with a new cpp frontend with sane defaults [1].In my opinion, the world is on standby until Anders Hejlsberg feels like tackling a modern, next generation systems language.[1]https://github.com/hsutter/cppfront
Herb Sutters' proposal is absolutely the right direction to go to reign C++ back into being a sane simpler language from the mess it has grown into. I doubt it'll ever happen though.It seems that in the eyes of most of corporate America's management, C++ is a legacy language, and should be replaced with Java (which tbh makes sense in many cases given that Java's tooling and libaries are better than those of C++). Other companies are looking at Rust as a successor to C++.Even if Herbs's work was completed and implemented by major compilers, it's hard to see much uptake - who's going to be approving porting legacy C++ code to a new modern C++, yet alone approving C++ for new projects?Maybe if there is a Sutter C++ successor, it should drop the "C++" name which carries legacy associations, and call itself something new. Present itself as C++ successor, even if it comes from a starting point of being 99% backwards compatible with modern (say C++11 or later) C++. Sounds superficial, but I bet it'd make a difference in perception!
I have seen that the biggest issue with C++ is that legacy C++ is quite hard to refactor to the new patterns. For example, I have been working on font substitution in LibreOffice, and I'd like to use a more functional style of programming, but I'm getting stuck because of an overuse of classes.I've been reading Functional Programming in C++ by Ivan Cukic and I'd like to adopt this - it does require a lot of refactoring of this massive codebase.Perhaps this is a massive, ancient codebase issue more than a C++ issue, though.
I think this is one place where you're totally working against the language.Edit: I don't know how you can arrive at this conclusion given the history of the language. The STL isn't the C++ standard library.I would love to discuss this more, but dang has limited and nerfed my account.
Not really. Look at the standard template library. It’s not really using classes with inheritance. It’s using non-member functions, often with iterators.Ironically I’ve just learned, however, that lambdas are actually anonymous classes! I’m currently up to the bit in yhe book I referred to before that explains how to do currying in C++.The more I read, the more I think I’m working against the code base than I am against C++. I’m actually very curious how ranges work, something the book talks about later.
Sorry, not STL, I meant the standard C++ library.Edit: send an email to news@ycombinator.com
If C++ wants to continue evolving, it needs breaking changes. It needs to impose a migration duty to the user, but that is not going to happen.So it will die a death by entropy where noone can know the language.
I think the great challenge with breaking changes is demonstrating how other systems have done breaking changes with great success. They are quite rare. And no, the python2->python3 migration is not one of them. Neither is ip4->ip6.
The important part about breaking backward compatibility is allowing forward compatibility. Java is good in that regard, for each breaking change, you could still create a library that worked for Java N and Java N+1.It is possible to have a network supporting IPv4 and IPv6. I don't think the speed of migration is an issue. It is not a tremendous success but it is not a failure either.In Python forward compatibility was often impossible. The migration benefit was low and the cost was high. Python is a great example how not to break things. I am little surprised that whole language is still popular after such failure of leadership.
> it needs breaking changes.Then nobody will use the new compiler because nobody wants to rewrite millions of LOC and introduce new bugs.
Introduce new bugs, sure.
That's what will end the AI hype.
> that is not going to happenIt’s even worse. They happily introduce breaking changes for some things, but not others.I have once wasted days of work updating a codebase to build with C++/20 because the new standard suddenly changed the type returned by `u8` string literals (available for a decade already, it was introduced in C++/11) from const char* into const char8_t*, and they made these types incompatible.
"Consider the hoops one needs to jump through to make std::print work with a custom type when compared to the old stream operators."As an old timer, this kind of made me laugh. I remember when people found the old stream operators burdensome.
In every job I've had so far, I've been writing C++ because the guy before me wrote C++. That's really all there is to it.What "C++" means depends on the project. Maybe that's why it's so hard for the standard -- they have to consider more than one project.
I feel the C++ committee saw the major quality-of-life improvements from C++11 and convinced themselves that Python's ergonomics with templated C++'s speed was possible. I won't say it's impossible, but the backwards compatibility sure seems like too much weight.
The C++ community has some of the best brains around.  Absolute geniuses.If C++ is ok with being a small language used by and for geniuses - I have no problem with that.If C++ wants to be popular - we need a breaking change between the world of 2023 and 1990 - they don't know what we know now.I like some parts of C++... it's too bad the "good language" is shackled to a mountain of razor blades.
What do you think about Carbon[1]?  I am hopeful.[1]https://github.com/carbon-language/carbon-lang
Not the OP, Carbon currently still doesn't have a working implementation, it is clearly a tool for Google to migrate away from C++ without rewriting the world, they are quite open that it isn't trying to be anything else, it is only an experiement, and it may even fail as an experiment.Carbon gets talked a lot by people that somehow miss the language goals.
Honestly, the changes in C++ 20 and 23, plus the upcoming changes for 26 (looking at you, Concurrency TS v2) have so drastically improved the language it's almost like the ES5 -> ES6 evolution of JavaScript.I might be in the minority here, but I genuinely enjoy writing modern C++.My complaints are about it's dependency management story and lack of integrated, standardized tools for things like dependencies, testing, logging, etc.
I don't know if you're in the minority or I am, but I find modern C++ to be borderline intolerable (and I've been programming mostly in C++ from before there were C++ compilers).I used to love C++ (and still do if we're talking about older standards), but the new stuff is just a baroque torture.
Aye, same here.I don't mond C++ evolving into some thing of its own beyond all recognition, but there is a distinct lack of modern "C with Classes" language. Basically, C on steroids. There are attempts at that, but none is perfect and/or has enough traction to be viable.
You can still have "C with Classes" in modern C++ if you really want that. Just set a style guide that limits your code to that.There's more viable coding styles in C++ than I can honestly even bother to count. There's absolutely no reason to limit the language to one specific coding style when doing so would alienate large groups of users and you can set those limits yourself on your project.
Yeah, I know and that's how I've been using it for the past decade, but it's not that simple. The language keeps changing and this forces adding cruft to the code if one wants to use newer compilers.For example, there are cases where the existence of move semantics necessitates adding some boilerplate when working with STL containers. There's no functional reason for it, just something to please the compiler.
This is what I do for my own projects, but it's not a viable thing to do with my work projects since I don't set the style.
I’m excited for Zig for this reason. I’ve been writing a lot of rust lately and - well, it’s fine. Good at what it’s trying to do. But Zig seems a lot more fun. Much more in the spirit of C.
To each their own, but I find zig syntax to be on the ugly side and the resulting code unelegant.
I just want a C with templates (and function overloading)?
"this" and virtual functions are pretty damn useful too.
Anything C++11 forward is alright with me, though 17 introduces some very nice conveniences like <execution>, <filesystem> and most critically <optional>. Honorable mention to std::clamp too.
> Honorable mention to std::clamp too.One shudders to think how many times variations of that template have been written in house.
Heck, I find so many cases where std::min and std::max could be used as well.
I would like to add:- structured bindings- auto parameters in lambdas- init statements in if and switch clauses- template argument deduction for constructors
23 introduced <expected> (result/either) as well
You're absolutely not in the minority, old C++ was awful and there have been so many improvements to the language and standard library to help write simpler, clearer code.I was just recently appreciating the if statement with an initializer in C++17, which helps me express exactly what I want for variable scope in one line instead of multiple awkward ones.
Honestly, everyone working with C++ sticks to a select subset of the language that they've chosen for their project, or the project they're contributing to. Nobody knows all of C++. Personally, I don't mind C++ forking out in different directions and accepting diverse proposals; while I wouldn't bother to use them myself, I realize that it may be useful to other people.C++ is an engineer's language, and it's ridiculous to imagine that we'd ever need a C++2.0 that cleans it up. Subjectively, you could say that some features are "ugly", but this is an evolutionary process, and there are bound to be vestigial features.Yes, there are memory safety issues, but in practice, these are isolated in very few places. Take a compiler like LLVM for instance: most developers are working on transforms or analyses, and they're exposed to zero manual memory management. Sure, the Pass Manager needs to build passes, and the IR needs to be allocated, but that's about all the manual memory management there is.Personally, I couldn't care less about standardized argv-parsing, as each project has its own set of complex requirements. There are JSON parsing libraries available for C++, and I don't see why it should be standardized. Faster hashing in std could be a low-priority feature, but projects like LLVM have their own optimized version of std data structures and algorithms.I suppose a module system could be useful; most C++ projects are built with CMake which is already very good at finding and linking dependencies. Personally, my biggest pain point is compile-times, but that's really an LLVM/Clang problem.Overall, the article doesn't seem to be written by someone who has a lot of experience with large C++ codebases.
> Yes, there are memory safety issues, but in practice, these are isolated in very few places. Take a compiler like LLVM for instance: most developers are working on transforms or analyses, and they're exposed to zero manual memory management.It is really easy to get use-after-free in LLVM passes due to using eraseFromParent() instead of removeFromParent(), etc. As I recall, in some places the optimization code goes through really awkward patterns in order to keep track of the IR nodes that are dead to avoid UAF, none of which would be necessary if LLVM were written in a GC'd language. (Note: I'm not saying LLVM should be written in a GC'd language.)
Maybe not, still I like to follow up on GraalVM, as I used to for MaximeVM and JikesRVM, and MSR Phonenix compiler, exactly because of that.Compiler building frameworks in managed languages.
Except the detail when LLVM gets shipped with application code, like a JIT, and then gets exposed to possible security attacks.Yes, most likely LLVM will never be rewritten into something else, yet security is a concern also in compiler code.
Is this a response to some specific set of proposals? I know there's been a lot of general angst / misunderstanding over C++ memory safety.
What's to misunderstand?  There is no memory safety in C++.
It's a response to Rust. Rust is significantly better than C++, and most importantly it's the first language that could actually replace C++ for the things C++ is generally used for.I guess the C++ community is coming to terms with not being the top dog of "zero-cost abstraction" languages anymore.Really I think it's too late for C++. They had literal decades to fix very basic flaws, and they just haven't done it.Accidental octal literals. Case fall-through. Missing `return`s. Accidental string literal addition. The whole module system mess.They focused way too much on adding complex features and not at all on fixing footguns. As a result C++ is painful and dangerous. Too late to fix IMO.
TFA cites a much more satisfying since technically substantial distillation of "what C++ is", especially regarding technical goals and technical philosophy: the "Direction for ISO C++" document, 2022, by core members of the ISO C++ committee. [1][1]https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p20...
What is the proposed legislation referred to in this article?
I have not heard anything about legislation, but the NSA and CISA have been arguing for the industry to embrace memory safe languages:https://www.nsa.gov/Press-Room/Press-Releases-Statements/Pre...Even without legislation, I could imagine that the US government could try to enact software procurement rules that favor or require memory-safe implementations.
I hope people don't throw out the effective and well-thought out parts with the problematic parts.It happened with Macromedia Flash and it happened with Microsoft's Visual Basic. The JVM and industry attitudes to Java. PHP.We have lost some good technology over time, please don't let us lose the good parts of C++.
I'm not sure this is best done by holding on to implementations of those ideas that have serious flaws. Yes, learning is slow, and most languages seem to start with a handful of ideas someone wants to realize and then ignoring a lot of the difficult stuff others took a long time to solve.
For whatever reason, the software industry never learns this lesson. Maybe because people would rather move on to something new entirely and new entrants are not aware of what they're losing at all.
weird to throw in 3 pet libraries he is advocating for at the end
> It is easy to see that C++ is fit as a general-purpose programming language–adoption by millions is a testament to that.No, that is false.  It is akin to arguing that Christianity must be true because 2.4 billion Christians can't be wrong.  The fallacy is easy to see because the argument can be applied equally to the world's 1.9 billion Muslims and 1.2 billion Hindus and 500 million Buddhists, etc.  And yet these groups hold mutually-exclusive positions and so at least N-1 of themmustbe wrong.Humans are social animals.  With only a few exceptions, we tend to conform to the group.  That tends to make us get stuck in very deep ruts.  "Everyone is doing it" is absolutely no indication that "it" is a good idea.[UPDATE] A lot of people seem to be missing the point here, so I feel the need to clarify: I'm not saying C++ is analogous to a relgion. I'm using the exclusivity of religious belief just as a short-cut to show that it is possible for large groups of people to hold false beliefs without getting into the weeds of which of those beliefs are actually false. The point is not that C++ is analogous to a religion, just that "adoption by millions" is not a valid argument for its merits.[UPDATE2] I am also not saying that religion has no value beyond the truth of its objective claims.  I am only saying that (some of) the world's major religions do make objective claims, some of those claims are mutually exclusive, and so some of them must be objectively false, and therefore it is manifestly true that large groups of people can hold objectively false beliefs, and therefore the fact that large groups of people hold a position is not evidence that that position is objectively true.  Being "fit as a general purpose programming language" is an objective claim.
Yeah, probably should have left the religion analogy out.  It wasn't essential to the point you were making and was based on a false model of religion.There's actually a pretty complex matrix in religious disagreement with or without exclusivity.  And not all religions are exclusive at all.  Islam's views of other Abrahamic religions is complicated, but it is exclusive about non-Abrahamic religions.  Christianity sees Judaism as having been correct, but incomplete.  Hinduism isn't essentially exclusionary (particularly to other religions which evolved from old Vedic practices), but is often confused with modern Hindu nationalism, which is.  And even within single "religions" there are complicated lines where e.g. some Christians consider themselves to be in "communion" with some Christians, but not others.So, yeah, bad example.  It's a terrible thing to have pulled out to try to show clear mutually exclusive groups.
What would have been a better example?The problem with trying to come up with examples of large numbers of people holding objectively false beliefs is that you have to look outside the realm of science.  The whole point of science is that it provides a mechanism for resolving disagreements about objective truth objectively (i.e. experiment) and so you just don't get a lot of people holding objectively false beliefs, at least not for any length of time.  Religion is all that's left.
Most of what humans talk about isn't science or religion, and to reduce the world to just that is pretty weird to say the least.  Most things can't be resolved by appeals to objective truth.  It sounds like you're working on a model where that's how one resolves conflicts, but literally most of the history of knowledge, humanity, whatever label you want to put on it -- isn't that.  Even science isn't by any means that binary.  Disagreements can last centuries.  And we're even at a spot in science where a lot of the interesting stuff fumbles around for decades before we can even come up with experiments that could possibly test it; and some of it we won't ever be able to test.  (A lot of cosmology isn't testable.)But the point me and a few others were making is that you don't seem to know much about religion, so it's probably not a good thing to use in analogies.  Religion is definitely not a set of neatly divided mutually exclusive beliefs.  Almost all of the world's adherents come from two families of religions -- Abrahamic or Vedic -- and within those groups there's a whole lot of similarity and varying levels of theological exclusivity.I feel like gambling or the stock market may be better examples.  If one person takes a long position on a stock and another short, they have mutually exclusive beliefs about it, and one of them will be wrong.
It's easy to see that driving on the RIGHT side of the road is a fit as a general mode of transportation. Adoption by millions is a testament to that.It's easy to see that driving on the LEFT side of the road is a fit as a general mode of transportation. Adoption by millions is a testament to that.Both statements are true. The author didn't say only C++ is fit. Nor did he say a program written in BOTH C++ AND python (i.e. a system of driving on both left and right sides of the road) is fit.The counter-argument (C++ is NOT fit as a general purpose programming language) is invalidated by millions of programmers who use it as such in the same way that thumbs are not fit for grasping is invalidated by, well, grasping with your hands.It doesn't mean pliers are not fit for grasping just because thumbs are fit for grasping.You're conflating types of evidence and types of arguments.
No.  I'm not saying that C++ is not fit as a general purpose programming language.  It very well may be.  All I'm saying is that "adoption by millions is a testament to [the fitness of C++]" is not a valid argument.  If it were, "adoption by millions is a testament to truth of [objective religious claim X]" would be a valid argument, and it manifestly isn't because different religions make mutually exclusive objective claims.
"adoption of X is a testament that it is fit for purpose" and "adoption of X is a testament of truth" are two incredibly different things.The equivalent statement would be that "adoption by millions is a testament to the viability of X as a cohesive religion".
"Fitness as a general purpose programming language" is (at least in part) an objective claim.  If a million people professed to believe that, say, brainfuck was fit as a general purpose programming language that in and of itself would not make it so.  This is not the case for being a cohesive religion.  If a million people profess to believe some religious belief, that in and of itself is sufficient for that belief to be a cohesive religion.
> If a million people professed to believe that, say, brainfuck was fit as a general purpose programming language that in and of itself would not make it so.No, but if millions of people actually did manage to use Brainfuck for general purpose programming, then that would be evidence that it really is fit as a general purpose programming language, even if it's not ideal.Brainfuck isn't in that position, hence no one thinks it's fit, but C++ is. People do use it for general purpose programming, even if their program could be rewritten in a garbage-collected language.
IMO the reason C++ isn't a general purpose programming language is due to memory management. Many many many applications can be built without having to worry about the garbage collector and the productivity gains of using a GC language is so so worth it. And I know you can force C++ into acting like a GC language, but why go through the effort? C++ is a precision tool for building complex and performant systems and that is nothing to be ashamed of, but it is not something you would use for web api's, or a quick script, or UI's or any quick and dirty project. I also feel like the rust community is forcing the language into places where it shouldn't really be. But yeah - people can argue about what general purpose means to them, just my 2c.
GC is not required for memory safety.  The proper use of GC nowadays is for dealing with problems that inherently involve spaghetti-like reference graphs for which no other memory management strategy is suitable.  Using it as mere convenience might be okay for quick prototyping, but it ultimately leads to half-baked, hard-to-refactor code requiring a lot of CPU and memory overhead at runtime.
> GC is not required for memory safety.>memory overhead at runtime.These statements are true. The rest is grandstanding.
> but it is not something you would use for web api's, or a quick script, or UI's or any quick and dirty projectMaybe not, but that's a function of available/standard/free libraries rather than the language itself.Personally I do use C++ for quick and dirty projects, but OTOH I've spent the last 10 years building libraries that make that convenient.
"Christianity is true" and "Islam is true" are mutually-contradictory positions.  "C++ is fit as a general-purpose programming language" does not contradict "X other language is fit as a general-purpose programming language".  So your logic in your first non-quote paragraph doesn't work.[Edit:  To respond to the actual point:  For every claim that X is fit as a tool to do Y, the evidence that millions of people use X to do Y is in fact proof of the claim.  If the claim had been "C++ isthe bestlanguage for general purpose programming", then your argument would have merit.  But that wasn't the claim.]
> "to show that it is possible for large groups of people to hold false beliefs"The claim from the article is not about theirbeliefs, it is about theiractivity. If a million people say that one can live many different lifestyles from a tent but those people actually live in suburban houses, they can potentially all be wrong. If a million people actually do live in tents while living many different lifestyles, QED, it is demonstrated - they aren't holding false beliefs, full stop. It's not a belief anymore, it's a fact. The fact that they are doing it shows it can be done at all, and the large numbers show it's not an extreme claim that only one or two weirdo obsessives could contort themselves enough to use, it's general enough for millions to do over many lifestyles.> "Everyone is doing it" is absolutely no indication that "it" is a good idea.It is too; "When in Rome" is advice because whatever the Romans are doing, it isn't killing them or getting them into fights or mugged or annoying someone powerful. If you don't have any reason to do otherwise, eating what the Romans eat, drinking what they drink, behaving how they behave, is a far far better starting point than almost any other. As a guest in someone's house, trying behave how the homeowners behave is a good idea; 90+% of starting points will be worse, most things you could try to eat will make you ill or kill you, most of the world's thousands of programming languages are toys or niche domain systems or wildly outdated or proprietary and gone out of business. The ones millions of people use? Pretty good idea to use one of those, unless you have very good reasons for doing otherwise.
> The claim from the article is not about their beliefs, it is about their activity.No, the claim is about a property of a programming language.  The activity is cited as evidence in support of the claim of the fitness of C++ as a general-purpose programming language.The problem with C++ is that it is a legacy language, and so the fact that a zillion people use it todaymightbe because it's a good language, but it might also be because it has so much institutional inertia behind it that this is enough to override the fact that it's a totally shit language.  The Catholic Church has been around for 2000 years, but that doesn't necessarily mean that its factual claims have any merit.  The Church's successmightbe because it is in communion with the truth, or it might be because it has so much institutional and societal inertia that it keeps chugging right along despite having no actual merit.  The success of the Church might also be due to people subscribing to the logical fallacy that because a lot of people subscribe to it that it must have some merit, which after a while becomes a self-sustaining cycle.  (Note that a self-sustaining cycle is different from a self-fulfilling prophecy because the latter actually becomes true if enough people subscribe to it.)
Awful analogy.C++'s fitness as a general-purpose programming language is not an exclusive assertion.
That's not the point. The point is that "N people can't be wrong" is not a valid argument for any value of N.
It's a programming language that has been used by lots of people in a lot of different contexts. The author of the article think it's a good definition for "general purpose programming language". The original author has never said "N people can't be wrong", it's the reframing of top comment. You can disagree with the "general pupose"-ibility of C++ but I don't think it's an honest way to interpret the argumentation of the author of the article.
I'm not saying it is.  I'm using the exclusivity of religious belief just as a short-cut to show that it is possible for large groups of people to hold false beliefs without getting into the weeds of which of those beliefs are actually false.  The point is not that C++ is analogous to a religion, just that "adoption by millions" is not a valid argument for its merits.
No, your analogy falls over.The equivalent assertion is "Christianity is a good/useful belief system" (and "Islam is a good/useful belief system")Now, you could endeavor to disprove that equivalent assertion, but your earlier argument doesn't.
My argument applies equally well to that: the mere fact that a religion has large numbers of adherents does not in and of itself show that it is either good or useful.  But that's a harder case to make because it turns on what is meant by "good" and "useful", and those are things about which reasonable people can disagree.To be clear, "fitness as a general purpose programming language" is also something about which reasonable people can (and manifestly do) disagree.  All I'm saying is that having large numbers of adherents is not a valid argument in favor of fitness any more than it is an argument in favor of goodness or usefulness.  It's possible that all it shows is that a lot of people drank the kool-aid.[UPDATE]  It's also possible that most of the people using C++ think that it sucks, and they are all just using it because everyone else is using it.
Your argument is "large numbers of people can be wrong, as shown by the fact that either Christians or Muslims are wrong."But metaphysics/religion is different than programming language utility.We can expect a higher degree of accurate judgment in the latter.
> metaphysics/religion is different than programming language utility.Of course it is.  But religions make mutually exclusiveobjectiveclaims.  Metaphysics has nothing to do with it.
A religion does not have to be true to serve a positive social function. Arguably, the persistence of the most ancient and widespread faiths suggests that they do serve such a function.
I mean the statement you quote is undeniable.If something is used by millions as a general-purpose programming language it is fit to the task, regardless of the alternatives or how ideal it is.
That depends on your criteria for fitness.  There are millions of people using homeopathic remedies.  That doesn't mean that homeopathy is actually fit for any of the tasks that people employ it for.
You know that people actually write complex software in C++ right whereas homeopathy does not achieve anything.
I know thatsomepeople write complex software in C++.  I suspect that number is actually very small.
"upcoming anti-C++ safety regulations"Is this a thing really or just FUD?
What is a real thing: various government organizations arguing that projects should move towards memory safe languages.What is not a real thing: regulations requiring such.Some think regulations may come, others don't.
Probably related to things like these:https://news.ycombinator.com/item?id=33560227- NSA urges orgs to use memory-safe programming languages
It's both and neither. Some countries are considering legislation to require "memory-safe" tooling for "critical" workloads, and C++ has never been placed on the "memory-safe tooling" list.
Which countries? Can you share the reporting on this legislation? I’d be very interested.
The best forecast I know is Sean Parent's on ADSP episode 160.[0:23:57] SP: We're also discussing internally around pending legislation around safety and security, what Adobe's response is going to be. Right now our thinking is we would like to publish a roadmap on how we're going to address that. That is not finalized yet in any form, but I expect a component of that roadmap is going to be that some of our critical components will get rewritten into Rust or another memory-safe language.[0:24:28] CH: When you say "pending legislation", is that a nod to some pending legislation that you actually know is on the horizon? Or just anticipating that it's going to happen at some point?[0:24:38] SP: Oh yeah, no. There are two bills (sorry I don't have...)[0:24:44] CH: It's all right, we'll find them and link them in the show notes afterward.[0:24:48] SP: Yeah, I can hunt down the links. The one in the U.S. that's pending basically says that the Department of Defense is going to within 270 days of the bill passing (and it's a funding bill which means it will probably pass late this year - early next year) that the Department of Defense will establish guidelines around safety and security including memory safety for software products purchased by Department of Defense. The E.U. has a similar wording in a bill that's slowly winding its way through their channels. I don't have insight into when that will pass. The U.S. one will almost certainly pass here within a month or two.[0:25:43] CH: Oh. Wow.[0:25:44] SP: There's a long way between having a bill pass that says almost a year later they have to establish a plan for what they're going to do, right. So it's not hard legislation in any way. But I view this-- I can send you a link. There was a podcast I listened to recently on macOS folklore. [...] It's talking about how in the early '90s there was a somewhat similar round of legislation that went around around POSIX compliance. Basically the Department of Defense decided that in order to have portable software, every operating system that they purchased had to have POSIX compliance. And there was a roadmap put into place. That's why Apple pursued building their own UNIX which was A/UX and eventually partnered with IBM to do AIX. And Microsoft in the same timeframe had a big push to get POSIX compliance in Windows OS. The thinking was eventually in order to sell to the government your operating system it would require POSIX compliance. What actually happened, if you wanted to buy just traditional Macintosh operating system you would just say "well I require Photoshop or pick-your-application and there is no alternative that runs under UNIX so therefore I need an exception to buy macOS" and it was extra paperwork but it got signed off on. So really never materialized into hard restrictions on sales of non-POSIX-compliant OSes. I expect the safety legislation to take somewhat the same route, which is, there will be pressure to write more software in memory-safe languages. When you don't write software in memory-safe languages there is going to be more pressure for you to document what your process is to mitigate the risks. And this is initially all in the realm of government sales, although there is some discussion in both the E.U. legislation and on the U.S. side of extending this to a consumer safety issue. But there will be an escape hatch because you couldn't wave any kind of magic wand as a legislator and say "you can't sell software anymore if it's written in C++". The world would grind to a halt. So there will be an escape hatch, and there will be pressure. So as a company you have to look at how are you going to mitigate that risk going forward. And what's your plan going to be so that you can continue to sell products to the government. And how do you make sure that you're not opening up a competitive threat. If you've got a competitor that can say "well we're written entirely in Rust so we don't have to do the paperwork" that becomes a faster path. So you want to make sure that you're aware of those issues and that you've got a plan in place to mitigate them.
Thank you. I have mostly heard people confusing the CISA stuff with "legislation," but this sounds like something that isactuallylegislation. I'll have to dig into it. Thank you.
The episode:https://adspthepodcast.com/2023/12/15/Episode-160.htmlThe legislation:*https://www.congress.gov/bill/118th-congress/house-bill/2670*https://digital-strategy.ec.europa.eu/en/library/cyber-resil...
Not familiar with said regulations, but Ada was a good example of not using C++ for these very reasons in a military context.
this feels defeatistc++ is with us for the near future, it needs an evolutionary path that provides a sane path forwardwhy not break with the past? anyone needing to compile c++11 can find the tools and still use them, they can live in the past as long as they likejust getting dependency/build management and integrated testing would be huge and really a small ask given what is happening in other toolsits crazy that you still can't make sane use of things like modules, even if you fully commit to "modern" c++
Lack of modules support is the biggest thing keeping me from doing more c++ greenfield. So much friction in managing apointlessinterface<->implementation layer.
I'm not well versed in C++ best practices, but unless it's a non-option for your situation, have you considered single file headers or perhaps even a unity build[1] approach? Of course, both are just workarounds for proper module support and have their drawbacks and limitations.[1]:https://en.wikipedia.org/wiki/Unity_build
Yes! Proper modules support would give c++ a well needed boost (no pun intended!). msvc is kinda there, but it breaks here and there with intellisense.
This is the never-ending problem with programming languages.If you make big breaking changes, the community suffers - even if the changes are very well intentioned. Look at the long tail of Python 2 vs 3 issues. If it happens once it's not so bad, but the older the community and the greater the size of its existing ecosystem - the more that pain becomes. Rust, from what I've seen, understands this issue.
I am a retiring engineer/entrepreneur. I made a lot of money with C++. It is hard for me to tell anyone not to use it.I recently developed an oil and gas derivative tool in Rust. IMO Rust has a long way to go. It reminds me of OCaml. I spent 2 years developing a Compliance System on DEC/OSF1 in OCaml. We made the mistake of adopting that language too early. Also, the C++ community is a lot more tolerant. A big plus for C++.
> Also, the C++ community is a lot more tolerant.Yeah the biggest turnoff for me with Rust isn't even the language anymore, but the community surrounding it.I realize it's mostly a vocal minority and the vast majority of Rust people, including the ones I know personally, are very nice. But boy are there some obnoxious Rust evangelists out there.
> Also, the C++ community is a lot more tolerant. A big plus for C++.Well I completely disagree with this. The C and C++ communities seem to me personally to be a lot more "elitist", engaging in extreme pedantry or dunking on each other over knowing obscure language trivia.  Both the language and the community are beginner-hostile in comparison to Rust.Not that the Rust community is perfect, because it's obviously not - but lack of "tolerance" has never seemed like one of the problems it has.
This echos my personal experience as well.I would also add that there now seems to be more of an emphasis around being intentional in how languages foster community. The C and C++ communities (in my opinion) became how they are not by design, but mostly by accident or chance.
Tolerant towards what exactly?
In which ways does Rust need to improve, in your opinion?
I'm fond of rust, but compared to go, I dislike that you need a different code path to support sync vs async in a module.  Or pick a different runtime like Tokio.  So you end up with modules that are async only, sync only, or both.Makes me miss go, which has channels build in, has a runtime built in, and both are generally solid.  None of use flavor A for this, use flavor B for that, or write wrappers for the wrong color function.
Rust has channels built in. In fact, the std channels were recently rebuilt on top of the excellent crossbeam crate.Function colors is kind of a meme but maybe-async functions are coming.https://lib.rs/tokiois king for std environments,https://lib.rs/embassyfor no_std.
Yes, MPSC, single consumer which is limiting.
Oh.cargo add crossbeam-channel
Compile time, async, steep learning curve are things that come to mind
I’ll add, thanks to their age, C and C++ have more compatibility with boards, OS’s, GUI’s, libraries you might need, etc. Especially in embedded, you have way more options if using or hiring for C/C++. There were also more tooling for analyzing the C/C++ software but they often cost $$$.For common scenarios and platforms, Rust’s library situation did get way better really quickly.
Thank you
Here we go. I'm not taking this bait.
I’m not trying to bait you, and people who attack you for what you say are morons.I don’t have much experience in either language, and I don’t have an opinion on which is better. I am just interested in your opinion.
I understand your hesitation, but I’d really appreciate your take, given you seem to know your stuff.Personally I think the sibling comment is pretty accurate, and rust is held back by:- slow compiling of larger projects- async stuff can quickly lead to very complex shenanigans- embedded still requires you to write HAL stuff for MCUs that is supported out of the box for C
> I recently developed an oil and gas derivative tool in RustSounds cool! Could you elaborate? Would you have a website somewhere? Do you have any contacts details?Cheers!
can you share more about how/at what type of jobs you made a lot of money with c++?
> I made a lot of money with C++.Please elaborate.
I've heard from some of my colleagues at other companies that Rust has a significant negative impact on developer productivity compared to C++. This makes Rust great for, say, the Linux kernel, where the security and performance features are really needed and the maintainers don't give a rat's ass about how much time and effort it takes for contributors to get their features merged. On the other hand my current company already has significant challenges in the area of developer productivity, and it could very well be that requiring Rust for a new internal project would doom it from the start just for that reason.
My personal experience and that of many people I've spoken or listened to is the opposite.  Mozilla and Oxide engineers talk about how much easier refactors are and how much less time is spent mucking around with build systems in Rust.
I can't confirm that. Rust's safety features allow moving fast without fear of subtle bugs.Developing in Rust can feel like fighting the compiler though when choosing approaches / architecture that aren't a good fit for Rust.  This happens a lot when people only have little Rust experience. For example: Writing a linked list can be quite challenging in Rust due to self-referential types. With more experience in Rust, developer learn how to best structure the code so that it doesn't result in this friction.
personal antidote - i've heard this too, but jumping in (after years of python/typescript), i'm blown away by how productive i can be. it's easily my favorite language.the hiring pool may be smaller, but don't believe the fear.
I used to have parallel feelings about Scala as a newer better alternative to Java. Then I ran into some issues with it in the real world. See if there are any parallel, I will copy-paste the issues I reported before:"There were some problems with Scala in the real world. Compared to Java it had a big learning curve, what with all the features it took from Haskell and functional programming while also keeping all the old features of Java. Some of the libraries weren't as mature as their Java equivalents or there were two equal competing libraries whereas with Java there was just one popular library for a given thing. Hiring Scala developers was an issue. Compile times were bad. Every major version of Scala (ex. 2.10, 2.11, 2.12, etc.) broke backwards compatibility so if you had something with say Scala macros that compiled in Scala 2.10 it wouldn't necessarily compile in Scala 2.11 without code changes. We had a library where the maintainer abandoned it at Scala 2.10 and we couldn't upgrade to Scala 2.11 without either taking the library out and using something else in its place or taking over the library, which just wasn't feasible. There was an issue with Scala that I've seen with C++ as well where the language is so big that different teams and different people program in different "dialects" of the language, using different subsets of language features (ex. some people used OOP class inheritance and some did not). Lots of people didn't get Monads or functional programming. But yeah, lots of places ended up going back to Java or using Node.js if they wanted to be async/reactive.
"
This is spot on. Scala is a powerful language and we get a lot of milage out of it, but it's despite the Scala ecosystem, not because of it.
1. Better integration with existing libraries, there is a lot of software the exist in C/C++, and a lot of it is weird and opinionated, and having to write bindings for that isn't great.2. Continuing on that, QT is still good for large desktop GUI projects. It doesn't seem it's well bound to Rust, and Rust is still trying to figure out their comparable UI story.3. Ironically, in some certain circumstances, with good discipline, (And mostly interfaces that looks like C) C/C++ can compile faster then Rust.
I'm "Rust curious", but I'm coming from a web/app background (Dart, JS, TS, Java, Python), so take my ideas with a grain of salt.1. One context, which appears to be yours, is when a team or individual has extensive C++ experience.You see, for me, learning C++ with all its decades of good/bad parts feels overwhelming, so Rust makes sense for me even if it's still the hardest language I've ever learned, but as you are already an experienced C++ dev, it might make sense to continue cpping.I'm used to simple, standard formatting tools, packaging, dependency management (Rust is great in that sense even for beginners), so to figure out how that's correctly done in cpp in 2023 would be hard. But again, for you, it's easy, you already know all that!Apart from that, I guess there can be libs that are great in C++ and still lacking in Rust.Also, need to consider if memory safety is really such a big issue for you and your future project.
Here is my advice on learning C++. Don't learn it first. Learn C first, become good at it. Then ease your way into using certain functionality of C++ in your code, and so on. Don't try to bite it all off at once. I've been developing in C++ for over 30 years now, and I'm still learning something new about it.Look at it like Mathematics. You learn Algebra, and Trig, but Calculus is a something different. If you don't have a solid understanding of the previous, you're not going to understand the symbolic manipulation of Calculus. It then gets worse from there as you go into Differential Equations.IMO procedural and imperative programming should be understood before trying to grasp polymorphic, object-oriented and functional programming. There is a lot of abstraction, and training of your mind to abstract is a process.Be careful, there are purists that exist out there, and they will scream and complain, but ignore them. Don't let others control how you think. A late professor of mine once told me something I thought I understood, but it didn't hit me until years later."Any program written in any language that produces the correct results is a correct program."As long as it does what it is supposed to do, it doesn't matter how you get there.Also, code purity is myth, and those seeking it are believers in mysticism and false aesthetics. There is good code organization, but when you're up against deadlines, good luck keeping it organized.This memory safety issue is important, don't get me wrong, but it is completely over blown. SpaceX built incredible rockets and systems in C/C++. We have external tools to check for these issues, and they still need to be used with Rust.
I think the biggest reasons to go with c++ over rust would be mature compilers and the relative abundance of mature libraries, and possibly your existing knowledge of the language.It seems like the bigger question is what are the goals of your project(s)? Like if gaining more experience with rust is a primary goal, then the aforementioned advantages are pretty much moot. OTOH if a project could benefit a lot from leveraging mature compilers and libraries, or if you can't afford the additional time investment associated with becoming proficient with a new language, then c++ may well make more sense.
The benefit of using Rust is that management of 3d party libraries compared to C++ is a lot easier with cargo, so you can do things quicker.The downside to using Rust is that its still an evolving language so you may have to refactor your code down the line. Generally Rust use case is targeted towards enterprise software, where you have lots of people working on it, and it needs to be fast, and you want to minimize chances of introducing memory bugs. For personal use, I don't think its honestly worth it tbh, it just slows you down.Personally, I use Python and C exclusively. Its pretty easy to write C extensions to Python to handle the things you need to go fast, and then let Python handle the launching of the code. For example, I was playing around with MCTS, and I would have the tree set up in Python, and then use multiprocessing to split the tree into x processes, and each process would launch C code that did the search a lot faster. Overall the implementation was super easy since I didn't have to write the low level C code for the initial setup, with memory management and so on.There was a recent video about using torch to compile custom CUDA kernels (https://www.youtube.com/watch?v=nOxKexn3iBo). I have implemented this for some custom data processing that I do that I previously had C code to do on the CPU, and its WAY faster using CUDA.
I've had to refactor a heck of a lot more C++ than Rust. C++11, std::optional, concepts and ranges, std::format, etc. It's not even that I dislike these changes necessarily, but you'realwaysrewriting C++ to keep up with the language.In rust it's mainly been nightly stuff getting merged to stable, pre-1.0, and async. Much more stable overall.
> you're always rewriting C++ to keep up with the languageIf that's what you want to do, which is a big if. I guess strictly speaking the same is true for Rust but in Rust, with it being new, it "feels" more like Ishouldkeep up (at least for me). In c++, I would mostly just not mess with old code that works...
I'm approaching this from an enterprise perspective of "how confident am I that this code is safe and correct?"If you hand me a pile of c++03 code that happens to compile, my confidence in it meeting both of those criteria is fairly low. If you ask the standards committee or bjarnes, what they're going to tell you is to rewrite it in modern C++, which constantly evolves.Contrast that with rust where it's probably fine if it compiles.
> standards committee or bjarnes, what they're going to tell you is to rewrite itThe same will be true for Rust in 50 years (assuming Rust doesn't die any time soon). If they didn't think it was worth modernizing, they'd just leave C++ alone (which I think they should, but that's another matter).For old C++, you run valgrind and a linter on it and if nothing comes up it's also "probably fine". At least it's hard for me to think of a situation where the most productive/profitable-for-buisness thing a developer can be doing is modernizing their old C++.
Would you bet your life/money/application security on valgrind and a linter being sufficient to catchallof those issues, knowing that a single mistake invalidates every guarantee the standard gives you? That's the situation we're in with C++.When you're building old code, the compiler can't check that it upholds the standard to the degree that it can for modern C++. That's why the advice is to update the code, because the tooling can't save you otherwise.Rust by contrast guarantees that you either have a compiler/runtime bug or the issue is more narrowly localized than "potentially every single line of code". Sure, I'd like better, but it's an improvement.
> Would you bet your lifeDefinitely not. But it's rare that so much is at stake. I would not bet my life on the borrow checker either, there's a lot of things that can go wrong in a program.Money is of course a question of how much. Application security is just "the above". The "undefined behavior can explode your PC, travel back in time and become your father" problem is not to be underestimated. However, in cases where old codeworks well for yearsbut really has some lurking bad UB, one can disable some optimizations and be conservative with compiler choice. A lot of people dislike the trend towards compilers being ever more punishing of UB and I can see their point, while it's of course hard to argue for (let alone find) a balance between performance and some robustness to ubiquitous and almost inevitable code issues.Rust's giarantees are nice and I sure would prefer working on a modern Rust project than any C++ project, as well as a modern C++ project compared to C99-with-classes. Modernizing existing code is a different matter. I've seen very few attempts at modernizing a big and old C++ codebase, no attempts where it clearly paid off and one attempt where it went wrong...
>If they didn't think it was worth modernizing, they'd just leave C++ alone (which I think they should, but that's another matter).What about C? That's being modernized too, sometimes with novelties from C++ modernization, like `nullptr`! :phttps://www.open-std.org/JTC1/SC22/WG14/www/docs/n3042.htm
The amount of new syntax and programming concepts being constantly crammed into C is much smaller. If I had to quantity it, I'd estimate that C++ is by now roughly 3.5 programming languages in one while C is maybe 1.5...The more stuff you have the more problematic (adding foot guns and making it hard to learn a language) it is to add more stuff because it all interacts. And a lot of C++ features interact in bad ways...
Consider cross-platform -ness. For example, if you make a new app with domain logic in C++, you can build it on iOS through Swift/Objective-C++ >> C++, or on Android through Java JNI >> C++, or on React Native through JavaScript Interface (JSI) >> C++, or on Linux through wxWidgets >> C++, or... etc.Other than that, Rust is a great language with a great compiler eliminating whole categories of bugs possible with C++.
I don't disagree that broader platform support is a reason to choose C++, but Rust is just as capable at integrating with the specific platforms you're talking about.
> Rust is just as capable at integrating with the specific platforms you're talking about.Maybe it’s possible but definitely not as straightforward to use rust on  iOS. Compared to C++.
I have not personally used C++ or Rust in this context, so I can’t speak to the ease of either, so could you say more?The use-cases I know of basically build a library exposing the C ABI, which is slightly annoying but effective. I'm guessing you're referring to Swift <-> C++ interop?
What I mean is that adding C++ files to your iOS project and having them compile and work together without any new compilation steps or settings, and to have Swift code call a C api on your C++ implementation -- all of this is straightforward to do for iOS, and Xcode even creates the "bridging" headers for you and has built-in support for C++. Now, adding Rust into the mix? That will be much more involved and certainly not as simple; nor is it natively supported by the iOS toolset in Xcode. That's what I meant when pushing back on your notion that "Rust is just as capable at integrating with the specific platforms you're talking about."
Thanks!I wasn't trying to claim that they're equivalently easy: just that they both have the same capabilities, there's nothing special that one can do that the other cannot.
In embedded development, sometimes gcc has been ported to many more architectures than rust(llvm) so unsafe either c or c++ (or fortran).
One I don't see mentioned here is market. There arefarmore c++ developers out there. Whether you're writing a library that someone else will be using, or you want to hire people (or have people contribute), c++ is a much larger "market".This doesn't mean I'm necessarily advocating for c++ or rust, but c++ definitely has advantages over rust, even if they're not just intrinsic.
Rust has good FFI with C++ meaning that you can write Rust libraries that are consumed by C++ programs.C++ developers understand all of the concepts needed to easily pick up Rust. As long as it's not a quick project that will be immediately thrown away getting C++ to use Rust will be worth it.
Rust has good FFI with C, and do does C++.  The Rust <-> C++ FFI experience is still a bit rough.
even C++ <-> C++ is rough as soon as you're dealing with object code...
Every language has its own merits, there's no single language that's universally the best for every task.With that said, C++ and Rust both occupy similar domains (high-performance, low-level, often interacts with OS syscalls or hardware).Pros of C++:- More mature, excellent library ecosystem, decades of code to reference from.- Syntax is arguably easier to read and write- It's very popular (top 6th language on GitHub), lots of talent to hire from.Pros of Rust:- Memory safety, it's much harder to introduce bugs of a certain type.- The borrow checker makes it easier to reason about lifetimes of objects.- Cargo is great for pulling in libraries (just needs more of them).For me personally (graphics / game-dev), cost and speed of development is the deciding factor. I use both: C++ by default, and Rust for low-level, safety-critical code.
>- Syntax is arguably easier to read and writeThis generally comes down more to familiarity than "ease".  C/C++ are familiar.
I usually hear about people using Rust with C. You use Rust and C++. If you use them together, how easy is it to call Rust from C++ and vice versa? And can you do that with their standard interfaces instead of watering it down to the lowest, common denominator?
You really want to be using C++ and Rust as little as possible - ideally only for high-performance code. The rest of your application should be developed using a RAD language like C# or Kotlin. Interoperability between C# and C++ right now is really great with first-party build tooling support from MS. Rust, as far as I know, doesn't have a great interoperability story with any other language except for, ironically, C++ based on this[0].[0]https://www.hobofan.com/rust-interop/
If you need to interact with existing C++ code or libraries obviously C++ is going to be a much easier path than using Rust. If you don't have such a requirement, or you think that the available Rust libraries are good enough for what you're doing, then use whatever you want.
This is it. C++ has the huge advantage of being able to use the entire ecosystem of existing C++ and C libraries without any kind of bindings.When you'd need to make your own Rust bindings or rely on some third-party project which may or may not be actively maintained, then C++ is more attractive.
I just started a new project in C++ for two reasons:1. I already know (some) C++2. more mature GPU ecosystem, particularly CUDA supportI don't mind learning new languages, so CUDA was the deciding factor for me. Maybe someday as Rust's GPU support improves I'll consider a rewrite.
Some arguments in favor of C++:1. Much faster compilation speed.2. Having to syntactically specify ownership is annoying and a waste of time in many cases. Unless you need the hand-holding, but then low-level programming isn't for you. :P3. Multiple compilers. Ironing out bugs is easier when you have both clang, gcc, and msvc.4. Valgrind and other tools for debugging, profiling, and analysis.5. Personally, I find Rust's syntax obnoxious and cluttered. With C++, most of the clutter can be avoided which I like.
3. and 4. sound a bit like like "For C++ you have nice tools that solve problems that don't arise with Rust in the first place"
That's the price you pay for #1. ;D
IMO one of the biggest downsides to Rust people don’t talk about as much is that there are a lot of abandoned and incomplete crates.  The same is of course true in C++ but there is a very mature ecosystem there as others have pointed out.The second thing I’d point out is that while Rust’s build tooling is obviously very nice for some cases, it’s not a silver bullet and C++ build _is_ a solved problem.  It just can be a bit more of a lift to get rolling and add new dependencies, but these are infrequent costs.  Adding a Rust component to a non-Rust build chain can be very painful, however.  Cargo really wants everything to live in Cargo.Last but not least, C++ also has extremely mature and comprehensive static and dynamic analysis tooling.  It may be that Rust has begun catching up here, I’m not certain.
No one in the comments mentioned OpenACC or OpenMP offloading, which is the main reason for me to start new projects in C++ (or even in C/Fortran for simplicity).
C++ seems easier to work with, tools for C++ are plenty and more mature, so if productivity and speed of development matters, those are good reasons to use C++.
While I can imagine that familiarity with tooling can be a massive boon, I have to say I tend to pick Rust over C++ for the exact opposite reason. I find Rusts tooling a lot easier to use than the C++ tooling. Clear compiler messages showing what's wrong, far easier error handling and logging, and far easier dependency handling through cargo.On the other hand there's of course the borrow checker which means you need to think a bit more about how you do your lifetimes, which can slow you down a bit, but I find that that prevents me from writing bugs more often than not.
Plenty & mature != Easy to work with. Ever had to run make from within CMake? (so that's a build system generator running a build system while generating a build system...)
Ultimately, the core argument for Rust isn't technical; there's nothing one can do in Rust that can't also be done in C/C++. From the point of view a single experienced C dev Rust might look like an overcomplexification.The real gain Rust brings is in the social dynamics of programming. Junior coders can be involved with low-level code with much less chances of breaking stuff. Seniors can push the envelope further while keeping the code maintainable by future contributors.In general, Rust sets a much more level playing field. This is a major advantage for any long lived project but it can be hard to see it if you're already set in the ways of C and unconcerned about the lifecycle of code outside of your hands.
Familiarity and library support would be the two obvious reasons.
Direct integration of C source code. Other than that, I can’t think of anything else.
As a random datapoint I learned recently the Solana blockchain consensus software, written in Rust and notorious for absolutely dismal performance, is being completely rewritten…in C.I’m curious if they’ve thought that through as it introduces an entire class of bugs Rust inherently protected them from previously.> Firedancer is written in C, in contrast to the Rust codebase of the original Solana validatorhttps://firedancer-io.github.io/firedancer/guide/firedancer....
Rust has a method for enforcing better memory safety. That is great for deployed applications, but can be annoying when you’re still exploring / mutating your code to figure out the right shape of things.
I have never had the experience that being precise about what I mean slowed me down, if anything it was the opposite.
because sometimes you don't know precisely what you mean. if you don't already know the shape of your solution the 'safety' features restrain you.
Existing library support. The rust ecosystem in some domains (Large parts of embedded) are immature. And, (hot take), while generally Rust's syntax is nicer than C++'s across the board (Not having headers, clearer struct and function syntax, array refs vice pointers etc, avoiding scattered/global state), the friction generated by generics and async in some parts of the ecosystem negates them.So, if you use rust on a given project, you may end up re-inventing things that could be libraries, due to lack of support, or the libraries having missing features or a poor/poorly-documented API. Normally, auto-generated Rust docs are a good way to learn a library, but generics negate it by ending the link trails.I think you'll find the tooling, and (non-async, non-generic) syntax of Rust are a step up from C++.
I like C++ so that's reason enough for me.
1. When you need the larger ecosystem of existing/proven software and available developers, then C++ can make more sense. If you are put off by the "NPM-esque" environment (crates.io), that may also be a factor.2. Can't speak to your situation, and in any case Rust and its "developer experience" are always evolving/improving.3. If Rust is an option, choose Rust over C++. That's an opinion, but it's quite firmly held.
C++ has DECADES of content to use. even if you filter everything that's outdated, the sheer quantity of libraries made in c or c++ first and then having either rust bindings or a third party conversion to rust as a hobby project is orders of magnitude bigger than the ammount of libraries and frameworks made with and for rust.Older c++ programs also had the "perk" of compiling and running even when there was some memory leak or other problem and, in production, many times the deadline is so strict that a developer has to ship something bugged and just add a note somewhere saying "if you use function X, it will freeze or crash". the Rust Compiler won't let you ship a program without fixing the problem first
From my perspective, if you really need the safety guarantees of Rust, use Rust, but if you are more comfortable with C++ and can be more productive with C++, use C++.Doesn't really make sense to take a productivity hit for small gains in a context where safety isn't a necessity.Just two cents from a junior person :)
Do you care about bootstrappability? I have a side project that I am writing in a very old-fashioned way (C99, autotools) specifically because it needs to participate in a bootstrap chain. Rust doesn't have a good story for that.
There's basically no reason to use C++ over Rust in 2024. Other than personal preference, I guess (I grew up with C++ and so it's nostalgic, but it's no Rust).
I think the question could be answered with... why do we use JavaScript at our company? Because there are a lot of libraries that are available and maintained there. Many times the existing libraries are more important than the programming language itself.
Because you're also curious about Zig?
No, but nice try. ;]
Firstly, my background is in Haskell and Rust. I generally defer to safety-conscious languages. So please no responses extolling the virtues of type safety, type systems, etc. I know, I agree, but that doesn't change what I'm going to say.Rust is awesome, but honestly modern c++ is quickly catching up. Rust has safety going for it, but one of the issues is that sometimes you have to be more inefficient for it (reference counting), and pay the price of code litter (Boxes everywhere).  Also, realistically type safety takestimeto think about (although not much). Either way, you will spend time playing type golf before your code even runs and compiles.C++ is strictly more powerful, even ignoring the memory safety. Modern c++ with its smart pointers basically has rust ownership semantics builtin, with the option to fall back to manual memory management, which is sometimes necessary. Modern c++ has concepts, template metaprogramming (MUCH more powerful than rust), constexprs, etc. This alone, for me, has made developing interpreters and compilers much easier in C++ than Rust.Finally, after almost a decade being a professional Haskell developer I have come to appreciate the benefit of being able to prototype fast. The truth is, type-safe languages are great for final work, but having the option to throw it all out the window to just get something done, is also really nice. In my ideal world, we would have a language with well-defined semantics for ill-typed programs (even with undefined behavior) and then an option to strict-ify the type-checking when releasing. Yes, I know rust has unsafe, and this is good, but maybe I just don't know enough unsafe rust to be efficient. I find it not obvious to use. Haskell of course has a REPL which (in my opinion) makes it easier to prototype in than both Rust and C++, so while it has type safety, the REPL makes prototyping easy. Without either a REPL or an untyped fallback, I think Rust takes more time.One more thing... Rust has great abstractions, but unlike the Haskell world where abstractions are taken to their logical conclusionsfast, it seems to me rust is more conservative in its approach (multi-param traits, for example). This seems to have worked out in terms of popularity. But for me at least, when working with Haskell, I often start to reach for yet more advanced features that reallyoughtto work. Usually, when that feature is not implemented yet, I will find an approved GHC RFC for it, and just wait for the next version of the compiler. In Rust, it seems things move much more slowly. I have found myself endlessly frustrated with what I perceive to be weaknesses in its trait system (no existentials... :( ).At the end of the day, both are solid programming languages, with Rust being much better for anything dealing with business logic, while C++ is better for projects involving heavy meta-programming. I have personally myself done a lot of prototyping in Haskell (due to the REPL) and then implementing the runtimes in C++. Rust I've used for one-off tools, utilities, and anything to do with high-perf async IO.
Is there a reliable way to enforce some subset of C++? I feel like the biggest disadvantage of C++ is too many features and their unpredictable interaction.
Agree with your assessment of C++. A lot of things have improved in modern C++ over the last 10 years, but C++ has a serious PR problem. Part of this is that it's been changing pretty quickly and people wonder if they should come up to speed on modern C++ or just learn something new (like Rust or Carbon) and in many cases people just decided to do the latter because coming up to speed on C++20 is basically like learning a new language anyway.Still, the remaining big deficiency that C++ has now over more modern languages like Rust is library management. The C++ standards folks seem to say every couple of years that library management is the next big thing on their schedule for the next C++ release, but it gets delayed until later.
OOP
FWIW, the creator of this course is a team lead at NVIDIA, and principal software engineer of CUDA cuSPARSE.
With the context of learning Modern C++, and the author's association with nVidia and CUDA, I share this anecdote: Partially if it helps anyone directly, and partially as an opportunity for someone experienced with C++ to poke holes at it.I've had great success using ChatGPTv4 to help me write modern C++ that works with CUDA, using prompts like these. (I have not attempted to get it to write parallelized code given loops/serial code as an input):> Please convert this Rust code to modern C++, compatible with the latest version of the nvcc CUDA compiler: ```rust ```It will output a compilable/working result, and explain each of the language differences in question. It will make sure to use `std::*` instead of the more rigid C-derived equivalents when able, and take advantage of looping over array pointers etc, vice i++ loops. `auto` var types etc.
Any ideas where to start learning C++ as an embedded developer? I wrote many lines of bare metal C code and want to transit to higher level jobs. I see many expensive or completely free courses, but I am not sure which one is usable in my complicated situation.
It depends on what you target. I do alot of Kernel and really low level C++, and it's a completely different style than even app dev, so you have to be sure you don't go down the completely wrong route.For my target space, there's a few rules for c++.Footguns:
A. No STL, it causes to much bloat when binary size is critical.
B. (almost) No inheritance or virtual functions. Again, it causes some bloat and adds in some instructions and code to support these features that can cause issues in low level environments.Things to learn (All work in the kernel):A. Learn templates, especially c++20. Templates add ZERO bloat, they literally just generate the function again with proper types the same way a macro would.B. Use RAII.C. Use constructors and overloading.D. Zero pointers should be exposed to code except where absolutely needed. If you need to pass by reference, use the c++ style.E. All allocations should be wrapped in a class with RAII. You can overload the operators on the class to make it seem like a pointer so you can still do if(!RAIIPointer).A good video on kernel c++ is herehttps://www.youtube.com/watch?v=AsSMKL5vaXw. You can use a surprising amount of C++ language features in the kernel and it makes it extremely smooth and safe if you do it properly.
I would add constexpr (I guess it might be included with templates?).You can do some nifty stuff like generate CRC lookup tables at compile time without hardcoding them.
Yes! 100% Right, can't believe I left that out. One of the big things is to make sure to use ConstEval if you can do a new verson of c++(20), since ConstExpr isn't guaranteed and can leave stuff in the binary that can hurt size or obfuscation. For example one thing I see alot is lookup tables for hashing who use constexpr and hide some key data in there, but it "may" just randomly do it, and can cause alot of issues.
How does the use of ConstEval address this issue?
> No STL, it causes to much bloat when binary size is critical> A. Learn templates, especially c++20. Templates add ZERO bloat, they literally just generate the function again with proper types"Avoid STL due to bloat but embrace templates" doesn't make sense to me.
Isn't that exactly what code bloat is? Too much code generation?
Bloat can also refer to the invisible infrastructure code that's compiled into a binary when using certain language features. C has a fairly linear relationship to the size of the compiled assembly, iirc, but C++ will vary wildly based on which features you use.
Indeed, it is a common refrain among C++ programmers to inspect ones binaries, I have found ..
I think you missed exceptions often being a problem in low level and embedded targets.  That knocks out most of STL anyway.I also think you are a bit harsh on virtual functions - it introduces a single indirection, yes, but sometimes that is fully justified.  RTTI on the other hand...   of course depends a bit on target characteristics.Perhaps controversial, I've also found (especially bare) lambdas and even std::function objects useful, although may evolve into something purely static when the dust settled.  Highly dependent on target of course.It will be interesting to see the final form of the new MISRA std, since the active one predates all of this.
If you can't use the STL because of exceptions:https://www.etlcpp.com/
Sure, there are more embedded friendly libraries, but that wasn't the topic.
It’s almost as if this is some sort of discussion board where people can expand on the topic as they see fit :D
You can use quite a lot of STL even with -fno-exceptions. Things that you probably need to ditch are:std::vectorstd::liststd::mapstd::unordered_mapstd::setstd::unordered_setstd::sortstd::transformstd::mergestd::stringstd::regexstd::swapstd::functionstd::stable_partitionBut there are much more than that in STL.
most (useful) containers are out, what else are left in STL for embedded systems, algorithms?
True:  "most" was a stretch.
newest MISRA for c++ just came out, though it's based on c++17
If you want to understand the C++ in terms of C code then probably you would need to understand the C++ object model. I mean, how C++ classes are translated into C equivalent code. If you are interested in these you can take a look at a tutorial I wrote long backhttps://www.avabodh.com/cxxin/cxx.htmlIf you want to write bare metal C++ then this page from the above tutorial will be usefulhttps://www.avabodh.com/cxxin/nostdlib.html
Well, you seem to be precisely the target audience for the course linked in this very post:> This open-access course is directed at those who are already familiar with C 
    > and object-oriented programming towards a proficiency level of C++ 
    > programming.
Maybe, in this context it makes lots of sense, if you know C well there are things which "just work how you expect" in C++ and so they need at most a brief refresher.I agree with Kate Gregory that this is a bad way to teach C++ from scratch. That is, if you know only say, Python or Java (or nothing) and want to learn C++, you shouldnotbegin by learning C. Kate's approach skips many of the things youcando in C++ but you shouldn't, whereas if we try to start with C you're going to see that a lot, because "they were legal in C" iswhythey're allowed in C++ even though they're a bad idea and then you have to unlearn things. You're maybe expecting some of that if you've years of C programming experience, but there's no need to introduce an idea on Monday only to tell students on Tuesday that it's a bad idea and they must never do that.The most insidious cases are where something is legalandcorrect in C but it's wrong in C++ yet it will compile and you maybe have broken software. There are some nasty type punning examples, where the pun was an idiomatic C expression which does exactly what you wanted, but in C++ that's Undefined Behaviour and all bets are off. You'd just never teach those type puns in a pure C++ course, they're never the Right Thing™.
There is a considerable divergence of opinion on that subject. In my view, C++ isn't remotely suitable as a programming language for someone without a healthy understanding of C.  Perhaps a dialect of C++ could be developed that was more of a cousin to Rust, but C++ as we know it is a C like programming language with a very large number of features added and all the ways to fail just like a C program does. There are real world advantages of course, but it is not a language for the faint of heart, not even close.
Depends what you mean by "understand C". As you say, understanding the C abstract machine and memory model is critical for a C++ programmer.Understanding C idioms, the standard library, best practices, and general C software architecture , is less important if not downright negative early in your formation. You will end up picking a lot up anyway if you stick to C++ long enough.
>  is less important if not downright negative"Less important" maybe but not "negative".You have to have a decent C base (not necessarily expert level with knowledge of dark corners/tricky idioms/etc.) before you start with C++. One example is being comfortable with raw pointers. I often see "Modern C++" proponents say you should never use (and by inference learn) raw pointers which is absolutely counter-productive. It is also easier to learn C++ as a "better C" in the beginning else it becomes overwhelming. I believe this is the main reason most beginning C++ programmers find the language "scary/difficult/huge/overwhelming/confusing". They are trying to learn everything at the same time which is an impossibility with a language as baroque as C++.
The main use of raw pointers in practical standard C++ is that they still don't have an analogue of Option<&T> so raw pointers (which can be null) let you write analogous code albeit in a less friendly way.But this isn't so much like C, where raw pointers often express ownership.You aren't going to really learn "everything" in C++ anyway, regardless of how you approach it, the language is a vast sprawling mess, the fact somebody wrote a serious bookjust about initialization[https://leanpub.com/cppinitbook] in C++ gestures at the problem. Freeing themselves of the need to have a language which is well-defined, or which can be demonstrated to be sound, or really follow any principles whatsoever was doubtless brieflyconvenientbut the result is unmaintainable nonsense.
The use of raw pointers in C++ which you deem analogous to a Rust feature is your view and not that of the vast majority of C++ programmers. Raw pointers in C++ are the same as in C and the usage techniques are up to the programmer.As a diehard proponent of Rust your views on C++ are well known and there is nothing new here. But the fact of the matter is that the industry runs on C/C++ and the main reason is due to its baroque set of features whatever one may think of them.
By "negative" idioms I'm mostly referring to stdio, string.h (except for memcpy/memmove of course), goto-based cleanup, void* based generics, overuse of macros and a certain fondness for global mutable state in a lot of classic C codebases.
Disagree here. Given the language's limitations, everyone of the idioms you list has its place and uses. They are just a way of structuring code for different abstractions.
In C maybe, but not C++.
That's not what i was saying (it's a given). After all C++ was invented to provide programmers with a large number of features that would allow them to express abstractions naturally and directly rather than building them up with all the basic plumbing that you would need to do in C.However there is a huge amount of C code and programmers who would like to move to C++ and it is for them that the techniques of C coexisting with C++ are very relevant. There can be no wholesale rewrite of the code into C++ but a gradual rewrite module by module and as needed. Using C++ as a "better C" is the path here.
Can you maybe share some paid/unpaid resources to learn C++ from scratch for someone coming from Java and JS?
Kate has a PluralSight course which I have watched (not really to learn C++ but to see her approach) and that seemed decent to me, although of course I can't vouch for it having taught what you'd need coming from Java and JS, not least because my background is far more polyglot including Scheme and the Standard ML for example.I definitely think Bjarne Stroustrup's book about his own language isn't very good, even in later editions (which correspond to a more modern language) and so I would avoid buying that, I'm not sure it's a good defence of the language as an idea and I'm sure it isn't an effective tutorial, even if not for being out-of-date.
That doesn't address the constraints that are typical in embedded systems such as limited memory (RAM) and the expectation that a program will run forever.There are resources oriented toward embedded C++ that address these (at least memory allocation.)On a project a couple years ago I was pushing to use C++ instead of C, if only as a "C compiled using the C++ compiler" for better type checking. I could not convince the project lead, who's technical depth far exceeded mine, that the C++ compiler was as likely as the C compiler to produce correct translations.
I wouldn't worry about the compiler producing correct code, I would worry about the heap eventually fragmenting enough that an allocation fails (perhaps thousands of hours after the program launches). I know C++ offers the ability to supply your own allocator, but it's not for the faint of heart.
I will go through the course before Xmas. The thing is that in bare metal development even C standard library is not always being used. Recently I wrote some string processing functions for very specific parsing task. Nobody would need such crap while working with operating system.
1)Real-Time C++by Christopher Kormanyos. - For using C++ in embedded systems.2)Software Architecture with C++by Adrian Ostrowski and  Piotr Gaczkowski. - Gives the overall picture of how to use C++ plus relevant modern tools as a complete development system.3)The C++ Programming Languageby Bjarne Stroustrup. - The bible/reference with programming techniques for the language.Learning C++ is not difficult and don't let the size of the language intimidate you. Survey the different ways of programming in C++ i.e. Imperative/Object-Oriented/Generic/Meta-programming and use them as needed in your project without trying to master everything; that will only happen over time as  you gain more experience.
You can definitely start putting C++ into your embedded projects, and get familiar with things in an environment in which you're already operating.  A lot of great C++ code can be found with motivated use of, for example, the platformio tooling, such that you can see for yourself some existing C++ In Embedded scenarios.In general, also, I have found that it is wise to learn C++ socially - i.e. participate in Open Source projects, as you learn/study/contribute/assist other C++ developers, on a semi-regular basis.I've learned a lot about what I would call "decent C++ code" (i.e. shipping to tens of thousands, if not hundreds of thousands of customers) from such projects.  I would suggest finding an open source C++ project, aligned with your interests, and study the codebase - as well as the repo history (i.e. gource) - to get a productive, relatively effortless (if the interests align) boost into the subject.(My particular favourite project is the JUCE Audio library:https://juce.com/.. one of many hundreds of great projects out there from which one can also glean modern C++ practices..)
This is maybe a very unpopular opinion, but if you go from C to a higher level language, I would advise to steer away from C++, certainly if it's would be your first OO language.
I’d recommend reading Effective C++ books from Scott Meyers and trying to interview for the jobs you’re looking for over taking a course!
If you want to transit to higher level jobs you may get better mileage out of broadening to other languages (e.g., python) or skills (e.g., project management) than from a C to C++ transition. My 2c.
Do you want to do embedded/low level c++, or are you looking to transition out of that?
I will get out. Application development using C++ and Python. Maybe C#.
Anyone went through this and can compare it tohttps://www.learncpp.com/?
learncpp might be more beginner friendly and more verbose
I picked C++ back after a long break and I have to say in reasonably recent iterations (like C++17) it's good enough, although OOTB Unicode is a PAIN. They should have solved it in C++11 and moved on. It's a disgrace.
Does Carbon or c++next fix the ootb handling?
It's being actively worked on for C++2x. This video is a fairly recent update on how it's progressing:https://www.youtube.com/watch?v=AoLl_ZZqyOkNote for those who are not used to C++Now talks: they are intented to be a bit more informal with regular interjections from the audience (who at C++Now are also often heavily involved in the development of the C++ standard), so don't think people are being rude by jumping in!
This is just an idea at this point. Ask again in 30 years.
Hopefully it's really good in the near future.https://thephd.dev/cuneicode-and-the-future-of-text-in-c
Just what I needed to brush up my long forgotten from University C skills, but I would prefer practical coding tasks for each part.
> Heap Memory - new, delete KeywordsShould "modern" C++ even use new/delete? I'm a C++ n00b, but I thought these can be avoided entirely now.
I noted that - IMHO one of the big advantages to "modern" c++ is the focus on the lifetime of objects with smart pointers rather than the mechanics of heap allocation and pointers. Pushing that down to chapter 19 as "Advanced Topics" seems like a mistake. Arguably the same with std::array vs object[] - the first should "probably" be the default now for teaching new programmers, so covered first (or at least highlighted when discussing array[]-style variables)I also note a number of the possibly-ambiguous c-style casts in examples, despite already having mentioned the explicit versions (static_cast and the like).Plus there seems to be a fair bit of system & machine dependent stuff mentioned like it's /always/ true - like stack being at a "higher" address space than the heap and code sections, or the stack growing down. It also says that the Data section is "slower" than the stack which... isn't true? If they're spilled out of registers, they're likely functionally identical? Or at least for small microbenchmarks depend on specific implementation details, like how the pointer to the object is calculated and dereferenced.... And object size and cache interactions... And a million other things...And that's just the first few chapters I looked at :P
Smart pointers should really be like, chapters 2-10.In my experience, every useful C++ application requires wrangling some historic smart pointers implementation: Boost, Unreal, libwebrtc, Qt.Even NVIDIA uses its own smart pointers (thrust::device_ptr) though that's understandable. Also there's stuff like this:https://stackoverflow.com/questions/65938991/how-to-use-lamb...for e.g. TensorRT.
Yes, in new code you should be able to replace almost all new and delete with unique_ptr, vector or, more rarely, shared_ptr. Placement new can still be useful, but delete not really. There are some patterns where you use new without a delete for a global singleton that is intentionally never deleted, as well.It's still useful to learn about the concept when dealing with old code.
Not entirely - you can't generally use std::make_unique with private constructors.
It's explained later in the course
I think nowadays smart pointers should not be considered an "advanced topic" in C++. Smart pointers are usually the best way to handle memory management.It's definitely useful to learn about "new" and "delete", because those are the primitives that memory management is built on top of. But it should be followed up with good advice like, rarely use these in practice. You should almost always be using unique_ptr or shared_ptr instead of new'ing up pointers that you intend to explicitly delete later.
> You should almost always be using unique_ptr or shared_ptrI really wish there was a thread unsafe version of shared pointers, without atomics overhead, in the standard. Maybe without weak pointers.
If there's one thing C++ needs, it's more slightly-different varieties of smart pointers in the standard. ;-)What exactly would you use these for? I find that usually I can get away with unique_ptr for the sort of object that I have many of where pointer performance matters.
Some sort of problems where you throw trees around. Almost interpreter like programs, where trees are arbitrarily stored on different places. And where you for some reason don't want a proper GC ...
This course is a bunch of presentation slides.  The idea that you can learn anything from slides is rather silly.  Learning from slides is almost as bad as learning from random youtube videos.
Slides can be an extremely good medium for learning. Because brevity is required by the format, slides can require a lot more thoughtful attention to emphasize and carefully explain key points than the long-form text format. I think the course looks really nice!
In fact when I read a book I do little cards, like slides, with the most important information condensed.
I find a good method.
All channels with reputation at some point were random (except the ones from universities maybe)
YouTube videos are fine, at least they have narration. With slides you're kinda left to infer your own story and lessons.
"A Little History of C    3/3" says that C was used in the special effects for Star Wars. I do not know what relevance this has, honestly, but independent of that is the strange choice of photo: an image from the Empire Strikes Back which shows stop-motion models with optically-composited, rotoscoped lasers. Excepting that this may be from the digitally recomposited Special Editions, no C code was used in the making of this shot.
Watch Lights & Magic on Disney+ and you will understand why the reference is in there. ILM, Pixar, WETA had huge influx on graphical swe... hell even photoshop was a sideproject of ILM at the time.
Isn't the wireframe death star scene the only part done in C (and therefore by computer) in the first movie?https://cdm.link/2021/11/watch-larry-cuba-explain-how-he-ani...
Yes, done by Ed Catmull himself.
Care to enlighten those of us who don’t already know the explanation?
ILM was and is an absolute software engineering power house and they started with motion controlled cameras for star wars at the time all their in house software was c++. A lot of software which is still used today. Pixars Renderman, Photoshop, Maya... was initially engineered at ILM or Lucasarts which were subsidiaries of Lucasfilm.
Maya was an Alias|Wavefront product, which was a subsidiary of SGI. It came along way after Pixar had been spun out of ILM. John Knoll, who co-developed Photoshop with his brother Thomas, is still at ILM as the CCO.
Thank you. That makes sense
Can someone explain how to read the Conversion Rules section in the second chapter? I haven't seen this style of notation before. For example:Implicit type conversion rules, applied in order, before any operation: ⊗: any operation (*, +, /, -, %, etc.)
    (A) Floating point promotion
    floating type ⊗ integer type → floating type
    (B) Implicit integer promotion
    small integral type := any signed/unsigned integral type smaller than int small integral type ⊗ small integral type → int
    (C) Size promotion
    small type ⊗ large type → large type
    (D) Sign promotion 
    signed type ⊗ unsigned type → unsigned typeEdit: oops, I missed the explainer. The ⊗ stands in for any operator in case that was confusing to anyone else who missed it. :-)
Is there a good guide on the toolchain? What do people use today to keep sane? Something like meson, ninja or cmake? I have inherited some project in scons that was generating msvc 14.1 project and I can heartily recommend against that
Meson or CMake. Neither one is really all that great, I would say personally that Meson is better in theory but CMake is more widely supported so I prefer Meson but it's pretty even. Either one is better than just an enormous makefile. Ninja is a different level of the stack, you can use meson+ninja or cmake+ninja.Either way the C++ dependency management situation even makes the Python dependency management situation look good by comparison.
> Either way the C++ dependency management situation even makes the Python dependency management situation look good by comparison.Is there a better way than “script the compilation of all dependencies and share the result” for any big project that has to support Windows? I’m interested in alternatives.
At least on Windows I quite like vcpkg for dependencies.
I've been setting up my little docker image that has both llvm/clang/tidy/clangd and gcc inside with boost with an idea I'd run it and from outside use it to code with live checking and compiling without littering my OS. Now, I feel like I'm doing something that already exists but couldn't really find what everyone use, ought of those that would prefer such a setup. Debugger within is probably a pipe dream, but who knows.Basically I'd prefer to have a docker image with tooling that I can hook into.
I find it absurd that a programming language which compells someone to set up a containerized OS just to manage their build toolchain could be considered "modern".
I install Rust's toolchain and development tools inside of a container, I also do this with Python and will do it with C++ sometime in the future.I don't do this because I have to, I do this because I prefer to keep non-system -critical software managed by non-root users and separated from the systems rootfs.On your common desktop Linux distro, I think C and C++ toolchains are the least difficult to setup and use without a container though (for me). On Gentoo I can just emerge gcc or clang and enable whatever USE flags I want, and they are installed and updated automatically with the rest of my system.I use the Gentoo system package manager to manage my Rust toolchain as well instead of using rustup, so that it behaves like described above, it's updated and managed automatically and with the rest of my system!I do realize that many distros have issues with software being out of date though, and that is a big problem! With Gentoo I can install multiple versions of most things in parallel and can very easily package anything that doesn't exist yet.Also to clarify, I use the system package manager to build and manage my containers, this is how I use the system package manager to manage Rust's toolchain but also have it inside of a container. All of my containers are just nested Gentoos that I can install stuff into with the system package manager. I can also install a package manager into the nested Gentoo and build/install stuff while "inside" of it.
> I install Rust's toolchain and development tools inside of a container, I also do this with Python and will do it with C++ sometime in the future.The thing is, you don't need to do this with Rust as far as I can tell. There may be some benefits, but ultimately your project can easily specify its own compiler version, its own target directory (the default is per-project), etc. There are some shared resources like caches, which you can split if you want to.I can see why you'd still do this - but, the main reason would be... if you have dependencies on C/C++.
> On your common desktop Linux distro, I think C and C++ toolchains are the least difficult to setup and use without a container thoughIn my experience, the hard part is rigorously controlling the libraries a build uses. Using CMake, it's easy enough to add libraries to a build, but harder to stop the thing going off and looking round /usr/lib64 and so on. On my physical workstation, there is all sorts of stuff in there, because i have a desktop environment and a cornucopia of tools installed. I don't want a build using any of it! If a build needs a library which i have not explicitly added, i want it to fail, not use something from the system. But between default paths and rpaths in libraries and so on, that seems hard to do in a watertight way. I've done endless fiddling with sysroot flags, but i'm not sure it's not leaking. A container takes care of all that in a very definite way.
I'd be really interested in a write-up of your setup with more details so I can try it.
I cannot disagree. Gcc is trivial to compile and set with prefix however you want. Llvm behemoth, not so very much though and I'd like to have both for reasons. There's always some gotcha involved, and when you finally set it all up, you forgot what and how you did it and then you dig through shell history file to reconstruct what you did in order to replicate it on other machines or same if you nuke the OS.. blah. Rustup and cargo and pyenv and nvm even spoiled us.
It would truly be absurd to claim that C++ is a modern language. A subset of C++ however, is modern. This does not include the toolchain and standard build practices.
Your point stands, but it is/was implied, how I took it at least.
I guess that's why i went into JavaScript instead of java. I use to write HTML back when i was around 10 years old, back then java applets were the only way you could have something interactive and dynamic on your page, so i started to learn it. After a while i learned that you could do a lot of neat stuff with JavaScript. So i stopped using java.
VS Code is designed to work with these -https://code.visualstudio.com/docs/devcontainers/containers
Interesting, thanks for that. That's kind of the idea, but preferably without the 'extend' part on top of containers. I'm not using VSCode though so I'll shop around or maybe I'll convert, who knows.
Maybe nix (https://github.com/NixOS/nix) is a better tool for what you're looking for if you're on Linux, you can setup nix shells and work in them, what's installed inside the shell won't be accessible from outside.
As is with most people and nix, it's on todo list to check it out. This might be the trigger. Thanks!
One thing that would have helped me when I started learning C++ was learning the C++ compilation model i.e. translation units.It is surprising how many people look surprised (that also claim they know the language) when you tell them that code in .cpp does not get inlined into other .cpp files no matter what doing compilation. (yes the linker can (and should) do that with LTO doing linking)
Compilation units are mainly about symbol visibility and as you already realized at the end of your comment don't have anything to do with inlining as far as the C++ standard is concerned. Neither does the inline keyword btw.The linker doesn't inline anything, LTO/LTCG is about running (part of) the compile process at link time. But that's really no concern for the C++ code but an implementation detail of the toolchain - you could just as well not have a separate link step and instead have the compiler process all compilation units in one step.
yes, LTO itself does not inline but from a user point of view it does, and that is exactly my point. If you are a beginner and you do not come from C, then you might not think about these things (maybe you do I did not).I get that it is just an implementation detail, but so are many things in C++ and you usually care about these things otherwise you would not be using the language in the first place.Many projects also disable exceptions that is also an 'implementation detail' (or at least something that is definitely not required by the standard I would imagine), but now you are technically not writing C++ anymore.I guess this is also one important thing when learning C++. The community is very fragmented, and there seems to be disconnect between C++ users and committee.
This also helps understanding why template code completely wrecks compiletimes and ram usage since the compiler cant share template instantiations. This becomes very relevant if template metaprogramming is used in bigger projects.
That's not why. Template processing happens during processing of the translation unit, so it would be expensive even if you only had a single TU in your build. It's true that a template has to be reprocessed for each instantiation, however, that's not merely from one TU to the next, but even inside each TU! For every distinct value of T for std::vector<T> in a single TU, the compiler has to process and generate std::vector entirely
What is the C++ job market like? I do mostly Python / web app developmnt, but I don't like the churn.
Having “spent some time in the job market lately”, the salaries seem astronomical if you’re C++ and fintech/HFT. I think some funny business is going on with those listings though…Kinda average in game development and sadly pretty low in embedded systems.All way less common than your average Python position, but probably way more interesting.
I'm not sure if you're in this field specifically, but since you mentioned it I'll ask. Is it your impression that fintech and HFT shops would possibly start to transition to Rust?I've always heard that they're currently mostly using C++ (and sometimes Java). But I'm not sure how dogmatic or change-averse they are on average.
If you are strong in C++ you can get pretty good jobs in robotics, trading, game engines. 
But TBH at that point it might be more interesting to learn Rust that is on the rise across the industry.
Nobody (except hobbyists and maybe an indie or two) is using Rust for game development, and it's unlikely they ever will. The cost of switching far outweighs any benefits in an industry where memory safety is not critical.C++ is here to stay for a very long time.
Treyarch gave a GDC talk in 2019 that they were using it for tooling, Embark hasn't shipped a game in Rust yet but is larger than an indie. You are absolutely right that it is small right now, but "unlikely they ever will" is a more open question, IMHO.Memory safety is absolutely important in certain kinds of games. At least, as a player, I want games to not be exploited for cheats, and I don't want progress lost when a game crashes due to memory issues.All that said, I would agree that if you want a job in games, learning C++ is a better idea than Rust right now.
No not strong, just considering making the switch.
C++ was my weapon of choice in college mostly because writing graphics stuff in C required re-inventing the wheel a lot of the time for basic data structures and the STL was pretty slick.Having spent the better part of the last decade writing go though, I think C++'s syntax is a bit too cluttered.
If you prefer to read a book rather than read slides:http://www.icce.rug.nl/documents/cplusplus/
I get 404d on that page. For others: you can download it (in various formats) through here:https://fbb-git.gitlab.io/cppannotations/(You might have to jump through some links)Also, fancy seeing someone else from RuG here :)
Just read the first pdf. This is an interesting quote."Every second spent trying to understand the language is one not spent understanding the problem"Did a lot of work in LaTeX the last few years and they make C++ looks like something made in heaven
I did not enjoy the compile-edit-debug cycle when I was using C++. Have there been any advances in the area off a C++ REPL?
This brings back horrible memories at uni.
How many interesting SE jobs use C++ anymore?Is anyone using it for new projects?
Compilers , OSs, browsers, VMs, Databases, scientific experiments, high performance systems, HPC, video games, complex desktop applications, simulations.In fact the question should be: what interesting projects don't use C++?For some very subjective definition of intersecting of course.
This is obviously very subjective, but C++ is used in many interesting fields: robotics, graphics, signal/image processing, physics/game engine, ...I'm currently doing some backend work, and I really miss it. I don't think I've done anything interesting algorithm-wise since I switched. Alas, the pay is usually much better in SaaS land.
What's your definition of interesting?
When they say modern, which standard to they mean?
Literally in the title of the linked page:> Modern C++ Programming Course (C++11/14/17/20)
It's described in the course, but canonically this means at least c++11 (and these days, likely 14/17 in practice - there is broad toolchain support)
looks good,is there one pdf for all chapters?
I have cloned the repository then combined the chapters using pdftk:pdftk *.pdf cat output Modern_Cpp_Programming.pdf
This worked on the Mac. I had to install pdftk (brew install pdftk-java) first. The result is nicely formatted for reading on an iPad after importing into Books.
on airplane with a phone so can't do that,wish it had a download link for the full PDF,will create when I am back to my pc
Termux?
It is of type <T>.
I like to watch Formula 1 and NASCAR races. As interesting as seeing the cars pushing the limit of materials and mechanics is just as interesting seeing them crash. It shows the flip side of design.
I wish this sunny-side tutorials had at the end of their books some black pages with the hall of horrors. Weird compilation messages and pointers getting demoted and random segfaults in real scenarios. We all remember the inverted wings in the F1 cars and vehicles literally taking off when the body loose close contact to the ground. Now in NASCAR the roof flaps come off when the car gets slight bump and the front wheels loose traction. C++ Is not a risk averse language.
I’m quite fond of the C++ frequently Questioned Answers:https://yosefk.com/c++fqa/
"Note: some parts of the FQA are not up to date with C++11/14/17. You can contribute to the FQA on GitHub."LOL, yeah I think we can stop spending time on this (which was quite funny back in the day).
There are at least a dozen C++ intros that go through every little detail like this and I don't get it. I have a pretty good ability to retain a lot of information presented this way, and I've been programming in various C-like languages for long enough that much of this isn't new to me, but this doesn't seem like a good way to learn the material. I'd much rather work through actual programs and iterate on them. I imagine that a lot of people who would be looking for something this basic aren't even going to retain most of this nor understand why they would need most of these things they are memorizing.
I agree with your grievance: Many c++ tutorials are basically like: these are the internal workings of how the graphite in a pencil is structured, now go draw the rest of the owl.I am not saying that this isn't important, but learning about pointers and references is totally useless unless someone shows you why you would use them, where you would use them etc.
I think Kate Gregory makes similar good points about the structure of C++ learning materials here:https://www.youtube.com/watch?v=YnWhqhNdYyk
I think C++ and Rust are honestly special beasts.  I've heard experienced programmers say that you want to actually pick up a book to learn Rust, and the same is true for C++.Like you can basically hack your way through learning Python or JS.  I didn't learn Python from a book, for sure.But with C++ and Rust that's not an optimal strategy.  You have to do both -- do practical projects, and actually study it a bit.There's also a huge amount of disagreement about C++, so I think it makes sense to start with a codebase you want to work on, and then ask the people who WROTE that codebase what C++ books and learning materials they like.e.g. Game C++ is kind of different than Google C++, but both are good (they get work done, and multiple people can work together in that style).  There is a lot of really bad C++ out there.  The gulf between good and bad C++ is bigger than good and bad Python or JS.  You want to learn from experienced people who get things done, not language lawyers
I virtually always read a book to learn a new language. My point is that memorizing every language feature as a starting point is not a great way to learn, but almost every C and C++ book/tutorial/course seems to do this. There is a reason why people love K&R's C book.
perhaps zig... the new cool kid on townor pascal.
Rust has a number of opinions that may not fit how you want to work. If you want to do a shared library rust will fight you. If you want to break your project up into multiple repositories rust will fight you. Rust will fight you if you don't want to use cargo as your build/package system.  You can make it work, but the above are things that may or may not be acceptable costs to you.
It depends on what your goals are. If you pick Rust for your product be ready to roll up your sleeves and contribute to the crates you are depending on. IMO your investment is worth it because developing in Rust is a delight and when it works it will be very solid, but the ecosystem isn't as mature yet.If you want to focus on building a product and get something out there relatively quickly, C++ has a huge amount of ready-to-use libraries, but you're on your own with making sure to run sanitizers and making any sense of object lifetime and thread safety. You might ship an MVP faster but I wouldn't want to maintain it after that IMO.Personally I've made peace with the Rust ecosystem being not-quite-there because I can see that many things are about to get to the point of being really solid. There's a lot of hard-working folks volunteering their time to build out great libraries, and I'm hopeful that the 2024 edition is going to be a huge improvement for async ergonomics.
Does it make sense to learn Latin when Chinese, Hindi, English, and whatever else exists? It all depends on what you want to do with your life.
> Does it make sense to still invest time to learn C++ when Rust exists?Yes. The overwhelming majority of industry is using C/C++.Rust is still very much niche when compared to the overall market.Additionally, (Someone can correct me if this is wrong) IIRC, mozilla, was initially one of the biggest proponents of rust, but still just uses rust in a supplementary role. C++ still powers the heavy lifting.
Mozilla is not “one of the biggest proponents of Rust” anymore, in that other, larger entities are now as well. Rust is at the center of many products, large and small.That doesn’t mean that C and C++ aren’t also large, even larger, but this has been changing for years and only shows signs of accelerating, not slowing down.
The size difference is like Sutter Buttes versus Mahalangur Himal, so unless it's accelerating at the speed of light, C++ will be dwarfing rust's usage for many years to come.This is the umpteenth time large tech companies have sponsored C++'s successor. Go was famously created to eventually supplant C++ at Google (or more specifically, solve the issue of long build times). Over a decade later, C++ is very much still in heavy usage at Google.If I were a bettin' man (or if I were trying to increase my general employment odds), and the choices were between Rust or C++, I would pick C++.That being said, if I were in a comfortable financial position, wanted to scratch a nerd itch, and have fun programming again, I would absolutely pick Rust over C++.But those are two very different scenarios.
I certainly do not disagree that C and C++ will be in general usage for a long time, if not forever. However, because the industry is still growing, the proportions change, even as the total number grows in size. C and C++ do not have to die, or even shrink, to become a smaller part of the overall pie, and Rust does not need them to outright die in order to grow and thrive.I agree with you that this is not the first time languages have tried to encroach on the last bastions of the spaces where C and C++ have not yet been ousted as the default choice, but Rust is actually gaining traction in production use-cases in those spaces, unlike many of those languages.
Thank you!
It really depends on your career goals and whether you're doing greenfield or maintenance coding. If you want to be part of the "new generation" of tech, learn Rust. If you want job security, dont care about doing anything glamorous or exciting, and don't mind a long learning journey, go with C++. It will always be around and it's hard enough to learn that you won't have as much competition.
Only things written in Rust I know are rewrites of decades old C applications or rewrites of js tooling. What "new generation" of games, browsers, editors, CAD applications, etc. are written in Rust? Is Rust used in aerospace, automotive, healthcare, instrumentation or finance industries? What is glamorous in Rust that hasn't been done in other languages?
I think rewrites can be as "glamorous" as greenfield, because you get to use a modern language and toolset. yes you have some drudgery tacked on, which is likely having to wade through legacy C code, but having a perfect set of working requirements (do what the old app does!) more than makes up for it.
It is difficult to answer this question because there are so many answers, and they're so broad. I will try to give some partial answers.> gamesSome indie games, a new AAA studio has been working on a game for a while (Embark, their first game is still in C++ but they are building the foundations for the next ones, which takes time), at least one AAA studio known to use it for tooling (Treyarch). The first time I personally gave a talk at a AAA studio about Rust was 2019, though I don't believe that studio is using it for anything I am aware of.> browsersFirefox is about 12% Rust by volume, but is also only 41% C and C++, so it's about a quarter of the relevant systems level code. Chromium has Rust in the tree, but only libraries right now, they don't write new code yet except wrappers. Brave has an adblock component in Rust.> CAD applicationsNot aware of movement here.> aerospace,Very early days, some small projects, nothing massive yet.> automotive,This one is gearing up massively: a rust compiler was just qualified for the relevant safety standards to be used in automotive, several large manufacturers have had job openings open mentioning Rust. It certainly hasn't taken over the world yet but there's a lot of actual movement in this space.> healthcare,Not aware of specific things here.> instrumentationI don't know what the "instrumentation industry" is.> financeSome players are using some things, but as a very proprietary industry, not a lot of specifics are known. I have given an internal talk at a big hedge fund a few years back, unsure if they're doing anything in production just yet.There are other big successes that aren't covered by these industries; Shopify adding a JIT to Ruby, Cloudflare's heavy use of Rust (which means that a significant chunk of internet traffic passes through Rust code), AWS's heavy use of Rust (the core of products like Lambda and S3 are in Rust these days), Apple using it for network services appparently, Meta keeping their monorepo in a version control system written in Rust, and it being an official language to use on projects there, Google using Rust for significant components in Android, Windows having added Rust code already, with more to come, Rust becoming the next language used in the Linux kernel... the stories are endless at this point.
Basically it is being used as any other language out there. Nothing more glamorous than what existing languages are used for.
Agreed, as a Python dev who tried a bit of Rust a while ago, I was wondering exactly the same thing. Bored of the churn and fads with higher level stuff.
Genuinely asking:Why many programmers need tools to enforce something upon them? Why not they can't take slow, be mindful about what they write and add a couple of layers as pre-commit hooks? Like a code formatter and maybe a linter?This makes me sad. Programmers have the knowledge base to make some of the most sophisticated things bend to their will, and they tell a programming language is bad, because they can make mistakes with it. You can cut yourself while chopping onions, too, if you're not careful.When I code C++, I generally select the structures/patterns I gonna use, use them, and refactor my code regularly to polish it. Nobody enforces this on me, but this is how I operate. I add a formatter to catch/fix the parts I botched on the formatting department, and regularly pass my code through valgrind to see whether it makes anything funny on the memory department.
> Why many programmers need tools to enforce something upon them? Why not they can't take slow, be mindful about what they write and add a couple of layers as pre-commit hooks? Like a code formatter and maybe a linter?Why would I want to? Most of that is boring stuff that's hard to get consistently right and highly amenable to automation. Automation is what the computer is good at, it can have that job.Like, why would I want to spend more mental bandwidth on tracking down whether every new goes with every delete than strictly necessary? Yeah, once in a while something is highly performance sensitive and it pays to design it just right for the use case. But it still tends to be surrounded with hundreds of other things that will do just fine with a smart pointer.
> Automation is what the computer is good at, it can have that job.Pre-commit hookisautomation. It formats the code and gives you linting notes the moment you write "git commit".> Like, why would I want to spend more mental bandwidth on tracking down whether every new goes with every delete than strictly necessary?It's not more mental bandwidth for me. Because I write thenew, and directly  delete at the point I need to, then I never think about it again. If I botch something, valgrind will tell me, even pinpointing it with line number.> ...other things that will do just fine with a smart pointer.If that works for you, use it. I have no objections.
> Pre-commit hook is automation. It formats the code and gives you linting notes the moment you write "git commit".So isn't that a tool that enforces something upon the programmer?> If I botch something, valgrind will tell me, even pinpointing it with line number.What is the benefit of needing valgrind? Isn't it even better to have the task automated away so that the problem doesn't even come up?
> So isn't that a tool that enforces something upon the programmer?It's a tool I voluntarily add to my workflow, which works the way I want, when I want, according to project needs. It's not a limitation put upon me by the language/compiler/whatnot.> What is the benefit of needing valgrind? Isn't it even better to have the task automated away so that the problem doesn't even come up?In most cases, performance (think of HPC levels of performance). If I don't need that kind of performance, I can just use the stack or smart pointers.If we're talking about moving to other programming languages, I'd rather not.
> It's a tool I voluntarily add to my workflow, which works the way I want, when I want, according to project needs. It's not a limitation put upon me by the language/compiler/whatnot.I spent a lot of time maintaining a lot of Perl code. While it was generally very well written, it also made me a big fan of strict, demanding compilers. Perl is quick to write, but can have very high debugging costs since it'll let you get away with a lot that it shouldn't like having a function called with arguments but that completely forgets to retrieve them.Based on my experience, IMO any class of error that can be eliminated, should be.So my modern approach is -Wall, -Werrors, -Wextra, and anything else that can be had to that effect, all the time.
> It's not more mental bandwidth for meMost people are not you. Which, I guess, also answers your original question of "Why many programmers [do the things not they way I would do]?". Incidentally, this is also the root of all the complexity in managing and coordinating people.
So use RAII and smart pointers. I think that's the point.
Yes, exactly what I'm saying. Use tooling to do the boring parts of your job.
It’s not tooling, it’s the language.
Languages are not tools?
Every programmer knows the difference the language and the tools surrounding it. This seems like you're trying to create a pointless semantic argument for no reason.
First of all: I am failable. I do make mistakes, even if I concentrate. Secondly I want to verify code others wrote. If a tool does the first pass quickly and automatically, I can quickly ensure some basic level of compliance and can focus on the relevant part.In the end it boils down to: Let the computer do what a computer does and do the things a computer can't.Doesn't say, one shouldn't think, but computers are there and are powerful, so use it. If the checker doesn't find anything: great. If it does: good it's there.
the problem is of course that tools aren't particularly smart. When you create heavy handed restrictions in your language, you're not just eliminating mistakes, you're eliminating tons of potential programs that make perfect sense, that's to say you drastically reduce the expressiveness of a language.That's why Rust say, has an escape hatch. Unsafe Rust wouldn't exist if all you could write in it were mistakes. Async Rust is to put it plainly, a pain in the ass.These high level tools are more like chemotherapy. You hope that they kill more of the bad code before they kill you. They're not sophisticated, and it's fairly reasonable to prefer a language that let's youopt-into stricter safety rather than opt out.
The biggest problem with C++ isn't that you can't write clean, structured code with it.It's that the language is so vast that the odds of any two developers working on two different projects agreeing on what that means are low. I programmed in C++ for a decade, then for a year or two at a second place, then picked it up again in a third... And all three places had nearly completely different best practice protocols. Pre-processor macros are banned, but deep template abstraction and CRTP abounds. Interface and implementation are separated even when templates demand they be in the same header chain, but here we do so by squirreling away the implementation in a separate file with a non-standard suffix instead of splitting it out but keeping it in the bottom of the same file. In my previous company, we used pointers to indicate the called function could mutate the data... In my new firm, pointers in interfaces are just about banned unless absolutely necessary and we indicate whether data could be mutated via const and documentation.The language is broad enough to let the developer change almost anything, but it is unfortunately broad enough to let competent developers build competing and equivalently (un-)safe styles.
Embedded shop? MISRA? Sounds strangely familiar...
Because not everyone has the same values, not everyone has an engineering background even though they like to call themselves engineers, most programming projects are delivered by external companies that don't  care about quality unless it is required by law (aka liabilities).
Look at it this way: every hour, people are coming across C++ for the first time. You can’t expect them to have the same discipline as seasoned programmers. The thing is that even if you teach them “the way”, there’s always a new batch who is clueless. You’re never going to get rid of them, and even the experts are going to make mistakes and take shortcuts.Better to have sane strict defaults with an escape hatch for experts rather than an open range filled with footguns for any newbie to pick up.
Because even the most careful C++ programmer still makes plenty of mistakes. Not the, "oops I cut my finger," kind. Those are usually found by linters and analyzers. More like, "there's a PCI breach and now we're liable for millions of dollars," kind: the semantics of C++ are hard to nail down and ensuring that private data doesn't leak to other threads is almost impossible to get right withonlyknowing C++ andbeing careful.You generally need to use higher-level tools for that kind of work: Coq + the Iris framework, for example, in order to prove that your system can freely share memory and not leak secrets so long as X, Y, and Z hold, etc.Or you need to run a ton of tools like Jepsen to find if your system behaves the way you think it does.What baking more of the specification language into the programming language does (ie: better type systems) is enable us to have more of these guarantees earlier in the development process when changes are cheaper and faster to make (at the expense of requiring more expertise to use them).
> Why many programmers need tools to enforce something upon them? Why not they can't take slow, be mindful about what they write and add a couple of layers as pre-commit hooks? Like a code formatter and maybe a linter?Running a formatter and linter in a pre-commit hook is literally using tools to enforce things?
I have noted elsewhere in the thread, but I think I was unable to express myself very clearly.What I really meant is enforcedexternallyon the programmer, in the form of compiler, development environment setup (from elsewhere) or other guidelines, without any free will to tune or disable them.The layers I add are voluntary, just checks and balances I decided to add myself because I think they help me, and not enforced as part of the language or toolchain I'm forced to use.IOW, a self-respecting developer striving to do a good job can continuously sharpen themselves iteratively, tuning their processes and fixing the problems they see as they go along their journey.Perhaps pjmlp understood the gist of my comment, and his answer is a pretty nice hit on the head of the subject. Honestly, I'm coming from a point where programming is more of a passion which pays rather than work/job for me, hence I have an inner drive to do my best and improve continuously, and not everyone shares the same set of values or inner drive about programming, and want to be railroaded into an environment where they can do the absolute minimum to get things done.
I agree with you. If those tool actually enforced good quality it would be one thing but what is actually being enforced is mediocrity and enforcing the power play of some individuals who have decided for all others what is good.
The fact that we now have languages enforcing safety makes them more desirable than a language that doesn't.If I'm writing a small toy project where I'm the only person messing with the code, I can totally do it in C, or even raw assembly if I feel like it.If anyone else will ever write to the same codebase, or the code is for a client/company, I want every automated static check I can possibly have.
That is all well and good on a small team of senior people , but if your project has more than a handful, with mixed experience levels, you want tools to enforce a minimum standard before even getting to code review
Most things learned don’t provide strict tools to enforce that you don’t use old practices where better modern practices exist. Do you question what the purpose is of learning most things?Also, therearetools to look for old practices and suggest modern ones.
There is no modern C++ without the old C++. C++ is the success that it is exactly because it has been able to evolve with no shameful past to cancel.
We don't talk about auto_ptr.
`union` is all I need to mark my territory and make sure no one touches my property!
This is still the language that supports setjmp and longjmp and just documents that if you mix them with exceptions the behavior is undefined, right?You can't have a shameful past when you started shameful. ;) This language's roots are in "I wrote some extensions to simplify C, but I don't want to make it incompatible with C so the extensions don't work coherently in all contexts and you only get sound code if you hold your mouth right" and it never actually got better because nothing ever got removed to make the language more sound.Well, it did. When it happened, it created other languages.
I've had the misfortune of needing to fix the C++ from someone who neither knew nor cared what STL and smart pointers are.
I'm still trying to clean up the mess from someone who knew what STL and smart pointers were, and then made their own broken smart pointers (their version of shared_ptr has a particularly nasty bug).
You mean someone who learnt Visual C++ from Microsoft and was conned to think he knew C++?
Visual Studio will yell at you if your code isn't conforming to the "C++ Core Guidelines" (those guidelines basically define what "Modern C++" even means).Unfortunately it also yells at you when your *C* code violates the C++ Core Guidelines (at least it was a few years ago when I permamently switched that "feature" off).
Many of the "Core Guidelines" are semantic requirements which are (provably) Undecidable, so even if tooling was created for them the tooling would necessarily have either false positives or false negatives (those are the only options, unless "both" counts as another option). In practice most of these are unaddressed, Microsoft understandably focused on checks which are never wrong and give actionable advice."Guideline support" does include libraries with classes that e.g. provide a slice which raises an exception on bounds miss, which is something, and it's certainly notable that the equivalently named C++ standard library feature is a foot gun† whereas the one in the guideline support library is not, so that's a good reason to adopt this. But VS does not in fact ensure you obey all the guidelines, it's very much a "best effort" approach even with that switched on.† WG21 (The "C++ Standards Committee") seems to have concluded that because sometimes the fast way to do something is unsafe, therefore the unsafe way to do something must be faster... as if car executives noticed that 240mph crashes in their flagship sports car were often fatal and concluded that if they re-design their under-performing SUV so that the fuel tank explodes during any collision killing everybody aboard that'd improve its top speed...
> tooling would necessarily have either false positives or false negatives (those are the only options, unless "both" counts as another option).Getting stuck in an infinite loop and never producing an answer is a 4th possibility for attempted solutions at deciding undecidable problems.
Good point. Note that we can't necessarilytellit's an infinite loop, the constructive proof for this Undecidability reduces it to our old friend the Halting Problem, so we're in the land of the Busy Beaver.
It still does, given Microsoft's stance on security, and that C++ used to be the main systems language, hence why C support languished until pressure from relevant customers to pick it up.I say used to be, given the recent announcement of Azure business unit to switch to Rust as the favoured systems language for new engineering activities.
> C++ Core Guidelines" (those guidelines basically define what "Modern C++" even means)No, it defines what Microsoft C++ means and nothing more.
I compile a lot of C++ code from a lot of places, and the only time I run into code that somehow simplydoesn't workon newer versions of C++ and where the developers aren't even sure if they will accept any patches to fix the issue as they claim it "isn't supported" to use a newer version of C++--even for the public headers of a library--is, you guessed it: code from Google.Meanwhile, most of the C++ code from Google seems to be written in some mishmash of different ideas, always at some halfway point along a migration between something ancient and something passable... but never anything I would ever dare to call "modern", and thereby tends to be riddled with state machines and manual weak pointers that lead to memory corruption.So... I really am not sure I buy the entire premise of this article? Honestly, I am extremely glad that Google is finally leaving the ecosystem, as I generally do not enjoy it when Google engineers try to force their ridiculous use cases down peoples' throats, as they seem to believe they simply know better than everyone else how to develop software.Like... I honestly feel bad for the Rust people, as I do not think the increasing attention they are going to get from Google is going to be at all positive for that ecosystem, any more than I think the massive pressure Google has exerted on the web has been positive or any more than the pressure Google even exerted on Python was positive (not that Python caved to much of it, but the pressure was on and the fact that Python refused to play ball with Google was in no small part what caused Go to exist at all).(FWIW, I do miss Microsoft's being in the space, but they honestly left years ago -- Herb's existence until recent being kind of a token consideration -- as they have been trying to figure out a tactical exit to C++ ever since Visual J++ and, arguably, Visual Basic, having largely managed to pivot to C# and TypeScript for SDKs long ago. That said... Sun kicking Microsoft out of Java might have been really smart, despite the ramifications?)
> code from Google.I spilled my coffee, I was just talking the other day to some coworkers how I don't trust google open source. Sure they open their code but they don't give a damn about contributions or making it easy for you to use the projects. I feel a lot of this sentiment extends to GCP as well.So many google projects are better than your average community one, but they never gain traction outside of google because it is just too damn hard to use them outside of google infra.The only Google project that seems to evade this rule that I know of is Go.
>  but they don't give a damn about contributionsHere is a concrete reason why Google open source sucks when it comes to contributions and I don't think it can be improved unless Google changes things drastically: (1) an external contributor makes a nice change and a PR on GitHub; (2) the change breaks internal use cases and their tests; (3) the team is unwilling to fix the PR or port the internal test (which may be a test several layers down the dependency tree) to open source.> making it easy for you to use the projectsGoogle internally use Blaze, a version of Bazel. It's so ridiculously easy for one team to use another team's project that even just thinking about what the rest of us needs to do to use another project is unloved dreadful work. So people don't make that effort.I do not see either of these two points changing. Sure there are individuals at Google that really care about open source community, but most don't, and so their project is forever a cathedral not a bazaar.
It is not only that, but often when google uses an open source project not owned by them they either try to take ownership of the project or fork it instead of trying to contribute to the original.
That's pretty common though? I mean isn't that part of the idea of open source? Forking is a pretty central part.I don't see a problem here. Why should google have to deal with the opinions of a maintainer if they can just maintain their own version. Yeah obviously it would be nice if they'd contribute their changes back to the upstream repo but from a business perspective it's often not worth it.At my company the inverse of this problem happened way more often: We find a problem but the maintainer just doesn't care. For example the backward-cpp library is a good example where the maintainer just isn't that active in the issues. Why wait for him to respond if you can just fork it and keep on moving.
Which cases did you have in mind? Seems like it should be easy to find half a dozen examples since you claim it happens often.
The GP's complaint was that Google "took over projects" or "forked them without trying to contribute to the original".In the case of KHTML, they never used it in the first place, so it seems like a particularly inappropriate example. I assume you actually meant Webkit? In that case, they spent half a decade and thousands of engineer-years contributing to Webkit, so it doesn't fit the original complaint about not "trying to contribute" either.
November 4, 1998; 26 years ago (KHTML released)
   June 7, 2005; 19 years ago (WebKit sourced)https://chromium.googlesource.com/chromium/src/+/HEAD/third_...* (C) 1999-2003 Lars Knoll (knoll@kde.org)
   * (C) 2002-2003 Dirk Mueller (mueller@kde.org)
   * Copyright (C) 2002, 2006, 2008, 2012 Apple Inc. All rights reserved.
   * Copyright (C) 2006 Samuel Weinig (sam@webkit.org)"...they never used it in the first place"
I think the point is that KHTML was already forked into webkit by apple long before google came along (though, they have in fact also now forked webkit into blink).
> I didn't even need to bring up the DragonEgg cartel (Chandler?) going down the gcc-llvm-clang pathway used essentially for getting rid of the pesky GPL quoted above.That's... not even close to what happened?Historically, LLVM was at one point proposed by Chris Lattner, while he was at Apple, to be upstreamed into GCC (and relicensed to GPL, natch) for use as at the LTO optimization phase, which was declined. For most of its early existence, it used llvm-gcc as the frontend to generate LLVM IR. In the late '00s, serious effort was put into making a new frontend for LLVM IR which we know as clang, primarily by Apple at that point, which become self-hosting in 2009 or 2010. Basically the moment clang becomes self-hosting, everyone jumps ship from using llvm-gcc to using clang to make LLVM IR.Google shows up around this time, I think primarily motivated by the possibility that Clang offered for mass rewriting capabilities, since it has extraordinarily good location tracking (compared to the other compilers available), which is necessary for good rewriting tools. The other major area of Google's focus at this time is actually MSVC compatibility, and I distinctly remember Chandler talking in one of his presentations that you need to be able to compile code to trust it well enough to rewrite your code, so I think the compatibility story here was mostly (again) for rewriting.Also around this time, gcc gains proper plugin support, and llvm-gcc is reworked into dragonegg to take advantage of the proper plugin support. But because clang now exists, dragonegg is no longer very interesting, with almost all the residual attempts to use dragonegg essentially being limited to people trying to use it to get LLVM IR out of gfortran, as LLVM had no fully-working Fortran compiler at that point.
Again, that seems to be in no way demonstrating the pattern that was claimed to be happening often.AFAIK Google did not take ownership of gcc, nor did they try to fork it without contributing to the original. They used GCC for a good couple of decades while contributing to it, but eventually switched to a different compiler. The same for clang, they neither "took it over" nor "forked it without trying to contribute".
https://web.archive.org/web/20241123183550/https://en.wikipe...https://web.archive.org/web/20241125065641/https://arstechni...Arsis controversial on YC news.Who knew?
One could ask whether Google works ‘open source’ or more ‘source available’; the source is there but you cannot contribute, if you can build it at all
No, "open source" doesn't imply open contribution. The standard terminology is cathedral vs bazaar.
Just to add a different perspective: sometimes people mean Open Source[1] when they say "open source," and sometimes they don't.Personally, I take the cathedral/bazaar distinction to indicate different development cadences and philosophies, rather than whether contributions are allowed/encouraged.Various cathedral-style projects (eg: FreeBSD, Emacs) still actively take contributions and encourage involvement.There's something even further along the spectrum that's "we provide dumps of source code, but don't really want your patches." I'm not sure what the best term is for that, but "source [merely] available" sometimes has that connotation.[1]https://opensource.org/osd
The quintessential example for providing source and discouraging contributions is SQLite. Nobody would argue that it's merely source available. It is full open source.In fact "source available" usually means you can see the source code, but there are severe restrictions on the source, such as no permission to modify the source even for your own use, or no permission to create forks of the project containing the modifications, or severe restrictions on such modifications. An example is MongoDB's Server Side Public License, which is source-available but not open source.
I think it depends on the contribution. I sent a bug report with a minimal test case. It was welcomed and quickly fixed. It is not a source code contribution, but I think it is a contribution.
OP is specifically talking about code contributions. You can (I have) make that type of contribution to proprietary software.
> sometimes people mean Open Source[1] when they say "open source," and sometimes they don't.And when they don't when talking about source code, they are wrong. If someone says that an RJ45 cable is "a piece of software" because it's "soft" (you can bend it), would you say it's just a different perspective?Open source, in the context of software, has a particular meaning. And it is the case that many software developers don't know it, so it's worth teaching them.
While I, too, believe that words should mean things, I don't think it's quite so cut-and-dry in this particular case. Part of the reason the term could not be trademarked was because it is too descriptive; it's easy for people to put those words together to describe software.I agree that the OSI meaning is worth teaching. But perhaps not by saying "you're wrong; there is only one right way." Perhaps more like "some people attach XYZ specific meaning to that phrase, please be aware of it. Also, here is some history of the term if you like."----Aside:On re-reading this, I wonder if it comes across as testy... I think I am just channeling my annoyance with the language police of the world, in general, who sour people's interest in topics with their gatekeeping behavior. I don't mean it too personally towards you (:
To take a step back, it came from this comment:> One could ask whether Google works ‘open source’ or more ‘source available’; the source is there but you cannot contribute, if you can build it at allThe author of this comment says "if you can't contribute, shouldn't you consider it `source available` instead of `open source`?".There is only one valid answer: "No, you should not. It is still open source even if you cannot contribute". The context is clear, we are talking about "open source" vs "source available", which are both very specific in this context.> I think I am just channeling my annoyance with the language police of the world, in general, who sour people's interest in topics with their gatekeeping behavior. I don't mean it too personally towards you (:No offense taken, and I don't mean it personally either =). My point is just that in this context, the author of the comment was pretty clearly talking (asking, even?) about the difference between "open source" and "source available".I don't even think it's shutting down the author: there was no other point than this, so the "thread" started by this author was purely about the meaning of those words.
Maybe you already know this and have discarded it (if so, no worries), but for what it's worth, this is my perspective on these things: Some people, in some contexts, use words like a laser — very specific, very targeted, with precise meanings, etc. Other people, other times (perhaps most people, most of the time?) use words more like ... a bucket of paint. Words are sloshy and approximate and about as precise as trying to sign your name using that bucket. Each has their value.Inevitably, a laser-minded person talks with a sloshy-bucket person and misunderstandings ensue.In sloshy-bucket land, I think "open source" has various connotations — a sense of community, encouraged contribution, being able to build it yourself, improve it yourself, etc.And I think the commenter, in broad strokes, was saying that Google is not upholding those various virtues that are often associated with "open source," so felt the term was not a good (sloshy) fit.In particular, I donotthink they were asking the question you say they were asking.In this space, it seems like there are both too many terms (so people rather just pick a popular one and over-apply it) and too few (so you can never find one thatquitesays what you want). Such is life, I guess. Maybe "open sourcey" would be good, to indicate it's talking about a hand-wavy vague "ness" rather than a particular nailed-down definition. "Google isn't being very open sourcey"? ¯\_(ツ)_/¯Anyway, all this to say: in the ethos of trying to take a charitable interpretation of people's words, I think it's good to consider the bucket-of-paint possibility, before jumping to corrections and yes/no determinations.----Edit: It occurs to me that originally I misinterpreted you as being persnickety, when perhaps you were just trying to answer the question you felt they had asked. Sorry!
Note that I did not write the original answer: I answered to you :-).> And I think the commenter, in broad strokes, was saying that Google is not upholding those various virtues that are often associated with "open source," so felt the term was not a good (sloshy) fit.Totally valid! And I like the idea of considering the "bucket-of-paint" possibility before saying "no you're wrong". But on the other hand, sometimes it's worth agreeing on the meaning of words while discussing something.I feel like I actually happen to regularly be on the bucket-of-paint side. I will often simplify the part of the discussion that I feel is not relevant by saying e.g. "okay this solution is bad, so if we look into this other solution we have to think about ...". And sometimes people really care about starting a discussion saying "by saying it's bad, you make it sound like whoever would think about it is stupid, and that's extreme. This solution is not necessarily bad, because in some situations it may work even though it is suboptimal". To which I tend to say "sure, I said it was bad as a way of saying that we seemed to agree that we would focus on the other one".Until this point it's perfectly fine for me. What frustrates me is when the discussion continues in what I feel sounds like, e.g. "no, I think that your saying it is bad reflects that you disrespect whoever would think about it, and you should never have used that word in the first place. I am not sure I can ever have a meaningful discussion with you now that you used this word in this sentence, even if you later admitted that it was an oversimplification".Anyway, communication is hard :-)
Google wants to be technically persuasive -- a player in an engineering space (ergo open source) -- without being beset with the roles and responsibilities of a 3rd party being a bonafide customer when push comes to shove.
Googletest is the most widely used test library for C++. Googlemock is the only mocking library available that's reasonably feature complete.
I you are using googletest, you owe it to yourself to check out catch2 which I find much better and uses modern C++.  There are a few other test frameworks in C++ that look better than google test as well,  but catch2 is the one I settled on (and seems to be the best supported): feel free to check them out.I've given up on mock frameworks.  They make it too easy to make an interface for everything and then test that you are calling functions with the expected parameters instead of the program works as you want.  A slight change to how I call some function results in 1000 failed tests and yet I'm confident that I didn't break anything the user could notice (sometimes I'm wrong in this confidence - but none of the failing tests give me any clue that I'm wrong!)
catch2 has become fairly bloated. doctest takes all of the best parts of catch2 without all the bloat and the end result is a test framework that is literally over 10x faster than catch2. It's also like 90% compatible with catch2 so porting your tests to it is pretty easy.Especially if you have a build process that always runs your unit tests, it's nice to have a very fast test/compile/debug loop.https://github.com/doctest/doctest
>catch2 has become fairly bloated. doctest takes all of the best parts of catch2 without all the bloat and the end result is a test framework that is literally over 10x faster than catch2. It's also like 90% compatible with catch2 so porting your tests to it is pretty easy.I feel like you could make a madlib where you could plug in any two project names and this sentence would make sense.
Madlibs have become fairly bloated. Copypasta memes take all the best parts of madlibs without all the bloat and the end result is a form of mockery is literally over 10x faster than a madlib. It's also like 90% compatible with madlibs so porting your gibes is pretty easy.
I was just about to suggest doctest, you beat me to it! I'm all about faster compile times, and it was mostly a drop-in replacement for catch2 in my case.Also, IMO, both doctest and catch2 are far superior to Google Test.
I've found exactly three places where I really want to have a mock available:1) Databases and other persistent storage.  Though in this case, the best mock for a database is generally another (smaller, easily snapshottable) database, not something like googlemock.2) Network and other places where the hardware really matters.  Sometimes, Ireallywant to drop a particular message, to exercise some property of the sender.  This is often possible to code around in greenfield projects, but in existing code it can be much simpler to just mock the network out.3) Cases where I am calling out to some external black-box.  Sometimes it's impractical to replicate the entire black-box in my test.  This could be e.g. because it is a piece of specialized hardware, or it's non-deterministic in a way that I'd prefer my test not to be.  I don't want toactuallycall out to an external black-box (hygiene), so some kind of a mock is more or less necessary.
For 1 have you looked at test containers?
Briefly, but frankly:  copying small SQLite files around works so well in almost all cases that I don't feel the need for a new abstraction.
Sounds like the mocks are overused or used inappropriately in your experience (whether by a colleague or yourself).Mocks have their place. A prototypical example is at user-visible endpoints (eg: a mock client).
I have found in my world it is easy to setup a test database (we use sqlite!) and the file system is fast enough (I have code to force using a different directory for files).  I have been playing with starting a dbus server on a different port in my tests and then starting the real server to test against (with mixed results - I need a better way to know when dbus is running).  I have had great success by writing a fake for one service that is painful - the fake tracks the information I really care about and so lets me query on things that matter not what the function signature was.I'm not arguing that mocks don't have their place. However I have found that by declaring I won't use them at all I overall come up with better solutions and thus better tests.
Exactly! This one gets it, real communism has never been tried! On another note I do not think that it is tiresome at all, that any critique of any pattern/teqnique in SWE, always is meet with the "you are holding it wrong" rebutle.
Do you not believe it's possible to hold something wrong? If someone is a skilled and experienced golfer, it's quite believable that they won't automatically be a skilled tennis player after three months of tennis playing. If someone is an experienced race car driver, they won't automatically be a skilled member of a basketball team. "You must be holding it wrong" can sometimes takeyearsof practising holding it right, not just minutes or months.If a team of people who have been SWEs for decades reports that something helped their team, and you try it and it doesn't work, and you have been SWEs for decades, that doesn't automatically mean they are charlatans selling nonsense. They might all be basketball players playing together for 5 years and you might be a team of a baseball player, a racecar driver, a track and field athlete, and a water polo player, trying to play basketball from only reading about it, with nobody who has done it or experienced it, and several people who quietly don't want to be playing it and are just nodding along while hoping it fails. The conclusion that they are liars and it can't possibly work is not a strong conclusion.
When I look close I discover that those people who tried agile and found it worked either were on a much smaller projects with much simpler problems than large projects have; or they are not telling the full truth about agile. (sometimes both).  I'm glad agile works for small projects, but it doesn't scale very well seems clear from all the large projects that have tried it and have gone back in major ways (generally not all the way back).  The people who have failed projects still often sing the praises of agile, but we have no idea if the project would have failed if something else had been used.
I used to really like Google Test, and then Google decided in it's infinite wisdom to make the OSS version depend on their C++ shared library replacement Abseil, and not just that but the live at head version.That makes sense internally for Google because they have their massive monorepo, but it sure as hell makes it a pain in the ass to adopt for everyone else.
I don't think you're reading those docs correctly. Googletest recommends living at head, but there's no reason you can't pin a release, either a git commit hash or a release label, of which there have been several. Googletest does not depend on the HEAD of abseil-cpp, it actually declares a direct dependency on an older LTS release of absl, but since you are building it from source any later release or commit of absl would work.Google open source libraries are often a mess when you try to include more than one of them in the same project, but googletest isn't an example of the mess. It's actually pretty straightforward.
> Google open source libraries are often a mess when you try to include more than one of them in the same projectCompletely agree. In isolation all of their libs are great, but inevitably I end up having to build Abseil from source, to then build Protobuf off of that, to then build gRPC off of that. If I can include the sanitizers under Google then that also becomes painful because Abseil (at least) will have ABI issues if it isn't built appropriately.
Thinking about it I'd really just like a flat_hash_map replacement so I can drop Abseil.
Protobuf depending on Abseil (which has ongoing macOS build issues) is clinically insane. I tend to use protozero now which trades half a day’s boilerplate for two days’ build heartache.https://github.com/mapbox/protozero
Wouldn't it be even more insane if protobuf had its own distinct string splitting/merging routines, its own flags and logging libraries, etc?
No. Not at all. String splitting is a couple of lines' code. I don't want have to think about a logging framework just to read a protobuf - it can send stuff to stderr like everything else. If Google wants protobuf to be a widely accepted standard then it shouldn't require you to opt into their ecosystem to use it.
> Thinking about it I'd really just like a flat_hash_map replacement so I can drop Abseil.boost has a flat_hash_map implementation for quite a few versions now, which from what I could see generally beat or is competitive with the absl implementation:https://www.reddit.com/r/cpp/comments/yikfi4/boost_181_will_...
The reddit thread mentions that the author was probably going to write a blog post about it at some point; I went and found it so you don't have to.I was curious what exactly differentiates boost::unordered_flat_map from absl::flat_hash_map, and was not disappointed. It seems that the lion's share of the performance improvement comes from using more of the metadata for the reduced hash value, although there are a few other contributing factors.The blog post further describes where absl::flat_hash_map performs better: iteration (and consequently erasure), which is ironic given those are a couple of areas where I always felt that absl::flat_hash_map was especially weak. But, it makes sense to double down on Abseil's strengths as well as its shortcomings.https://bannalia.blogspot.com/2022/11/inside-boostunorderedf...
Iteration has been improved since, and now we’re beating Abseil on iteration plus erasure:https://github.com/boostorg/boost_unordered_benchmarks/tree/...
Very cool!I especially like how you can see the load factor across the graphs, where there are sharp downward spikes each time the map resizes, and how they vary as you move through the memory hierarchy.I am curious what Abseil could learn from other modern hash map implementations, since my understanding is that the fundamental structure of its swisstables implementation hasn't changed meaningfully since 2017.
FWIW the flat hash map in Boost is now faster. I am not sure if integrating Boost is any easier for you.
I occasionally reconsider it so I can try a bunch of the FB alternatives (Folly, Thrift, CacheLib, etc.), but... yeah. Still just kind of waiting for a panacea.
> Googletest ... declares a direct dependency on an older LTS release of abslLooking at the build configuration code:https://github.com/google/googletest/blob/main/CMakeLists.tx...it seems like the dependence on Abseil is optional. i.e. you can use googltest on its own. I wouldn't recommend it (I kinda like doctest), but still.
It's been a few years to be fair, I stopped working with C++ in early 2021 or so so maybe I've just misremembered. I do remember having to take Abseil on where we previously didn't.
Google test and mock are quite powerful but are a big hit at both compile time and runtime, which matters for quick edit-compile-fix loops.I still go back and forth on whether google test and mock are worth it.Google benchmark is also nice.
> big hit at both compile time and runtime, which matters for quick edit-compile-fix loopshonestly if you write C++ for work, there's no excuse for your company to not give you the beefiest dev machine that money can reasonably buy. given that rust exists, I think "get a faster computer" is a totally valid answer to build times, especially now that skylake malaise era is over and CPUs are getting faster
> given that rust exists, I think "get a faster computer" is a totally valid answer to build timesI find this amusing because one of the main reasons i avoid Rust (in the sense that i prefer to build things written in other languages if possible - i don't mind if someone else uses it and gives me a binary/library i can use - and it never went beyond "i might check this at some point, sometime, maybe" in my mind) is the build times compared to most other compilers :-P.Also, at least personally, if i get a faster computer i want my workflow to be faster.
> 640k should be enough for everyone
Does it not support only running some or no tests? I only run the full test suite rarely, close to releases.
I blame monorepo culture.  If it doesn't grow up in a context where it's expected to stand on its own, it crashes and burns when you kick it out of the nest.
I heard that Meta also has a monorepo but most of their open source projects are very community driven. I think it is corporate mandate thing, no resources to be spent on open source and not tracking open source contributions as part of career development.
Meta does have a monorepo but their open source stuff lives outside it. Or at least it did when I worked on PyTorch (2019). I did all my work in the separate open-source PyTorch repo and then commits got mirrored back to the monorepo by some automated process.You could also build and run it using completely standard tools; you didn’t need to download random internal source control software etc. like you do for e.g. Chromium.
Curious about the organizational dynamics around this kind of decisions. There is no reason why google couldn't do the same.I assume there is little will internally because everyone there is so focused on their performance reviews and helping external people using google open source projects is not tracked by that.
I think it's more of a strategic difference. Google seems like their long term planning involves thinking about open source less than Meta's. They're more wait-and-see about it.React must've been destined to be open source from the get go: gotta create a mountain of js to hide in so the users can't strip out the malicious parts. Kubernetes on the other hand could've been internal forever and still would've made sense. It just happened to later make sense to open source it (it feels lopsided to me, like they kept certain parts secret. It wouldn't feel that way if they had planned it as OSS from the get go).
Tensorflow is/was decent. It looked like they made a lot of effort for it to be accessible for outsiders.
Have you tried building the damn thing ?Nix build is still stuck in the one from 3-4 y back because bazel doesn't play well. Debian too has some issues building the thing...
As an industry we need to stop treating breaking changes as an acceptable thing. The rate of bit rot has accelerated to an absurd pace. I can't remember the package but I had to spend considerable time fixing a build because a package.. changed names.. for NO REASON. They just liked the new name better. This should be career death. You're wasting your fellow humans' time and energy on your vanity when you make a breaking change that is at all avoidable. I should be able to run a build script made 20 years ago and it should just work. No renamed package hunting, no WARNING WARNING DEPRECATED REWRITE ALL YOUR CODE FOR LEFTPAD 10.3 IMMEDIATELY in the console, no code changes, no fuss, we should expect it to just work. This state of affairs is a stain on our industry.
One day we will have bled enough and we'll switch to using cryptographic hashes of package contents (or of some recipe for deterministically building the thing on different architectures) instead of anything so flimsy as a name and version number.For the humans, we can render the hashes as something friendly, but there's no reason to confuse the machines with our human notions of friendliness.
You’re basically describing nix and Guix.
They use a hash of the derivation and its inputs as a memoization strategy: providing yesterday's answer to today's question since it was asked yesterday. But so far as I know nobody's actually using those hashes for the initial request.It's not like python will let you:import nix.numpy-hsbdjd...8r5z2 as npSuch that the import mechanism ensures that the correct build of numpy is used.For that to work you'd have to change nix such that the hash did not digest parameters like `amd64-linux` witch indicated the system architecture (you'd want those to be satisfied at import time).
In Guix at least (I assume also nix) you can build things from source with a verified hash. I.e. write a numpy package definition that says download the Numpy source from this URL, and expect its hash to be equal to this string. You could then depend on that package from another package ensuring it uses a numpy built from that bit-for-bit exact source tree. Does that not amount to the same thing as what you want?
this is why you build to a specific version of a library. drop your build script into a container with the versions of software it expects and it should do fine. containerization is the admittance that versioning environments is needed for most software. I expect the nix/guix crowds to win in the end.
Blindly wrapping a build script in a Dockerfile is not nothing, but it's no replacement for being careful while writing that script in the first place.Otherwise I agree, because if you must be careful, you might as well use tooling that's built for such care.  But if you're doing that, do you need the Dockerfile?  And that's how you end up with nix/guix.
Having tried on other platforms, it's not Bazwl, it's not evenjustGoogle.It's python packaging and the way the only really supported binary distribution method of Tensorflow for many many years was to use Pip and hope it doesn't crash. And it's reflected in how the TF build scripts only support building python lib as artefact, everything else at the very least involved dissecting bazel intermediate targets
The issue with Microsoft until recently, has been the power of WinDev, which are the ones responsible for anything C++ on Microsoft dungeons.Hence the failure of Longhorn, or any attempt coming out from Microsoft Research.Ironically, given your Sun remark, Microsoft is back into the Java game, having their own distribution of OpenJDK, and Java is usually the only ecosystem that has day one parity with anything Azure puts out as .NET SDK.
What is "WinDev"? A quick search didn't turn up much except a French Wikipedia article.
Windows Development, per opposition to DevDiv, Developer Division.Two quite common names in the Microsoft ecosystem.
As a former MS employee some time ago I don't think I ever heard "windev". It was always referred to as "Windows". Though there were a lot of different groups within that, so sometimes you'd hear an initialism for a specific team. For example during some of my time there was a big organizational split between "core" and more UI oriented teams.
Here is an example in the press, with an email from Somasegar, leader of developer division in the past.https://www.zdnet.com/article/microsoft-splits-up-its-xaml-t...
I was an employee in Windows on the date of that email. I left a few months later. Note that the email itself doesn't say "windev". It says "Windows" a bunch of times.If I'm stretching this "windev" thing, the domain for a lot of employee accounts (including mine) was NTDEV, that had a longer history afaik, nobody called an org that..
I think it was sort of externally derived based on "DevDiv", but as another former MS employee - albeit from DevDiv - I can confirm that "WinDev" is not something that was routinely used inside the company the way "DevDiv" is. Usually it's just "Windows", or "Windows org" if the context is ambiguous.
For a moment there I thought you were referring to this trademark:https://pcsoft.fr/windev/index.htmlWhich was known at a time for having young women in light clothing in their marketing material.
aha, that's the windev that comes to mind too. I didn't know they were actually a french company, wild that they're still around... their ads were plastered everywere in the 2000s.Apparently they have a programming language for which you can "one-click-switch" between english and french for the keywords???https://pcsoft.fr/windev/ebook/56/
That's actually kind of neat, also I love how the brochure uses the American flag for English...
Yes. I would have preferred that they had used Canadian flags for both.
I use the Microsoft JDK daily - to develop in Maui for Android. Other than that, I'm not too sure what anyone would use it for over the actual OpenJDK versions. I'm pretty sure the MS OpenJDK is mostly there to support pushing people to Azure (hence your observation) and Android. I don't think it is there for much else outside of that, but I'm happy to stand corrected if anyone has another use cas for it.
It was thanks to Microsoft that you get to enjoy the JVM on ARM for example, or better escape analysis.https://github.com/microsoft/openjdk-aarch64https://www.infoq.com/news/2023/02/microsoft-openjdk-feature...
Sure, but the first link is surely only benefiting those using Windows on ARM? I do have Windows on ARM on a MacBook under VMWare, but my daily usage of Windows is under x64. Second link - not really knowing much about Java I don't know enough to comment. 99% of my Java use is indirect because it only gets touched by MSBuild when compiling my APK from C#.
The C++ from Google that people in the outside world are seeing is not the C++ the article is talking about. Chromium and open sourced libraries from Google are not the same as C++ in Google3. I worked on both back in the day and ... There's slightly different style guides (not hugely different), but most importantly the tooling is not the same.The kind of mass refactorings / cleanups / static analysis talked about in this article are done on a much more serious and large scale on C++ inside the Google3 monorepo than they are in Chromium. Different build systems, different code review tools, different development culture.
Going from g3 to AOSP has been downright painful. It was like suddenly working in a different company the contrast was so stark.
Interesting. I never worked in Android, but did in Chromium & Chromecast code bases. Biggest difference with Google3 was honestly in the tooling. Style guide was fairly close, maybe a bit more conservative. Also the lack of the core libs that eventually became Abseil.I work full-time in Rust these days and everytime I go back to working in C++ it's a bit of a cringe. If I look long enough, I almost always find a use-after-free, even from extremely competent developers. Footgun language.
Whatever gave you the idea Microsoft "left" C++ years ago? It has massive code bases in C++ and continues to invest in its compiler teams and actively tracks the C++ standard. It was the first compiler to implement C++20 mostly completely, including modules, which other compilers have yet to catch up to. Like other mature companies, Microsoft realized decades ago that they can be a one-tech-dependent company and hence has code in C++ and .NET, and is now exploring Rust.
Cppwinrt is in maintenance mode[1]. Cppwin32 is abandoned (with windows.h as the official alternative). It is now possible to deploy WinUI 3 apps as single files in C#[2] but not in C++. From experience, the entire C++ side of WinUI 3 documentation is underbaked to the extent that the easiest approach is to read the C# documentation and attempt to guess the cppwinrt equivalent (as docs for cppwinrt are not really... there).I don’t know if they’ve really abandoned C++ entirely—the compiler team certainly hasn’t, that’s true. But the above doesn’t feel like first-class support.[1]https://github.com/microsoft/cppwinrt/issues/1289#issuecomme...[2]https://learn.microsoft.com/en-us/dotnet/core/deploying/sing...
WinUI3 itself feels kind of abandoned. Heck, everything except desktop OS (which changes we neither need nor want) and cloud (where everyone has gone) feels a bit neglected.C#/dotnet continues nicely, but the team is surprisingly small if you look closely.
Microsoft doesn't commit to UI frameworks in any language. By contrast, DirectX 11 and 12 (and Direct2D) are C++-native and have become core modules within NT. I don't think MS has abandoned C++, but the use case for C++ has shrunk considerably since the 1990s
If you go into Visual C++ developer blog, you will notice it has been all about Unreal support during the last year, and not much else.
Besides the sibling comments, officially Windows is going to go under some rewrites under the Secure Future Initiative, and the MSVC team has been reduced in resources, to the point now they are asking what features of C++23 people want to have.https://developercommunity.visualstudio.com/t/Implement-C23-...I suppose usually one would like to have everything from a language standard.The C++20 winning run seems to have been one of a kind, and whatever made it possible is now gone.Speaking of gone, Herb Sutter has left Microsoft and most certainly had to do something with whatever is going on MSVC, C# improvements for low level coding, and Rust adoption.
Being smart, well-educated, and knowing how to program isn't good enough for creating great code. It takes experience. I've been programming for 50 years now, and keep finding ways to make code more readable and more maintainable.
How do you find gimmicks from Bob Martin like (d + e*g) which in theory are great but to use it in practice would take loads of coaching?
I'm not familiar with that gimmick.One thing I learned, for example, is do not access global immutable state from within a function. All inputs come through the parameters, all outputs through the parameters or the return value.
Global immutable or globalmutable. I vehemently agree with the latter, but while I could definitely make a case for the former [1], I think it is a bit too extreme especially without language support.Would you access a global M_PI constant? Or another function name? Or would you require every dependency to passed through?[1] i.e. a total capability based system.
Global mutable state is to be avoided at all costs of course, but IMO global immutable state is to be avoided... at some costs.The main issue comes in when you change (in the code! not as mutation!) the global immutable state and now you have to track down a bunch of usages. If it wasn't global, you could change it only in some local areas and not others.You aren't likely to change M_PI to a new value (int 3 for performance?) so for pure constants, fine, global immutable state works. However many usages of global state are things like singletons, loggers and string messages that often eventually benefit from being passed in (i18n, testability etc.)As to ergonomics, you can stuff all that global state into a single instance and have one more parameter that is passed around. It will still allow calls to eg change logging on their downstream functions much more easily than having singleton configuration.
As someone without a lot of experience (in my first dev job now), would you care to expand on this? Does this mean that you wouldn’t have a function fn() that manipulates a global variable VAR, but rather you’d pass VAR like fn(VAR)?
To expand on the other reply, some related things:1. don't do console I/O in leaf functions. Instead, pass a parameter that's a "sink" for output, and let the caller decide what do with it. This helps a lot when converting a command line program to a gui program. It also makes it practical to unit test the function2. don't allocate storage in a leaf function if the result is to be returned. Try to have storage allocated and free'd in the same function. It's a lot easier to keep track of it that way. Another use of sinks, output ranges, etc.3. separate functions that do a read-only gathering of data, from functions that mutate the dataGive these a try. I bet you'll like the results!
I heartily agree with #2 if the language isn't Zig.  Which actually supports your point: allocating in leaf functions is idiomatic in Zig, and it works out fine, because there's no allocation without an Allocator, and even if that's passed in implicitly as part of a struct argument, error{OutOfMemory} will be part of the function signature.  So there's no losing track of what allocates and what doesn't.This actually supports your broader point about always passing state to functions, and never accessing it implicitly.  Although I don't know that I agree with extending that to constants, but maybe with another several decades of experience under my belt I might come to.Zig also makes it easy for 'constants' to change based on build-specific parameters, so a different value for testing, or providing an override value in the build script.  I've found that to eliminate any problems I've had in the past with global constants.  Sometimes, of course, it turns out you want those values to be runtime configurable, but as refactorings go that's a relatively straightforward one.
> So there's no losing track of what allocates and what doesn't.Having an allocator implicitly passed in with a struct argument is not quite what I meant. D once had allocators as member functions, but that wound up being deprecated because the allocation strategy is only rarely tied to the struct.
There are some meaningful differences between Zig and D in this specific area, specifically, D uses exceptions and has garbage collection as the default memory strategy.  That will surely result in different approaches to the leaf-allocation question being better for the one than for the other.
You've got the gist of it. By decoupling your function from the state of your application, you can test that function in isolation.For instance, you might be tempted to write a function that opens an HTTP connection, performs an API call, parses the result, and returns it. But you'll have a really hard time testing that function. If you decompose it into several tiny functions (one that opens a connection, one that accepts an open connection and performs the call, and one that parses the result), you'll have a much easier time testing it.(This clicked for me when I wrote code as I've described, wrote tests for it, and later found several bugs. I realized my tests did nothing and failed to catch my bugs, because the code I'd written was impossible to test. In general, side effects and global state are the enemies of testability.)You end up with functions that take a lot of arguments (10+), which can feel wrong at first, but it's worth it, and IDEs help enormously.This pattern is called dependency injection.https://en.wikipedia.org/wiki/Dependency_injectionSee also, the "functional core, imperative shell" pattern.https://www.youtube.com/watch?v=yTkzNHF6rMs
Yes. Global variables or singletons are deeply miserable when it comes to testing, because you have to explicitly reset them between tests and they cause problems if you multithread your tests.A global variable is a hidden extra parameter to every function that uses it. It's much easier if the set of things you have to care about is just those in the declared parameters, not the hidden globals.
Cool I am just confirming my own bias against much of „clean code” teachings. That it might be a bit easier to read order of the operations - but no one uses it so it doesn’t matter.
There are lots of things that look like great methods, but experience with them often leads to disillusionment. For another example, Hungarian notation is a really great idea, heavily adopted by Microsoft Windows, and just does not deliver on its promises.For example, types can have long names, but that doesn't work with HN. Changing a declaration to have a different type then means you've got endless cascading identifiers that need to be redone. And so on.
> Changing a declaration to have a different type then means you've got endless cascading identifiers that need to be redone.This is actually a good thing, every mention of that identifier is a place that you might need to adapt for the new type.  Hungarian notation is an excellent coping mechanism when you have to use compilers that don't do their own type checking - which used to be a huge issue when Hungarian notation was current.
On balance, it isn't a good thing. Having high refactoring costs means:1. you become reluctant to do it2. lots of diffs cluttering up your git history. I like my git history to be fairly narrowly targeted.I don't use languages that don't do type checking. Microsoft uses Hungarian notation on their C interface and example code.
Could someone explain what this is since that expression is unsearchable?
So (d + e*g) is an example where if you do mathematical operations you put spaces between ones that will be lower rank and higher rank no spaces. This way you could a bit faster grasp which operation will be first so (2 + 3*4) you know first to evaluate 3*4 will be 12 and then you add 2 giving 14 - but given variable names of course you are quicker to evaluate result.But no one has time to craft such details in the code.
I only have 20 years of development experience, so I'll defer to Walter here, but if I were to write that equation it would look like `d + (e * g)`. I don't trust mine or anyone's understanding of operator precedence. Just look at how ridiculously hard to read their implementations in parsers are.Specifically d+e*g I might make an exception for in a code review (and allow it), since it's such a widely known precedence in mathematics you can expect the reader and writer to know the way it goes, but any more complex and I'd reject it in the review for lack of parentheses.
Operator precedence is so deeply burned into my brain I would never think of adding parens for it or modify the spacing.I will use parens, however, for << and a couple other cases. It would be a shame to use lack of spacing to imply precedence, and yet get it wrong. Oops!I also like to line up things to make vertical formatting of similar expressions, something a formatting program doesn't do. Hence I don't use formatters.
Parens were not the main part - main part is having multiplication without spaces and addition with spaces.I would say it is a neat detail but if no one cares or uses it - it is pretty much "feel good about yourself" use and not practical one.
.. that seems like a strange optimization when there's a tool to indicate to both reader and compiler which operations will be performed first: brackets!
I second the observation of the state of Google C++. Just look at Chromium. There are a lot of unfinished refactoring there, as if people lost interest the moment the clean refactoring hit a roadblock requiring efforts to communicate with other teams. Only by a sort of direct order from the management things can be completed.
> Honestly, I am extremely glad that Google is finally leaving the ecosystem, as I generally do not enjoy it when Google engineers try to force their ridiculous use cases down peoples' throats, as they seem to believe they simply know better than everyone else how to develop software.Well, you may be celebrating a bit prematurely then. Google still has a ton of C++ and they haven't stopped writing it. It's going to take ~forever until Google has left the C++ ecosystem. Whatdidhappen was that Google majorly scaled down their efforts in the committee.When it comes to the current schism on how to improve the safety of C++ there are largely two factions:* The Bjarne/Herb [1] side that focuses on minimal changes to the code. The idea here is to add different profiles to the language and then [draw the rest of the fucking owl]. The big issue here is that it's entirely unclear on how they will achieve temporal and spatial memory safety.* The other side is represented by Sean Baxter and his work on Safe C++. This is basically a whole-sale adoption of Rust's semantics. The big issue here is that it's effectively introducing a new language that isn'tC++.Google decided to pursue Carbon and isn't a major playing in either of the above efforts. Last time I checked, that language is not not meant to be memory safe.[1]https://github.com/BjarneStroustrup/profiles[2]https://safecpp.org/draft.html
(Carbon lang dev here.)Carbon is intended to be memory safe! (Not sure whether you intended to write a double negative there.) There are a few reasons that might not be clear:* Carbon has relatively few people working on it. We currently are prioritizing work on the compiler at the moment, and don't yet have the bandwidth to also work on the safety design.* As part of our migration-from-C++ story, where we expect code to transition C++ -> unsafe Carbon -> safe Carbon, we plan on supporting unsafe Carbon code with reasonable ergonomics.* Carbon's original focus was on evolvability, and didn't focus on safety specifically. Since then it has become clear that memory safety is a requirement for Carbon's success, and will be our first test of those evolvability goals. Talks likehttps://www.youtube.com/watch?v=1ZTJ9omXOQ0better reflect more recent plans around this topic.
Not super familiar with Carbon but .. what's the elevator pitch for porting my C++ to unsafe Carbon? Can it be done with an automated refactoring tool or something?I feel like if I'm gonna go through the whole nightmare of a code port I should get something for it as opposed to just relying on interop
The idea is that it is an incremental process. By default you should be able to make minimal changes to your code and it should mostly just work. Over time you can use features that more tightly couple you to Carbon, such as memory safety. Google's motivation is supporting its massive C++ codebase while providing a path for memory safety and other features. If your use case does not closely mirror that of Google's, namely, that you have 10+ year old code you intend, and have, to maintain, Carbon probably doesn't make sense for you and that is generally made pretty clear for anyone interested in the language.
Thanks for the correction, I appreciate it!The double negative was not intended :)
People like to always talk about Carbon like that, yet the team is the first to point out anyone that can use something else, should.Carbon is an experiment, that they aren't sure how it is going to work out in first place.> "If you can use Rust, ignore Carbon"https://github.com/carbon-language/carbon-lang/blob/e09bf82d...> "We want to better understand whether we can build a language that meets our successor language criteria, and whether the resulting language can gather a critical mass of interest within the larger C++ industry and communit"https://github.com/carbon-language/carbon-lang/blob/e09bf82d...
Carbon isn't currently memory safe, but Chandler Carruth has made it clear thateverysecurity expert he talked to says the same thing: memory safety is a requirement for security.He at least claims that Carbon will have memory safety features such as borrow checking down the line. I guess we'll see.
It’s worrying to me that Carbon separates data races and memory safety as two distinct things when data races can easily cause both spatial and temporal memory safety issues. Similarly, type safety, can also cause spatial issues (e.g. many kernel exploits in Darwin were a result of causing type confusion for the SLAB allocator resulting in an exploitable memory safety issue).The entire philosophy errs too much in the direction of “being reasonable” and “pragmatic” while getting fundamental things wrong.> Over time, safety should evolve using a hybrid compile-time and runtime safety approach to eventually provide a similar level of safety to a language that puts more emphasis on guaranteed safety, such as Rust. However, while Carbon may encourage developers to modify code in support of more efficient safety checks, it will remain important to improve the safety of code for developers who cannot invest into safety-specific code modifications.That’s really just paying lip service to Rust without recognizing that the key insight is that optional memory safety isn’t memory safety.It is kind of neat just how much Rust has managed to disrupt the C++ ecosystem and dislodge its position.
In principle data races can cause memory safety issues, but they are usually very hard to exploit.Java guarantees VM integrity in the face of data races, while for example many races are UB in theory and in practice in Go. Both are considered safe languages.Sometimes pragmatism is in fact a valid goal.edit: from a practical point of view I don't know how realistic is to retrofit memory safety to a language that lacks it.
An interesting claim to make. Do you have evidence to support that position?I could just as easily offer a valid counter analysis to explain the data. They’re “hard” just because there’s so many easier avenues so attackers just often don’t bother not because they’re intrinsically unlikely. Let’s say you’re successful in eliminating temporal & spatial classes of failures completely (even Rust proponents do not claim this). They’ll focus on data races and type confusion next.> Both are considered safe languagesGo is considered a memory safe languagetodaybecause C and C++ are the anchor that we compare against and we have overwhelming evidence against it (but it’s also where a huge amount of value is in terms of the systems they underpin). In 50 years time, it’s not inconceivable that a lot of the languages may lose their memory safety designation if their runtimes continue to be written to C/C++ (Java) and/or data races remain unaddressed at the language level (Go) and we have overwhelming evidence that exploits just moved on to architectural defects in those languages.It may raise costs of exploits but cybercrime is estimated to be a 10T dollar market next year so there’s clearly a lot of money to put towards exploits.> from a practical point of view I don't know how realistic is to retrofit memory safety to a language that lacks itI think letting people evolve into a more memory safe situation is good. I think doing it partially instead of tackling memory safety in all its forms is just asking for trouble - your attackers will be able to develop exploits more quickly than you are able to update all existing code to more secure language features.
Golang is very muchnotconsidered "safe" from data races.
I explicitly said it isn't. It is still considered a broadly memory safe language.
> Herb side that proposes minimal changesHerb is developing a whole second syntax, I wouldn't call that minimal changes. And probably the only way to evolve the language at this point, because like you said Sean is introducing a different language entirely, so its not C++ at that point.I really like some of Herb's ideas,but it seems less and less likely they'll ever be added to C++
Have you seen some of his recent talks? Lots of underpinnings of cppfront have been added or are in committy.He compares it to the JS/TS relationship.
Nope, that is mostly sales pitch, the only thing added thus far has been the spaceship operator.He also sells the language differently from any other language that also compiles to native via C++, like Eiffel and Nim among others, due to conflict of interest to have WG21 chair propose yet another take on C++.
It's not really a valid comparison though. cppfront is a different language that just happens to be compatible with C++. ts/js is were ts is just js with types. You can comment out the types and it just runs. cppfront's language you'll actually have to re-write the code to get it to compile in C++typescriptfunction add(a: number, b: number): number { return a + b };javascriptfunction add(a/*: number*/, b/*: number*/)/*: number*/ { return a + b };cppfrontadd: (a: float, b: float): float = { a + b; }cppfloat add(float a, float b) { return a + b; }
> ts/js is were ts is just js with types. You can comment out the types and it just runs.Is this true in the general case? I thought there were typescript features that didn't have direct JavaScript alternatives, for example enums.
Enums and namespaces are the only runtime features of TypeScript.So, yes, you can't just strip types, but it's close.
Is there a comprehensive list of such incompatibilities documented somewhere?
https://www.typescriptlang.org/tsconfig/#isolatedModules
That's not the same.That guarantees that the types do not determine the output (e.g. no const enums), not that you can "strip" types to get the same output.
Not that I'm aware of.Decorators would be another example. (Though they have always been marked experimental.)And of course JSX, but that's not a TypeScript invention.
Do you realize that the Typescript example contains strictly more information than the Javascript one (namely, declarations for the type of three things) and is therefore more complex to compile, while the two C++ examples are semantically identical (the last expression in the function is returned implicitly without having to write "return") and the new syntax is easier to parse?
There are several semantic differences between Cpp1 and Cpp2. Cpp2 moves from last use, which is the biggest one. In a contrived example, that could result in a "hello world" changing to "goodbye world" or any other arbitrary behavior change you want to demonstrate. Cpp2 also doesn't require you to order functions and types or declare prototypes, which means partial template specializations and function overloads can produce similar changes when migrating from Cpp1 to Cpp2.
I've written a little demo here:https://godbolt.org/z/xn1eqd5zbYou can see where CPPFront inserts a `cpp2::move` call automatically, and how that differs from a superficially equivalent Cpp1 function.
yes, of course. That's not my point. My point is TypeScript succeeds because it's just JavaScript with types. It's not a new language. cppfront is an entirely new language so it's arguably going to have a tougher time. Being an entirely new language, it is not analogous to typescript
> He compares it to the JS/TS relationship.OP is right, TypeScript is a whole new syntax, and it's shtick is that it can be transpiled into JavaScript.
This phenomenon is mostly because, as the article notes, Google has one of the largest C++ deployments in the world. And since much of the C++ code needs to be extremely platform-agnostic (any given library might be running in a web service, a piece of Chromium or Android, and an embedded smart home device), they tend to be very conservative about new features because their code always has to compile to the lowest-common-denominator (and, more importantly, they're very, very sensitive to performance regressions; the devil you know is always preferred to risking that the devil you don't know is slower, even if it could be faster).Google can embrace modernprocesses, but thelanguage itselfhad better be compilable on whatever ancient version of gcc works on the one mission-critical architecture they can't upgrade yet...
> I compile a lot of C++ code from a lot of places, and the only time I run into code that somehow simplydoesn't workon newer versions of C++I'm impressed that you even get as far as finding out whether that much C++ from disparate sources works on a newer version of C++. The myriad, often highly customized and correspondingly poorly documented build systems invented for each project, the maze of dependencies, the weird and conflicting source tree layouts and preprocessor tricks that many projects use... it's usually a pain in the neck to get a new library to even attempt to build, let alone integrate it successfully.Don't get me wrong, we use C++ and ship a product using it, and I occasionally have to integrate new libraries, but it's very much not something I look forward to.
> riddled with state machinesWhy is this bad?  Normally, state machines are easy to reason about.
The set of developers who say "I want to implement this logic as a state machine" is MUCH larger than the set of developers who say "I should make sure I fully understand every possible state and edge case ahead of time before making a state machine!"
> "I should make sure I fully understand every possible state and edge case ahead of time before making a state machine!"Attempting to understand every state and edge case before writing code is a fool's errand because it would amount to writing the entire program anyway.State machines are a clear, concise, elegant pattern to encapsulate logic. They're dead simple to read and reason about. And, get this, writing one FORCES YOU to fully understand every possible state and edge case of the problem you're solving.You either have an explicit state machine, or an implicit one. In my entire career I have never regretted writing one the instant I even smell ambiguity coming on. They're an indefatigable sword to cut through spaghetti that's had poorly interacting logic sprinkled into it by ten devs over ten years, bring it into the light, and make the question and answer of how to fix it instantly articulable and solvable.I truly don't understand what grudge you could have against the state machine. Of all the patterns in software development I'd go as far as to hold it in the highest regard above all others. If our job is to make computers do what we want them to do in an unambiguous and maintainable manner then our job is to write state machines.
> And, get this, writing one FORCES YOU to fully understand every possible state and edge case of the problem you're solving.lol? ;P Here is just one example of a bug I know of that should exist, today, in Chrome, because, in fact, state machines are extremely hard to reason about. (I have filed previous ones that were fixed, and tons of bugs in Chrome in general are in this category. This one is top of mind as they haven't even acknowledged it yet.)https://issues.webrtc.org/issues/339131894Now, you are going to tell me "they are doing state machines wrong as they don't have a way to discriminate on what the state even is in the first place"... and yet, that's the problem: the term "state machine" does not, in fact, mean a very narrow piece of inherent algorithmic limitations any more than "regular expressions" implies the language is regular, as this is an engineering term, not a computer science one.In the field, state machines just kind of happen by accident when a ton of engineers all try to add their own little corner of logic, and the state is then implied by the cross-product of the state of every variable manipulated by the edges of the machine. This results in a complete mess where, in fact, it is essentially impossible to prove that you've provided edges for every possible state. Nothing in the type system saves you from this, as the state isn't reified.This contrasts with approaches to these problems that involve more structured concurrency, wherein the compiler is able to not only deduce but even some concept of what kinds of edges are possible and where the state lies. (This, FWIW, is a reason why async/await is so much preferable to the kind of callback hell that people would find themselves in, maintaining a massive implicit state machine and hoping they covered all the cases.)
>State machines are a clear, concise, elegant pattern to encapsulate logic. They're dead simple to read and reason about. And, get this, writing one FORCES YOU to fully understand every possible state and edge case of the problem you're solving.It doesn't force you to do that at all.You can start piling in hacks to handle edge casesinsideof certain states, for instance, instead of splitting them into their own states. Or the next dev does.Now it's an implicit ball of mud that pretends to be something else and has a execution pattern that's different from the rest of your company's business logic but not actually strictly "correct" still or easier to reason about for the edge cases.And that's what most people do. They don't use it as a tool to force them to make things unambiguous. They bail when it gets hard and leave crappy implementations behind.Copy-pasting from another reply to a different comment: As a simple example of something that's often left out in a way that fucks up a lot of devs' attempts at state machines, and is super annoying to draw in a typical state diagram: the passing of time.
This just hasn't been my experience but I suppose it's possible if your team is determined enough to write bad code. I'd still wager a bungled state machine is probably 
fairly easier to fix than a bungled mess of branches, but I've never seen such a thing.I actually use passage of time as a factor in state machines all the time on game dev projects. It's pretty simple, just store a start time and check against it. I don't see how "ten seconds have passed since entering state A" is a more difficult condition than any other to draw or model.
In my experience Ye Olde Web App backend services tend to be particularly bad with time because so much is generally done in the request/response model.For business-logic reasons, where I've generally seen it fall apart is when things go from fairly simple things like "after six months of inactivity this account is considered idle" to more complex interactions of timers and activity types. "Move fast and break things", "just get to MVP" attitudes rarely have the discipline to formally draw all the distinct states out as the number of potential states starts to exceed a couple dozen.
The times I’ve bothered to write explicit state machines have created the most solid, confident and bug-free pieces of software I’ve ever built. I would send someone to the moon with them.
Couldn't this be said about any alternative solution?  I fail to see how this is specific to state machines.What do you suggest instead of a state machine?
The "riddled with state machines" from the post I was replying to, while sounding negative, is at least better than the "single state machine" which is probably combinatorially huge and would be impossible to maintain.My rough rule of thumb based on experience is that if the state machine being a state machine is visibleoutsideof it's internal implementation (compared to just an interface with operational methods that don't hint at how things are managed behind the scenes) it's probably too leaky and/or incomplete.I would trust code with extensive state-transition testing (regardless of internal implementation) - I wouldn't trust code that claimed to implement a state machine and didn't have that testing, or extensive documentation of edge cases and what wasleft outof the state machine.As a simple example of something that's often left out in a way that fucks up state machines: the passing of time.
Like properly model a domain in domain terms?
And that won't be a state machine with the states having more fancy names?
It will be, but the idea of having an overview over the states is gone then. There is just modules-> objects with the transitions being method calls. Nobody will have to know all the things about all the state transitions, resulting in another problem (dys)solved by architecture obscurity.If needs be the state-machine can be reconstructed on a whiteboard by a team of five.
A state machine makes the actual program state first class and easy to reason about. One does not even need mutable state to model one. Whereas you appear to be advocating mutable objects. The state space then becomes a combinatorial explosion of all the hidden mutable state "encapsulated" inside the objects. Object oriented programming is not the only way and often leads to a poor domain model. Some OOP evangelists even model a bank account with a mutable balance field and methods for making deposits. This is a absolutely not a faithful model of the domain (ledgers have been used for hundreds/thousands of years). In summary, yes a state machine can absolutely be a good domain model.
It's interesting to know about what state machines you talk. From my experience most of the time it's an entity with state property with finger countable cardinality. And state is assumed to be changed directly. And it's not easy to reason because author only heard about state machines and state transitions are spread over all code base.
I was talking in the most general sense. I am sure there are state machine implementations that are terrible to reason about, especially any that emerge from a codegen tool. But hopefully they are the exception and not the rule.
Woah, this caught my eye:> especially any that emerge from a codegen toolCan you give an example?
I don't follow the connection you're making.State machines often are implemented with mutable objects.And one does not need mutable objects to make "modules-> objects with the transitions being method calls". Every method call could return a fresh, immutable object, nothing requires mutation there.I'd see a method like:`TransitionTo(newState)`as a major smell compared to an explicit`TransistionToNewState`and I think OOO can be helpful (hardly required, of course) in that one neat way of streamlining usage of your code is that if you're implementing objects then the object for something in "State A" might not even have "TransitionToStateC" if that's not a valid operation.(No, you don't HAVE to write state machine code that allows you to ask for invalid operations, but it's a common pattern I've seen in real code and in online discussion/blogs/stack overflow.)
Objects and "method calls" generally implies mutable state to me, but yes the parent was not explicit about this. I assumed mutable (implicit) state was being argued in favour of an explicit state representation. Perhaps I misunderstood.For a state machine, I would expect a function such as:transition : (old_state, event) -> new_stateOr if we use immutable objects, and one method per simple event, then something like:transition_event1 : () -> new_stateWhich I think is similar to what you hace. So I think we are in agreement here.
> I assumed mutable (implicit) state was being argued in favour of an explicit state representation.I definitely was not: I would argue for structured logic rather than implicit state. The idea you are discussing seems to be more about imperative vs. functional design, and that would also be a lot better... but these are Google engineers managing a million interacting state machines via a giant pile of global (mutable) state, resulting in tons of bugs such as this one that isn't even fixed yet:https://issues.webrtc.org/issues/339131894A reply I just left elsewhere on this thread, noting that "state machine" doesn't imply the clean perfectly-factored concept you'd learn in a computer science class: these are ad hoc state machines that result from having a number of developers who don't really care and who think they can "dead reckon" the state of the system if they just add enough transition functions.https://news.ycombinator.com/item?id=42250142
> these are Google engineers managing a million interacting state machines via a giant pile of global (mutable) stateYeah that doesn't sound good. I understand the point you are making now and agree.
The problem is, that the state transition information is usually a complex set of events unleashed upon the little world build into objects. Its basically a small reality simulation, parametrized by all sides, similar to a physics simulation with the external world filtered being input.Now, if we said to someone to model a weather prediction via state-machine- that would be obvious madness. But if we take a small object (like a cubic meter of air) and modelled that part to handle inputs and transfer forces, that generic statemachine will do the job- because the atmospheric ocean of statemachines knows more about the whole system, than a single statemachine.My point is- there is state in a system, that is not explicitly modeled.
Implementasa state machine? But. Your program exists as a set of transforms upon memory. Your programisa state machine! You just need to define the proper morpisms to map your problem domain to the computer domain.
Transformations are separable by principle, it's a fundamental property of them that state machines have as an afterthought that is even hard to represent.It doesn't matter if they have equivalent power. One of those representations fundamentally allows your software to have an architecture, the other doesn't.
How much of software architecture is requiredbecauseof the architecture? If your program has types that are the possible states, and functions to transform between those states, what architecture is needed beyond that? A grouping of related types, perhaps?
Yeah, just one layer of functions is enough for everybody.Let's look next at that "compiler" thing and high-level languages. The hardware-native one suffices, no need for all that bloat.
Please describe "normally". State machines can turn into nightmares, just like any design pattern used poorly.
I've never understood this claim. I find state machines very hard to follow because there's no easy way to tell what paths lead to a given state; they're like using goto instead of functions (indeed they're often implemented that way).
State machines don't have syntax for "transition here when event is encountered no matter what state you are in" so the whole diagram becomes a spaghetti mess if you have a lot of those escape hatches.
> State machines don't have syntax for "transition here when event is encountered no matter what state you are in" so the whole diagram becomes a spaghetti mess if you have a lot of those escape hatches.I place a note at the top of my diagrams stating what the default state would be on receipt of an unexpected event. There is no such thing as "event silently gets swallowed because no transition exists", because, in implementation, the state machine `switch` statementalwayshas a `default` clause which triggers all the alarm bells.Works very well in practice; I used to write hard real-time munitions control software for blowing shit up. Never had a problem.
> hard real-time munitions control software for blowing shit up. Never had a problem.Ha, Ha, Ha! The juxtaposition of these two phrases is really funny. I would like to apply for a position on the Testing team :-)
> Ha, Ha, Ha! The juxtaposition of these two phrases is really funny. I would like to apply for a position on the Testing team :-)It had its moments: used to go to a range where we'd set off detonators. Once or twice in production on site where we'd set off actual explosives.
State machines don't have a native syntax in C++at all, so you can structure them however you want. It's easy to structure a state machine, if needed, so that all (or some) states can handle the same event in the same way.
I always thought this framework was neat:https://doc.qt.io/qt-5/statemachine-api.htmlDownside of course is now you have a dependency on qt.
The downside is that you're now heap allocating at least one object for every state, and I'm willing to bet that each QState has an associated std::vector-style list of actions, and that each action is also its own object on the heap.If you can afford to do things like this you can most likely use something other than C++ and save yourself a lot of headaches.
> If you can afford to do things like this you can most likely use something other than C++ and save yourself a lot of headaches.Surely you can understand that, despite the recent c++ hate, my job doesn't give a fuck and we aren't migrating our massive codebase from c++ to... anything.
Switch + goto is very close to being a native syntax for state machines. It's also very efficient.
goto is exactly this feature.
I believe HSMs can model this, but don't quote me. :)
Yes, of course in theory nested state machines should be able to model this. I feel like adding more complexity and bending the rules is a bit of a concession.
Back in the days we implemented HSM helper classes in about 500 LoC and generated them from Enterprise Architect. No need to write a GUI yourself, but better to have a visual for documentation and review. Worked very well until we replaced EA with docs-as-code, now I miss that there is no nice simulator and Modeler for that workflow.
They can be.  Or they can be... less easy.Imagine you have an informally-specified, undocumented, at-least-somewhat-incomplete state machine.  Imagine that it interacts with several other similar state machines.  Still easy to reason about?Now add multithreading.  Still easy?Now add locking.  Still easy?Cleanly-done state machines can be the cleanest way to describe a problem, and the simplest way to implement it.  But badly-done state machines can be a total mess.Alas, I think that the last time I waded in such waters, what I left behind was pretty much on the "mess" side of the scale.  It worked, it worked mostly solidly, and it did so for more than a decade.  But it was still rather messy.
> Imagine you have an informally-specified, undocumented, at-least-somewhat-incomplete state machine. Imagine that it interacts with several other similar state machines. Still easy to reason about?You think that developers that wrote an informally-specified, undocumented, at-least-somewhat-incomplete state-machine would have written that logic as a non-state-machine in a formally-specified, documented and at-least-somewhat-complete codebase?State-machines are exceptionally easy to reason about because you can at least reverse-engineer a state-diagram from the state-machine code.Almost-a-state-machine-but-not-quite are exceptionally difficult to reason about because you can not easily reverse-engineer the state-diagram from the state-machine code.
In fact state machines are great for documentation even if the code is not explicitly written as a state machine!
Yes, and it's much better than having a dozen or more `bool` values that indicate some event occurred and put it into some "mode" (e.g. "unhealthy", "input exhausted", etc) and you have to infer what the "hidden state machine is" based on all of those bool values.Want to add another "bool state"? Hello exponential growth...
But that is just true of any problem-solving/programming technique.In general, state/event machine transition table and decision table techniques of structuring code are easier to comprehend than adhoc and even worse, poorly understood pattern-based techniques are.
> Now add multithreading. Still easy?> Now add locking. Still easy?Don't do that then.Or rather, either manipulate the state machine from only a single thread at a time; or explicitly turn the multithreading into more states. If you need to wait for something instead of having "do X" you transition into state "doing X".  C#-style async does this in a state machine behind the scenes.
I am out of the loop, what kind of pressure were they putting on Python?
> riddled with state machinesWhat's wrong with state machines? Beats the tangled mess of nested ifs and fors.
That depends on your problem. I've seen useful state machines. I've seen someone implement a simple decoder as a complex any-to-any state machine that couldn't be understood - a single switch statement would have been better. Nothing about state machines, but some people have a hammer and are determined to prove it can drive any screw - it works but isn't how you should do it.
I've adopted a rule of thumb to have a very low bar to skip straight to writing a state machine. I've never once regretted it, personally. I'm sure they can be misused but I haven't came across that.
Switch + goto is the classic way to implement a state machine in C.
> Like... I honestly feel bad for the Rust people, as I do not think the increasing attention they are going to get from Google is going to be at all positive for that ecosystemWe are just now feeling this. Some original contributors left the field, and lately the language has went in directions I don't agree with.
But Google is not even the first. Amazon has had their eyes in Rust for quite some time already.
As an outsider, I'm curious what directions those are. Are you referring to effects or keyword generics or something else?
Endless bikeshedding about `Pin` would be one example. I'm also not sure keyword generics are the correct way.
The discussions around 'Pin' are the opposite of bikeshedding.  It's not about what color to pick for the shed, it's about reworking the feature to make it hopefully easier to reason about and use.
Microsoft has been net negative to the ecosystem as well.
Golang is a great programming language if your alternative is Java, C# or scripting languages like Python/Ruby/etc.  Not everything needs to be written in C++ or Rust from the outset.  It's also reasonably possible to rewrite small codebases from Golang to, e.g. Rust for better performance.
Don't be so mean, it's definitely a step up from PHP.
Based on what an ex-Google developer said in conversation at a party at the weekend (the discussion was about the choice of First Language for a Computer Science degree course, yes, I do go to exciting parties, many of those attending have never even been a CS lecturer):Some years ago Google decided that Go projects were similar engineering effort, better performance, lower maintenance, and so on that basis there was no reason to authorise new Python software and their existing projects would migrate as-and-when.
To the people who work on C++ standards: I approve of the current C++ trajectory and please ignore all of the online noise about "the future of C++." To anyone that disagrees severely with the C++ trajectory as stated, please just consider another language, e.g. Rust. I don't want static lifetime checking in C++ and if you want static lifetime checking, please use Rust. I am not a government contractor, if you are a government contractor who must meet bureaucratic risk-averse government requirements, please use Rust. I have an existing development process that works for me and my customers, I have no significant demand for lifetime checking. If your development process is shiny and new and necessitates lifetime checking, then please use Rust. To Rust advocates, you can have the US government and big tech. You can even have Linux. Just leave my existing C++ process alone. It works and the trade offs we have chosen efficiently accomplish our goals.
You frame it like "Rust advocates" try to infiltrate into C++ language decision making and inject safety features into it. That's not the case at all. For years C++ committee simply ignored the need for safety and they didn't take Rust and lifetime analysis seriously. But now they themselves want it.
C++ has lifetime rules just like Rust. They're simply implicit in the code and not enforced by the compiler. Do you prefer the uncertainty of silent miscompilations and undefined behavior to upfront compiler errors?You're already using a language with a strong type system, so it's confusing to me why you would choose to draw the line here.
> Do you prefer the uncertainty of silent miscompilations and undefined behavior to upfront compiler errors?Yes because then I don't have to spend hours writing esoteric spaghetti code to prove something to the compiler that is trivially known to be true. Your error is assuming static lifetime checking is free. As an engineer, I use judgement to make context-dependent trade offs.If you like playing the compiler olympics, or your employer forces you to, please use Rust.
"Trivially known to be true" until the code evolves making your unstated assumptions not hold and everything breaks, often in complex and unintuitive ways involving interactions across modules.  This is why these automated soundness checks are valuable.
"until the code evolves [...]"That is already a desirable place to be, where you managed to get a working implementation ready to evolve. My issue with opinionated languages like Rust is that they make development more expensive. I then afford to pay the necessary work-effort for fewer projects than I otherwise could if I was to focus more on the problem(s) at hand instead of that and other mandatory constraints forced upon me by the compiler. I very much want my development tools to limit themselves on being tools, to assist me on the part of the problem I chose to focus on with little to no cost paid for their usage. I want to be able to focus on prototyping some working solution first, and only then, if the project's needs really warrant it, to switch on paying the development cost for other aspects, be it safety or whatnot.
> Yes because then I don't have to spend hours writing esoteric spaghetti code to prove something to the compiler that is trivially known to be true.And that’s exactly the reason why we need more safety in C++.I’m terrified at amount of code in real world written with this mindset.
At the same time, you should recognize that not all real code in the world is used to run planes & thermonuclear power plants. For a lot of the business software, it's actually fine if it's not perfectly safe.  So if it's cheaper/ faster to develop it without paying the price of static safety checks, who is to say that this was a bad tradeoff?I actually love the ideas that Rust brought forth. It definitely has a place in the ecosystem, and I'm glad to hear critical software is being rewritten in Rust! But that doesn't mean that C++ should copy it.
C++ doesn't permit you to write code that's not perfectly safe. By using a C++ compiler, you're promising that youwillwrite safe code even if the compiler can't verify that, lest nasal demons and other misfortunes fall upon you. If your code isn't safe and you expect that to be fine, you're not writing C++. This is a discussion about C++, so the default assumption is that you'll pay the costs of safe code instead of inventing an ill-specified dialect that happens to do what you want when it's shoved into a C++ compiler.If you think we should instead evolve C++ so that safety isn't mandatory I'm right there with you, but it's not where the language is today and that discussion has also been shut down by the evolution working group. Moreover, Bjarne's policies mean that telling the critical software people to go fuck off to a different language fundamentally isn't part of the plan either.
It is kindof an interesting point you bring up here. However, it's also true that languages (and software in general) are what people make of it, not necessarily what their creators intended. I do believe that the stuff that happens to do what you want when shoved into a C++  compiler _is_, for all intents and purposes, C++. And I kinda' think/feel that that is also what the committee is saying - "we want C++ to keep doing that, rather than evolve into a safer thing that is no longer C++"
I get the argument, but the community argument doesn't actually change anything. No compiler will guarantee any particular behavior in the presence of memory safety issues. Very few programs happily tolerate random memory corruption or race conditions, etc.
Run valgrind on any large successful code base and you will find tons of memory corruption. It just happens to occur in places where it does not matter.
> For a lot of the business software, it's actually fine if it's not perfectly safe.Is it fine if it silently gives the wrong answer? If so, why are you bothering with the software at all?In my experience all nontrivial C++ codebases have silent memory corruption bugs (at least when built with popular compilers).
Well, let's put it like this:- Webkit, GCC, and a few others are non-trivial C++ codebases that are (I argue) useful.- In your experience, since they are non-trivial, they have silent memory corruption bugs (i.e. they are not "perfectly safe").Does this answer the "why bother with software at all" question?
Most C++ developers care greatly about the quality of their code, and suggesting that since the code isn't in a life threatening situation like a flight controller or medical device it can be buggy with no repercussions is pretty silly.Your examples of GCC and Webkit are both projects that have spent enormous amounts of effort to be as memory safe as they can be, and have both had many memory safety related CVEs in the past. As was already pointed out, you still have to pay the cost of engineering memory safe code, even when your compiler/static analysis doesn't have your back.
I was not saying anywhere that people don't or shouldn't care about the quality of their code. I was just pointing out that, whether we like it or not, "quality" is just one of the factors that goes into the mix of "things to optimize of". Other factors like "time" and "effort" and "efficiency" and "compatibility" and even trivial stuff like "familiarity" play a role - or else you'd have formal proofs written in TLA+ or Alloy or the like, before writing any system; And you'd have people immediately switch to safer languages like Rust (which is obviously not happening at scale).The GCC/Webkit examples were not the best examples, but were nevertheless easily available examples that made one particular point: OP's comment was self-contradictory.
> Most C++ developers care greatly about the quality of their codeNot at our org. Though I know a couple of die hard fans that will eat you for lunch if you do something stupid or ugly.
Webkit, as I understand it, is not really a C++ codebase built with a popular compiler, it's a codebase that follows its own significantly stricter standards and has a lot of additional tooling to avoid bugs.And I'd say that even with all that additional effort, it has a level of bugs that's not "fine". Indeed, per the article, I suspect that the maintainers of Webkit are some of the people pushing to make C++ more Rust-like.
Webkit TBH wasn't a great example, since it's arguably a piece of software that would benefit from being developed in Rust. That said, the point is that we don't need "one language to rule them all" - C++ has made some tradeoffs, that will not be ideal in all circumstances/for all projects. Trying to change the tradeoffs because of a handful of projects (like Webkit) would be better suited to new tradeoffs is not necessarily the right choice for the language itself, or its community of users. Things are not so simple, "There are 2 factions of C++, those that agree with me and are right, and those that disagree and are wrong".
I think silent memory corruption is almost never a good tradeoff. (The one possible exception is something like a single-player videogame, where unknown corruption might be less bad than crashing - but even then, avoiding having the situation come up in the first place is better). An argument used to be made (if not in so many words) that accepting a certain amount of occasional memory corruption was a necessary tradeoff for performance; it's an argument that I was always dubious about, and now Rust has proven it completely false.Fundamentally I don't think this is a case where C++ makes a deliberate design tradeoff that makes sense for some projects. I think it's just a bad design choice (not even a choice as such - it wasn't a question that was considered at all when C++ was first designed) that should be corrected. Sometimes there is a right answer.
> Sometimes there is a right answer.Indeed. And when that "right answer" comes along, it tends to swipe away everything else. If it's universally better, why wouldn't it?Except that, Rust does not do that. Which is a hint that it's not an "universally right answer", but a right answer for a subdomain of problems. That's basically what I was trying to say. That it does come with its own tradeoffs/downsides.(maybe I'm wrong and it's only a matter of time until that happens; but I don't think so.. it's been a while, there was time for it to make the impact. Lifetime annotations are not yet adopted by any other mainstream language, AFAIK)
> Indeed. And when that "right answer" comes along, it tends to swipe away everything else. If it's universally better, why wouldn't it?> Except that, Rust does not do that. Which is a hint that it's not an "universally right answer", but a right answer for a subdomain of problems. That's basically what I was trying to say. That it does come with its own tradeoffs/downsides.Rust may not be the only right answer, but memory unsafety is the wrong one. New projects overwhelmingly pick memory-safe languages, governments and organisations are banning memory-unsafe languages at least for new projects. I don't think anyone is picking C++ at this point if they don't already have a big sunk cost invested in it (even if that cost is just their personal programming experience).> Lifetime annotations are not yet adopted by any other mainstream language, AFAIKLinear Haskell is getting there, but most languages aren't flexible enough to retrofit lifetimes (or at best it would be a multi-year effort, like adding types to Python) - as we're seeing in this whole C++ discussion. Also non-GC languages are niche in the first place, and the problem lifetimes solve is a lot less urgent in a GC language. I don't think any post-Rust language has hit "mainstream" yet (we only really get a couple of new mainstream languages a decade), so we'll see what happens in the future.
I've found that often when I am writing esoteric spaghetti rust code... I need to start thinking about what I am trying too do! Most of the time it's a bad idea :)
If one needs to "prove something to the compiler" it is usually something both complex and against the grain; on the other hand lifetime annotations are usually just "promise something to the compiler" to allow it to make a better job.
> As an engineer, I use judgement to make context-dependent trade offs.Well said.This is why i am firmly in the Stroustrup camp of backward compatibility/zero overhead/better-C/etc. goodness of "old C++". I need to extend/maintain/rewrite tons of them and that needs to be as painless as possible. The current standards trajectory needs to be maintained.The OP article is a rather poor one with no insights but mere hoopla over nothing.
If it's hoopla over nothing, why do you firmly identify with one of the factions defined by the article?
What a silly question! There is no major schism in the C++ community as the article implies; merely a strong difference of opinion on certain proposals. This is normal in any committee. But since people are strongly wedded to their own proposals it might seem more severe than it actually is.
> to prove something to the compiler that is trivially known to be trueI don't think you've ever done any serious work with lifetimes. I've been a rust developer for a number of years, and I have never once encountered a situation where the rust compiler forces me to add annotations for something which is trivially true. Never.What actually happens is 95% of the time I never have to add lifetime annotations anyway because the compiler infers the correct annotation from the lifetime elision rules.  The remaining 1 in 20 instances is when the borrow checker yells at me, and literally every single time it is due to a latent logic bug in my code. For example, accessing memory after it's been freed, or using a container after it has been consumed. Stuff that C++ would call "undefined behavior" and are generally considered Very Bad Things by C++ developers as well.It boggles my mind that you don't want the compiler to tell you that “you have a logic error here.”
> C++ has lifetime rules just like Rust. They're simply implicit in the code and not enforced by the compiler.The problem is that the rules enforced by Rust is not restricted to lifetime rules, it's a much much larger superset that includes quite a lot of safe, legitimate and valid code.
Sure, but that's not a design philosophy C++ adheres to. Look at the modern C++ guidelines or profiles. The entirepointis to rule out large swathes of safe, legitimate, and valid code in an optional and interoperable way.C++ isn't beholden to Rust's trade-offs either. There's a whole spectrum of possibilities that don't require broken backwards compatibility. Hence:
"Why draw the line specifically at lifetime annotations?"
> You're already using a language with a strong type systemI'll have you know I made a variable void* just yesterday, to make my compiler shut up about the incorrect type :D
While programming in Rust, I've never thought to myself, "man, this would be so much easier to express in C++". I've plenty of times thought the reverse while programming in C++ though.Edit: except when interfacing with C APIs.
I have had the exact opposite experience.
Most kinds of OOP can be expressed idiomatically in Rust.  The big exception is implementation inheritance, which is highly discouraged in modern code anyway due to its complex and unintuitive semantics.  (Specifically, its reliance on "open recursion", and the related "fragile base class" problem)
People often say that modern c++ doesn't have the problems needing a solution like rust. Ironically that means people who write modern c++ haven't had any aramp up time needed when joining our rust projects. They were already doing things the right way. At least mostly. But now they don't have to worry about that one person who seems to be trying to trick the static analysis tools on purpose.
Anything that involves object graphs (as opposed to trees) is a pain in Rust.
True, but not in a way that wouldn't be just as painful in C++.
In Rust, the de facto standard advice for such cases seems to be, "just use indices into an array instead of references".While this is sometimes done in C++ as well for various reasons, it's certainly not the default pattern there. If you have two things that need to point to each other, you just do that.
> While this is sometimes done in C++ as well for various reasons, it's certainly not the default pattern there. If you have two things that need to point to each other, you just do that.And then you have to handle all the subtle memory bugs that you've introduced by doing that.
I'm not arguing that there isn't a gain here, but GP's original assertion was that> While programming in Rust, I've never thought to myself, "man, this would be so much easier to express in C++".This is a concrete example of something that is much easier to express in C++. And, sure, you do pay the tax for that (although I will also dispute the notion that it isimpossibleto write C++ without memory bugs; it's just hard).
I guess this is a semantics argument, but I assume they mean to express the same thing with same (or reasonably same) security guarantees. After all, the security and "bug freeness" is part of what they are expressing. If you attempt to create something reasonably similar to Rust, you do suddenly need a lot of complex checking code and maybe tests for things that were trivial in Rust (because the compiler does the tests for you).
This is interesting because i'm writing quite a bit of embedded Rust, and i always run into limitations of very barebones const generics. I always wish they'd have half the expressiveness of C++ constexpr and templates.Win some, lose some though, as the overall development workflow is lightyears ahead of C++, mostly due to tooling
The expressiveness of const generics (NTTPs) in C++ wouldn't go away if it adopted lifetime annotations and "safe" scopes. It's entirely orthogonal.Rust decided to have more restrictive generic programming, with the benefit of early diagnostic of mistakes in generic code. C++ defers that detection to instantiation, which allows the generics to be more expressive, but it's a tradeoff. But this is an entirely different design decision to lifetime tracking.
Rust generics are not intended as a one-to-one replacement for C++ templates.  Most complex cases of template-level programming would be addressed with macros (possibly proc macros) in Rust.
Const generic expressions are still being worked. They are what is blocking portable simd. They are also a much cleaner way to implement things like matrix operations or really anything where a function with two or more arguments of one or more types returns things that have types that are a combination or modification of the input types.
The problem AIUI is that "const generic expressions" in full generality are as powerful as dependent types. It's not clear to me that the Rust folks will want to open that particular can of worms.
I thought dependent types were types that depended on a value?  What they are proposing are types that depend on types or compile time constants.
The problem is combining the "const generic" and "expression" part.  If your "compile time constants" can actually be complex expressions, you arguably end up with the same kind of generality as dependent types.This is true even for expressions that are only evaluated in a compile-time context, since dependently-typed languages do "everything" at compile time anyway, they don't have a phase distinction where you can talk about "runtime" being separate.
C++ is not a dependently typed language, for the same reason that templates do not emit errors until after they are instantiated. All non-type template parameters get fully evaluated at instantiation time so they can be checked concretely.A truly dependently typed language performs these checksbeforeinstantiation time, by evaluating those expressions abstractly. Code that is polymorphic over values is checked for all possible instantiations, and thus its types can actually depend on values that will not be known until runtime.The classic example is a dynamic array whose type includes its size- you can write something like `concat(vector<int, N>, vector<int, M>) -> vector<int, N + M>` and call this on e.g. arrays you have read from a file or over the network. The compiler doesn't care what N and M are, exactly- it only cares that `concat` always produces a result with the length `N + M`.
I'm not sure what "dependently typed" means but in C++20 and beyond, concepts allow templates to constrain their parameters and issue errors for the templates when they're specialized, before the actual instantiation happens. E.g., a function template with constraints can issue errors if the template arguments (either explicit or deduced from the call-site) don't satisfy the constraints, before the template body is compiled. This was not the case before C++20, where some errors could be issued only upon instantiation. With C++20, in theory, no template needs to be instantiated to validate the template arguments if constraints are provided to check them at specialization-time.
This is the wrong side of the API to make C++20 dependently typed. Concepts let the compiler report errors at the instantiation site of a template, but they don't do anything to let the compiler report errors with the template definition itself (again before instantiation time).To be clear this distinction is not unique to dependent types, either. Most languages with some form of generics or polymorphism check the definition of the generic function/type/etc against the constraints, so the compiler can report errors before it ever sees any instantiations. This just also happens to be a prerequisite to consider something "dependently typed."
> performs these checks before instantiation timeNotably Rust type-based generics do this, a key difference wrt. C++ templates.  (You can use macros if you want checks after instantiation, of course.)
In c++ it does care what N and M are at compile time, at least the optimizer does for autovectorization and unrolling. Would that not be the case with const generic expressions?
The question of whether a language is dependently typed only has to do with how type checking is done. The optimizer doesn't come into play until later, so whether it uses the information is unrelated to whether the language is dependently typed.
Ok, I think I understand now, but is it really dependently typed just because it symbolically verified it can work with any N and M? Because it will only generate code for the instantiations that get used at compile time.
Is what really dependently typed? I'm saying C++ isnotdependently typed, because itdoesn'tdo any symbolic verification of N and M.
If rust did add const generic expressions I mean. It still would only generate code for the used instantiations.
Ah, I wasn't really talking about Rust.Rust already does have some level of const generic expressions, but they are indeed only possible to instantiate with values known at compile time, like C++.The difficulty of type checking them symbolically still applies regardless of how they're instantiated, but OTOH it doesn't look like Rust is really trying to go that direction.
the only thing needed here is to be able to lift N & M from run-time to the type system (which in C++ as it stands exists only at compile-time). For "small" values of N&M that's doable with switches and instantiations for instance.
The point of dependent types is to check these uses of N and M at compile time symbolically, for all possible values, without having to "lift" their actual concrete values to compile time.Typical implementations of dependent types do not generate a separate copy of a function for every instantiation, the way C++ does, so they simply do notneedthe concrete values in the same way.
> as the overall development workflow is lightyears ahead of C++, mostly due to toolingMy experience has been the other way around. Eclipse-based IDEs from NXP, TI, ST all have out-of-the-box usable tooling integration:- MCU pinout and configuration codegen- no need to manually fiddle with linker scripts- static stack and code size analyzers (very helpful for fitting stuff in low-cost MCUs)- stable JTAG-based debugging with:- peripheral registers view (with bitfield definitions)

  - RTOS threads view (run status, blocked on which resources, ...)And yes, these are important enough for me to put up with Eclipse and pre-modern C/C++. I really want to write Rust for embedded but struggling with the tooling all the time didn't help.
That's actually quite interesting because this is not an inherent limitation of Rust, and it is definitely planned to be improved. And AFAIK, today (as opposed to last years) it is even being actively worked on!
And what about, for example, those government contractors who are in the same position as you: they have a large C++ codebase that currently works, and is too big to re-write in rust? Now they're being asked to make it safer. How will they do that with the "existing C++ process"?
Didn't Project Zero publish a blog post a few months ago, saying that old code isn't your security problem? They said it's new code you have to worry about. Zero also had copious amounts of data to demonstrate their point. In any case, if you really want to rewrite C++ in Rust, LLMs are fantastic at doing that. They're not really good yet at writing a new giant codebase from first principles. But if you give them something that already exists and ask them to translate it into a different language, oftentimes the result works for me on the first try. Even if it's hundreds of lines long.
A link would be helpful, but at face value: of course old code vulnerabilities are still a problem. Vulnerabilities in old code make the headlines all the time.
It was difficult to dig up, but I found it for you.https://security.googleblog.com/2024/09/eliminating-memory-s...Also headlines do not accurately model reality. The news only reports on things that are newsworthy. It's comparatively rare that we'll discover new vulnerabilities in old code that's commonly used. That's what makes it newsworthy.
Thanks. It's an interesting analysis around the "vulnerabilities decay exponentially" model, discussing how there are more vulnerabilities to be found in new code than old code given equal attention.
Yeah I remember reading that post about bugs over time.  IIRC 5 years was the time it takes for most bugs to get ferreted out.
The funny thing about government funding is that it may be easier to secure capital for a Rust rewrite than for ongoing maintenance to add static lifetimes and other safety features to an existing C++ codebase.Legislatures seem a lot more able to allocate large pots of money for major discrete projects than to guarantee an ongoing stream of revenue to a continuing project.
C++ is on the trajectory to create a future with more safety. Should we do profiles or static lifetime checking (or something else??) is still an open question (and both may be valid).  However I'm glad c++ is thinking about that. We have real problems around safety in the real world and people are writing unsafe code even when modern safe code would be easier to write.Of course it remains to be seen how this all plays out.  Static lifetimes can be done good or bad. Profiles can be good or bad. Even if whatever we come up with is done well that doesn't mean people will (I know rust programmers who just put unsafe everywhere).
Profiles are vaporware.  The C++ folks are pushing a fantasy of "full memory safety with no changes to existing code, not even annotations to enable sound static analysis."  That's just a non-starter, there is no way to get to full memory safety from there unless you count very silly things like making "delete" and "free()" a no-op - and also running everything in a single thread for "concurrency safety".
The only way to get anywhere is provide a path forward.  I have a lot of C++98 code that has been working just find for 14+years (that is since before C++11).  It isn't worth changing that unless we discover a bug in the code (after 14+ years unlikely) or we need to add new features (if we haven't in 14+ years we probably won't need a new feature there anytime soon).  Code I write today is the latest C++. What I really want is a way to say don't write the bad things today, but still allow that old code to work.  That is what profiles promises to me.  Sure we will never to get full memory safety that way, but that isn't my goal, I just want to make my new code better, and when I come back to old code improve that too.
The case for "100% Safe C++" is that you might be able to annotate that old C++98 code in ways that don't otherwise alter its semantics, but still ensure safety.  That would be a one-time cost that might be well-worth paying if the cost is low enough - Where "cost" depends on developer experience as opposed to mere volume of annotations.  A "viral" compiler feature that auto-surfaces all the places that will need annotation for a given level of safety has the potential to be quite easy to learn and use effectively.  It's not clear why the C++ folks are rejecting that approach, seemingly out-of-hand.
I have > 10 million lines of C++ that is not annotated.  There are many projects much larger than mine.  If you cannot automatically annotate the code there is no point in trying as you can't do it manually. If you can automate it why not just build that into the compiler and skip the syntax?
> If you cannot automatically annotate the code there is no point in trying as you can't do it manually.How can you know this without a "viral" analysis that tells you how much annotation is needed, and where?  Perhaps the code factors out all the low-level, "memory unsafe" hacks to its own module, and that can be feasibly annotated.  It's just not something we can know in advance.
>  Perhaps the code factors out all the low-level, "memory unsafe" hacks to its own module, and that can be feasibly annotated.While it is theoretically not impossible for that scenario to occur, I'd say it sounds wildly unlikely for anything that can be descried as 'old' code.
I suspect the best case scenario is a "Stone soup".https://en.wikipedia.org/wiki/Stone_SoupThe fantasy is enough to get engagement and once you have engagement you can persuade people to do a "little" extra work to get the full benefits. My mother won't buy the product for $5, but if you tell her that it costs $10 but they're 2-for-1 today, she's going to buy that and feel like she got a bargain.In terms of actually solving the problem well, it's not even captured in these hypothetical regulatory requirements. What you actually want is asafety culture, Rust has one, C++ does not, and no technology will change that. From what I can tell nobody at WG21 wants that to change anyway.
> What you actually want is a safety culture, Rust has oneRust has a safety culturebecauseit involves requirements for Safe Rust that preserve safety while also playing well with modularity and iterative development.  If "Safe C++" can enforce similar requirements, we can expect that a safety culture can be sustained there as well.
The technology does not gift you associated culture, and it's worth knowing that even far outside this business because it applies everywhere.Yes a technology can be enabling, but, it isn't enough to inculcate the desired culture, that has to come from somewhere else. You can't "sustain" something which does not exist.Actually WG21 ("The C++ Language Committee") illustrates this well in another way. When WG21 was created it wasafterthe Mother Of All Demos, and so after video conferencing exists as an idea, but to be fair to them it was not really practical at the scale needed for WG21 processes at that time. When C++ 98 shipped it was just about practical, although most ordinary people would have needed to travel to some place with appropriate equipment. By this point the IETF is routinely but not yet universally using such technology.By the time C++ 11 shipped, I have an ordinary job where I worked full time from home, travelling to a physical location only once or twice per month because video conferencing is now such a mudane and ordinary capability as to go unremarked.Onlysincethe COVID-19 pandemic has WG21 finally adopted the option for attendance withoutflying around the world several times per year. The technology to do this had existed for decades, but theculturedid not exist.
If you have access to the WG21 meeting minutes, it appears the safety discussions of the last meeting were quite entertaining.
I assume they aren't freely available online? How does one gain access to these meeting minutes?
One becomes a WG21 member.
Look, we need more than just promises. C++ is charting a future to the past in the most torturously slow process possible, primarily because of absolutely intrasigent performance obsession that won't even admit the possibility of a 1% performance overhead for bounds checks. The C++ steering committee are the real extremists that are holding back the entire software industry because of a sacred cow and a free pass to externalize that cost onto the rest of us in terms of significantly less secure software.
> The C++ steering committee are the real extremists that are holding back the entire software industry because of a sacred cow and a free pass to externalize that cost onto the rest of us in terms of significantly less secure software.The C++ leadership serves the C++ community, not the entire software industry. You and everyone who disagrees with them are free to use and write software based on other languages, e.g. Java and Rust.
Many in the C++ community wouldn't acknowledge that.Which is why disabling RTTI, disabling exceptions, creating their own standard library replacement, static analysers forbinding specific language constructs, is such a big deal in some C++ circles.
You can even add nonstandard features to existing compilers!The neat thing is that once the standard committee learns about this use case, it could get de facto support as existing use!
Ok.
Please, just use your current C++ standard.
But we will go to use the new one with all features we want to use.
Who’s “we”? The C++ developers that like the “Safe C++” proposal which is tacking Rust on top of C++ are a tiny minority.It seems very fair to tell them to just use Rust and leave C++ alone.
Indeed, that is exactly what many FAANG companies are doing, have you noticed the slow down in velocity in major compilers regarding ISO C++ compliance?
See Apple’s slowdown on clang development and subsequent advances in Swift<->C++ interop (even going as far as merging Swift code into FoundationDB)And ofc Google’s investment in Carbon
Or MSVC slow pace with C++23, after being the first to reach full C++20 support.Everyone else outside the big three, is somewhere between C++14 and C++17.
Nope, still using C++17 and not bothered by any slowdown. 
C++ has been moving too fast lately.
It is currently an open debate what will be the very last ISO version the world will care about, C++17 might be the one, or C++26, bets are open.
obviosly.. we is> Relatively modern, capable tech corporations that understand that their code is an asset. (This isn’t strictly big tech. Any sane greenfield C++ startup will also fall into this category.)and @bagxrvxpepzn is ofc> Every ancient corporation where people are still fighting over how to indent their code, and some young engineer is begging management to allow him to set up a linter.:)
Thank you for this. C++ should NOT try to be Rust. I find modern C++ really nice to program in, for the work I'm doing - 3D graphics. The combination of very powerful abstractions and excellent performance is what I'm looking for. I'm more than willing to endure percived lack of safety in the language.
The lack of safety isperceivedbecause it is there. There is no proof that anyone can write a C++ program larger than, say, 100k lines of code that doesn't have memory safety issues.
And that memory safety is completely not an issue if you're writing something like a game, trading system, simulation, internal application or science calculation where there's no potentially hostile users who could do real harm by hacking your code. It's just a class of bug that in modern C++ is generally far outnumbered by logic bugs.
Games absolutely are a problem for lack of memory safety - because the majority of games played today are connected to the internet explicitly. For trading system I don't even know what you mean, but I can't think of a trading system where youwouldn'tcare about security.For simulations and scientific calculations, I do agree, to a vast extent. But in a world that is moving more and more towards zero-trust networking, even many of those will start being looked at as potential attack vectors into other systems.
As a DAW developer, I find myself chuckling over security concerns in other kinds of apps.You see, it is absolutely expected and required that our applications will load and run arbitrary 3rd party code, generally with the expectation that it lives in the same address space as our application (though this is not formally required).No sockets, no network, no backdoor hacks. You write code, call it a VST plugin, make it sound desirable ... we are expected to load and run it.Yes, several DAWs have made the move toward out-of-process execution of plugins, but that doesn't begin to address the myriad problems caused by loosely-written plugin APIs not adequately pinning down threading, thread priority, memory access and more.Filesystem access? Of course! That code runs as you! Because you want it to!
And when someone creates a project file that sends them the personal information of anyone who opens it, is that an issue? Yes, pervasive arbitrary code plugins are game over if you can get anyone to use your plugin, but there's at least some awareness that you need to be careful opening a plugin you don't trust.
Not sure that's true for the majority of DAW users.Plugins are not associated with attack vectors, even though they are literally just that.
I may be off base, but as the world moves to zero-trust networking, we can always embed a zero-trust network into our C++ app so that it can be distributed across the network while having no listening ports on the underlay network  - i.e., my memory safety exploit cannot just be exploited by anyone on the WAN, LAN, or host OS network. My C++ app unattackable via conventional IP-based tooling, all conventional network threats are immediately useless.This capability exists in completely open source, such as OpenZiti -https://openziti.io/.
The way C and C++ are standardized, you can't rely on the correct functioning of anything in the presence of undefined behavior, including memory unsafety. For what it's worth, I also opened a random file in the OpenZiti C SDK and immediately found safety issues like this:https://github.com/openziti/ziti-tunnel-sdk-c/blob/9993f61e6...That's why this topic is such a big deal. Even people who really should know better like the OpenZiti authors aren't able to reliably write safe code.
Why is that a safety issue?
Malloc/Calloc can fail even if they typically don't on most Linux systems. You should always check for null pointers before accessing the resulting buffer, which doesn't happen here. The connections() block is also never explicitly freed anywhere I was able to find in a quick search. That's allowed, but definitely bad practice.
You'll still have to e.g. parse and interpret data from the internet if you want to communicate with anyone else, and that's a potential vector for an exploit. This has commonly be the way exploitations work in games.
The edge SDKs do not parse and interpret data from the internet, they provide ingress/egress off the overlay. They authenticate and authorise to the controller and make outbound connections to the overlay network. This is why any app embedded with Ziti has no listening ports to host OS network, LAN, or WAN; they only listen to specific application calls across the overlay.Now, you may say, "well, you have merely moved the listening port from the app to the overlay". Yes, true, not simple. Firstly overlay is written in Golang (thus memory safe). Secondly, if a vulnerability exists in the overlay network that would allow an attacker to bypass the security of the zero trust network, but what does that mean in practice? Well, to do this they would need to:- bypass the mTLS requirement necessary to connect to the data plane (note, each hope is uses its own mTLS with its own, separate key).
- strong identity that authorizes them to connect to the remote service in question (or bypass the authentication layer the controller provides through exploits... note again, each app uses separate and distinct E2E encryption, routing, and keys)
- know what the remote service name is, allowing the data to target the correct service (not easy as OpenZiti provides its own private DNS that does not need to comply to TLDs, so it could literally be 'madeup.service.123')
- bypass whatever "application layer" security is also applied at the service (ssh, https, oauth, whatever)
- know how to negotiate the end to end encrypted tunnel to the 'far' identitySo yes, if they can do all that, then they'd definitely be able to attack that remote service. But I said "remote service", not "remote services". All that work and compromises and they only have access to 1 single service among hundreds, thousands, or potentially millions of services. Lateral movement is almost impossible. So the attacker would have to repeat each of the 5 steps for every service possible. Also, they don't know which company sits behind which OpenZiti fabric, so its pot luck if its even against the target they want to try and exploit.Finally, we have developed a stateful firewall called 'ZitiFW' -https://github.com/netfoundry/zfw- which uses eBPF to look at the IP information of any incoming connections/packets to an Edge Router (Ziti's Policy Enforcement Point), if a connection/packet is received from an IP address which is not correlated to a known, bootstrapped endpoint to the overlay, the packet can be blackholed.
The issue of memory safety goes well beyond adversaries "hacking your code".  Without memory safety, your code doesn't even have any kind of well-defined semantics so it's not feasible to defend against even "logic" bugs by automated means.If you care about program correctness in any real sense, memory safety is table stakes.
No, this is not how it works. Even without memory safety, the code has well-defined semantics for correct input, i.e. input that does not trigger undefined behavior.  And if you prove your program correct for all inputs, this then implies that it does not have undefined behavior for any input. Memory safety is not a prerequisite for applying formal methods to show correctness.
On the contrary, why would I not want these things in C++ if I'm developing every project with -fsanitize=address,undefined to catch these types of errors anyway?
What I don't understand is why you demand that C++ evolution be halted in a clearly suboptimal position so you don't need to change your processes. Just use the version of C++ that meets your needs, you clearly don't want nor need new developments. You are fine with being locked into bad designs for hash maps and unique ptr due to the (newly invented, in 2011/13) ABI stability being made inviolable, you clearly need no new developments in usability and security.So why not be honest and just use C++01, or 11, or whatever it is that works for you, and let the rest of the ecosystem actually evolve and keep the language we invested so much effort into as a viable alternative? There's zero benefit, except to MS who want to sell this year's Visual Studio to all the companies with 80's-era C++...
> What I don't understand is why you demand that C++ evolution be halted in a clearly suboptimal position so you don't need to change your processes.I don't demand that C++ evolution be halted. I support the current trajectory of not adding viral annotations for the sake of implementing static lifetime checking. I want C++ to evolve into a better version of itself, I don't want it to become something it's not. If you want static lifetime checking, please use Rust. It already exists and it's great for people who need static lifetime checking.
> evolution be halted in a clearly suboptimal positionIt’s clear it’s imperfect. But not clear there is an obvious path to a nearby local maxima.Design choices have tradeoffs.And even if that were true, who would take advantage of that “better” language in a purely abstract sense? New language standards primarily exist to benefit existing C++ code bases, and the cohort of engineers who work on them. You have to consider that social reality.
> I approve of the current C++ trajectoryBut this isn't one thing. Part of the problem is contradictions even in the definition of this trajectory. An important example: Will it be "you don't pay for what you don't use", or will it be "stable ABI"? There are adopted papers which say both - as the linked article indicates.> please just consider another language, e.g. RustI'm not one of those people necessarily, but - Rust has its own set of design goals and problems. It is legitimate to want C++ to go one way rather than another.> To Rust advocates, you can have the US government and big tech.Now you are kind of contradicting yourself, because wide applicability is definitely a goal for C++.
Imagine an engineer in any other field acting like this."I don't want to install air bags and these shiny safety gadgets into my cars. We have been shipping cars without them for years and it works for us and our customers."The problem is that it doesn't actually work as well as you think, and you are putting people at risk without realizing it.
You're trying to install airbags on a motorcycle, though. The design of the vehicle/language is incompatible with airbags/lifetimes. So if you want airbags... don't use C++.(Yes, I know about airbag vests. Let's analogize those with external static checkers.)
We learned a long time ago as an industry that the expert car drivers are not immune to causing pile ups, which makes it all our problem to solve.Safety by default with escape hatches when absolutely necessary is the better way to go for all, even if it means some power users have to change their ways.
I don't know enough about what it would take to implement static lifetime checking.  Is that fundamentally impossible to do in a backwards compatible way?
It depends on what you mean by "backwards compatible," and what you mean by "static lifetime checking" :)The profiles proposal suggests adding static lifetime checking, "without viral annotations." I use quotations because I don't really agree with this framing, but whatever. The paper is here if you'd like to read it yourself:https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p30...The core idea here is that you add annotations to opt in or out of certain checks. And opting in may be a compiler flag, requiring no changes to source code. So that would be "backwards compatibility" in that sense. Of course, code may fail these checks, so you'll have to add annotations to opt out, or re-write the code. We will see in practice how much change is required once implementations exist and are tried out.But the other part is, these profiles do not attempt to cover all valid cases. And what I mean by that is, there are some lifetime issues that this proposal does not attempt to analyze. And, where the analysis is similar, they offer a subset of what other proposals do. These decisions were made because the authors believe that they'll reduce a significant number of issues, and are easier to adopt. And that's worth it instead of going for more checks.The competing proposal, Safe C++, has you opt into safety checks on a file-per-file basis. So in that sense, it is also backwards compatible: all existing code compiles as-is. When you opt in to those checks, it adds new syntax, similar to Rust, to do the safety analysis checks. So you gain this benefit for only new code, but it also you get much more power. This syntax is necessary to communicate programmer intent to the checks, but is the "viral annotations" that the proponents of profiles don't like.So, basically, that's the thing: both are backwards compatible, but offer very different tradeoffs in the design space.
If you want alias tracking and lifetime checking, yes they are backwards incompatible. They need "viral annotations" if we speak with the C++ committee terminology.
Please don't shame people for using pseudonyms on here, regardless of whether you disagree with their concrete point. It's nice to have a place where people don't have to think about how their friends, family or colleagues will react before posting something.
> But why say so under a pseudonymThat's a rather odd complaint, coming from a pseudonym.
It's not a hard dichotomy. Almost all of the rules Rust imposes are also present in C++, enforcement is simply left up to the fallible human programmer. Frankly though, is it that big a deal whether we call it unique_ptr/shared_ptr or Box/Arc if a lifetime is truly unknowable?Rust shines in the other 95% of code. I spend some time every morning cleaning up the sorts of issues Rust prevents that my coworkers have managed to commit despite  tooling safeguards. I try for 3 a day, the list is growing, and I don't have to dig deep to find them. My coworkers aren't stupid people, they're intelligent people making simple mistakes because they aren't computers. It won't matter how often I tell them "you made X mistake on Y line, which violates Z rule" because the issue is not their knowledge, it's the inherent inability of humans to follow onerous technical rules without mistakes.
Yeah, I don't end up fighting rust very often, and when I do, it is right. And when I run into a case that it isnt, I have unsafe and the rustonimicon to help me. You can do anything in rust you can do in c++, it is just that rust defaults to safe instead of unsafe, and there is no single keyword to let you know the c++ you are looking at is safe.
Something that Rust got _really_ right:
 Editions. And not just that they exist, but that they are specified per module, and you can mix and match modules with different Editions within a bigger project. This lets a language make backwards incompatible changes, and projects can adopt the new features piecemeal.If such a thing came to C++, there would obviously be limitations around module boundaries, when different modules used a different Edition. But perhaps this could be a way forward that could allow both camps to have their cake and eat it too.Imagine a world where the main difference between Python 2 and 3 was the frontend syntax parser, and each module could specifically which syntax ("Edition") it used...
But Edition can exist only because Rust intrinsically has the concept of package, which naturally defines the boundary. C++ has nothing. How do you denote a.cpp be of cpp_2017 edition which b.cpp be cpp_2026? Some per-file comment line at top of each file?C++ is a mess in that it has too much historic baggage while trying to adapt to a fiercely changing landscape. Like the article says, it has to make drastic changes to keep up, but such changes will probably kill 80% of its target audiences. I think putting C++ in maintenance mode and keep it as a "legacy" language is the way to go. It is time to either switch to Rust, or pick one of its successor languages and put effort into it.
Rust doesn't have the concept of package. (Cargo does, but Cargo is a different thing from Rust, and it's entirely possible to use Rust without Cargo).Rust has the concept of _crate_, which is very close to the concept of compilation unit in C++. You build a crate by invoking `rustc` with a particular set of arguments, just as you build a compilation unit by invoking `g++` or `clang++` with a particular set of arguments.One of these arguments defines the edition, for Rust, just like it could for C++.
That only works for C++ code using C++20 modules (i.e. for approximately nothing).
With textual includes, you need to be able to switch back and forth the edition within a single compilation unit.
It's not clear that modules alone will solve One Definition Rule issues that you're describing. It's actually more likely that programs will have different object files building against different Built Module Interfaces for the same module interface. Especially for widely used modules like the standard std one.But! We'll be able to see all the extra parsing happen so in theory you could track down the incompatibilities and do something about them.
Modules are starting to come out.  They have some growing pains, but they are now ready for early adopters and are looking like they will be good.  I'm still in wait and see mode (I'm not an early adopter), but so far everything just looks like growing pains that will be solved and then they will take off.
At the current rate, we'll have full module support for all of the most popular C++ libraries sometime around Apr 7th, 2618.https://arewemodulesyet.org/
I expect modules to follow a S curve of growth.  Starting in about 2 years projects will start to adopt in mass, and over the next 5-10 years there will be fast growth and then (in about 12 years!) on a few stragglers will not use modules.  They are not free to adopt but there appear to be a lot of long term savings from paying the price.
I'll mention that library maintainers/authors can't even _consider_ modules unless they set C++20 as a requirement. Many/most popular libraries will not do that anytime soon. I maintain a moderately-popular library and my requirement is C++11... now, to be fair, I started it back in 2016-2017; but still, I won't even consider requiring C++20 until C++17-and-earlier application code is close to disappearing.
Mixing editions in a file happens in Rust with the macro system. You write a macro to generate code in your edition and the generation happens in the callers crate, no matter what edition it is.
> I think putting C++ in maintenance mode and keep it as a "legacy" language is the way to goI agree but also understand this is absolutely wishful thinking. There is so much inertia and natural resistance to change that C++ will be around for the next century barring nuclear armageddon.
I don't think even that would suffice. :)
COBOL's most recent standard was released in 2023, which rather ruins your point.
I think the existence of COBOL-2023 actually suggests that it's not merely possible that in effect C++ 26 is the last C++ but that maybe C++ 17 was (in the same sense) already the last C++ and we just didn't know it.After all doubtless COBOL's proponents did not regard COBOL-85 as the last COBOL - from their point of view COBOL-2002 was just a somewhat delayed further revision of the language that people had previously overlooked, surely now things were back on track. But in practice yeah, by the time of COBOL-2002 that's a dead language.
Fully agree, because for the use cases of being a safer C, and keeping stuff like LLVM and GCC running, that is already good enough.From my point of view C++26 is going to be the last one that actually matters, because too many are looking forward to whatever reflection support it can provide, otherwise that would be C++23.There is also the whole issue that past C++17, all compilers seem like a swiss cheese in language support for the two following language revisions.
You can interoperate via C ABI and just not use the C++ standard types across modules - which is the sane thing to do.  Every other language that supports FFI via C linkage does this, only C++ insists on this craziness.
Also I wouldn't start by rewriting the thing that calls do_something, I'd start by rewriting do_something. Calling into rust from c++ using something like zngur lets you define rust types in c++ and then call idiomatic rust. You can't do it in the opposite direction because you cannot safely represent all c++ types in rust, because some of them aren't safe.
I have millions of lines of C++.  do_something exists and is used but a lot of those lines and works well. I have a new feature that needs to call do_something.  I'm not rewriting any code.  My current code base was a rewrite of previous code into C++ started before rust existed), and it costs a nearly a billion dollars! I cannot go to my bosses and say that expensive rewrite that is only now starting to pay off because of how much better our code is needs to be scrapped.  Maybe in 20 years we can ask for another billion (adjust for inflation) to rewrite again, but today either I write C++, or I interoperate with existing C++ with minimal effort.I'm working on interoperation with existing C++. It is a hard problem and so far every answer I've found means all of our new features still needs to be written in C++ but now I'm putting in a framework where that code could be used by non-C++.  I hope in 5 years that framework is in place by enough that early adopters can write something other than C++ - only time will tell though.
Yeah that use case is harder, but I'm involved in a similar one. Our approach is to split off new work as a separate process when possible and do it entirely in rust. You can call into c++ from rust, it just means more unsafe code in rust wrapping the c++ that has to change when you or your great grandchild finally do get around to writing do_something in rust. I am super aware of how daunting it is, especially if your customer base isn't advocating for the switch. Which most don't care until they get pwned and then they come with lawyers. Autocxx has proven a painful way to go. The chrome team has had some input to stuff and seem to be making it better.
Sure I can do that - but my example C++ function is fully memory safe (other than don't go off the end of the vector which static rules can enforce by banning []). If I make a C wrapper I just lost all the memory safety and now I'm at higher risk.  Plus the effort to build that wrapper is not zero (though there are some generators that help)
How about going off the end of the vector with an iterator, or modifying the vector while iterating it, or adding to the vector from two different threads or reading from one thread while another is modifying it or [...].There is nothing memory safe whatsoever about std::vector<something> and std::string. Sure, they give you access to their allocated length, so they're better than something[] and char* (which often also know the size of their allocations, but refuse to tell you).
> rust will just say noThis is just not accurate, you can use atomic data types, Mutex<> or RwLock<> to ensure thread-safe access. (Or write your own concurrent data structures, and mark them safe for access from a different thread.)  C++ has equivalent solutions but doesn't check that you're doing the right thing.
Only if using a hardened runtime with bounds checking enabled, without any calls to c_str().
> And not just that they exist, but that they are specified per moduleNitpick: editions are specified per crate, not per module.---Also note that editions allow to make mostly syntactic changes (add/remove syntax or change the meaning of existing ones), however it is greatly limited in what can be changed in the standard library because ultimately that is a crate dependency shared by all other crates.
My C++ knowledge is pretty weak in this regard but couldn't you link different compilation units together just like you link shared libraries? I mean it sounds like a nightmare from a layout-my-code perspective, but dumb analogy: foo/a/* is compiled as C++11 code and foo/b/ is compiled as C++20 code and foo/bin/ uses both? (Not fun to use.. but possible?)Is that an ABI thing? I thought all versions up to and including C++23 were ABI compatible.
How does foo/bin use both when foo/a/* and foo/b/ use ABI-incompatible versions of stdlib types, perhaps in their public interfaces?  This can easily lead to breakage in interop across foo/a/* and foo/b/ .
> ABI-incompatible versions of stdlib typeslibc++ and glibc++ both break ABI less frequently than new versions of C++ come out.  As far as I'm aware, libc++ has never released a breaking change to its ABI.
How does Rust do it?
There’s only ever one instance of the standard library when a program is compiled, so an and b cannot depend on different versions of it.For normal libraries, an and b could depend on different versions, so this could be a problem. The name mangling scheme allows for a “disambiguator” to differentiate the two, I believe that the version is used here but the documentation for it does not say if there’s more than that.
By linking both and not allowing mixing types, i.e. it considers types from a totally unrelated with types from b.Also, Rust compiles the whole world at once, so any ABI breakage from mixing code from different compiler versions doesn't happen. (Editions are different thing from compiler versions, a single version of the compiler supports multiple editions.)
Rust does not compile the whole world at once. Each crate is compiled separately, and then they’re all linked together at the end.
Yeah, I know, but I mean that it's normal to link together crates compiled with the same compiler, unlike with C, where ABIs are stabler and binary dependencies are more common.
Ah, that is very true, yeah.
What is the point?  C++ is mostly ABI compatible (std::string broke between C++98 and C++11 in GNU - but we can ignore something from 13 years ago).  The is very little valid C++11 code that won't build as C++23 without changes  (I can't think of anything, but if something exists it is probably something really bad where in C++11 you shouldn't have done that).Now there is the possibility that someone could come up with a new breaking syntax and want a C++26 marker.  However nobody really wants that.  In part because C++98 code rebuilt as C++11 often saw a significant runtime improvement.  Even today C code built as C++23 probably runs faster than when compiled as C (the exceptions are rare - generally either the code doesn't compile as C++, or it compiles but runs wrong)
There are plenty of things between C++11 and C++23 that have been removed and hence won't compile:Implicit capture of this in lambdas by copy.std::iterator removed.std::uncaught_exception() removed.throw () exception specification removed.std::strstream, std::istrstream, and std::ostrstream removed.std::random_shuffle removed.std::mem_fun and std::mem_fun_ref, std::bind1st and std::bind2nd removed.There are numerous other things as well, but this is just off the top of my head.
None of those things I've never used.  Many of those are in bad practice for C++11.
Sure. But per your own other posts in this thread, you've got > 10 million lines of "legacy C++". Probably those bad practices are present in that code and not automatically fixable. So switching to compiling everything with a C++23 compiler is every bit as much not an option for you as switching to Rust, no?
If I turn on C++23 and get a handful of errors over those million lines of code I will spend the week or two needed to rewrite just those areas of code. That is much easier than rewriting everything from scratch in rust. Even if we just wrap all needed C++ APIs in C so we can use rust that is a lot of effort before all our frameworks have the needed interfaces (this is however my current plan - it will just be a few years before I have enough wrappers in place that someone can think about Rust!)Note too that we are reasonably good (some of us are experts) at C++ and not Rust. Like any other human when we first do Rust we will make a mess because we are trying to do things like C++ - I expect to see too much unsafe just to shut up Rust about things that work in C++ instead of figuring out how to do it in safe rust.  (there will also be places where unsafe is really needed)  I want to start slow with Rust so that as we learn what works we don't have too much awful code.
> Like any other human when we first do Rust we will make a mess because we are trying to do things like C++ - I expect to see too much unsafe just to shut up Rust about things that work in C++C++ has its own Core Guidelines that are pretty rusty already (to be fair, in more ways than one).  There's just no automated compiler enforcement of them.
cppreference say  strstream is removed in C++26, not C++23.I knew they are bad, but I don't think it should be removed.
std::shared_ptr::unique()
There is no inherent point, I was just wondering, if it's possible, why people don't use such a homegrown module layout like Rust editions in C++.I only ever worked in a couple of codebases where we had one standard for everything that was compiled and I suppose that's what 90% of people do, or link static libs, or shared libs, so externalize at an earlier step.So purely a thought experiment.
There was a similar proposal for C++, using rust’s original names: epochs. It stalled out.
They should call them 'eras'.  Then they can explain that epochs did not lead to a new era in the language, but eras will mark the next epoch of C++.
The C++ profiles proposal is something like an "editions lite". It could evolve into more fully featured editions some day, though not without some significant tooling work to support prevention of severe memory and type safety issues across different projects linked into the same program.
There will be eventually only one faction left using C++ — the legacy too-big-to-refactor one.The other faction that has lost faith in WG21, and wants newer, safer, nimble language with powerful tooling is already heading for the exits.Herb has even directly said that adding lifetime annotations to C++ would create "an off-ramp from C++"[1] to the other languages — and he's right, painful C++ interop is the primary thing slowing down adoption of Rust for new code in mixed codebases.[1]:https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p34...
> newer, safer, nimble"newer" is hopefully a non-goal.Unfortunately, an option that is both safer and nimble doesn't appear to exist. I'm still hopeful, but at the moment it looks like rust is our future. A fate only marginally better than C++.
Everything out there is nimbler than C++. So you only have to select for safer to get those, and anything with managed memory and Rust are safer. (Not an exclusive set, but you'll need to actually evaluate other options.)
Oh no! Herb Sutter is leaving Microsoft?!That does not bode well for Microsoft. At least from the outside perspective it looks like he was the adult in the room, the driving force behind standards adoption and even trying to steer C++-the-language towards a better vision of the future.If he is gone, MSVC will again be the unloved bastard child it has long been before Herb's efforts started to pay off. This is very disheartening news.I'm happy he held out for this long even though he was being stonewalled every step of the way, like when Microsoft proposed std::span and it was adopted but minus the range checking (which was the whole point of std::span).Now he has been pushing for a C++ preprocessor. Consider how desperate you have to be to even consider that as a potential solution for naysayers blocking your every move.
The rumor that has been widely circulating is that the MSVC backend is being reused as a code generator for the Rust compiler (because nobody really understands PDBs anymore, not even Microsoft, and especially LLVM doesn't. So rustc could be a MSVC frontend instead to reuse all the existing arcane logic.)MSVC will continue to be used for many years, and especially the backend might see renewed effort. But I don't know about the C++ frontend specifically, I've seen complaints about more and more bugs on the cpp subreddit. It's possible MS will be investing a little less in C++.
Disregarding the rumor,  it is quite public information that on Azure side, C and C++ are now only allowed for existing code bases, or scenarios where nothing else is available.Meanwhile on Windows side, it was made officially at Ignite that a similar decision is now to be followed upon Windows as well.Here the official stuff, so whatever happens to MSVC is secondary,https://azure.microsoft.com/fr-fr/blog/microsoft-azure-secur...https://blogs.windows.com/windowsexperience/2024/11/19/windo...
>  in alignment with the Secure Future Initiative, we are adopting safer programming languages, gradually moving functionality from C++ implementation to Rust.This seems like one hell of an initiative for the Windows OS. That is millions of lines of C++ code, often with parts from waaay back. A friend who works on one of the OS teams told me that his team got a boomerang hire that worked on Windows back in the 90s and he was still finding parts of his code in there!I hope this corporate interest bodes well for Rust though. It seems like for C++ it really caused a schism over the ABI break issue where Chandler et al were basically rebuffed finding some timeline to break it, and then Google dropped all their support on the committee in favor of Carbon, Rust, etc.
Apple and Google focusing on their own stuff, is one of the reasons why clang lost velocity in ISO C++ adoption, most of the C++ compiler vendors that fork clang don't contribute frontend stuff only LLVM, and with them out, it took some time until new folks jumped in to replace their contributions.Likewise you will noticed MSVC is no longer riding the wave in regards to C++23, after being the first to fully support C++20.Then there are all those other compilers out there, lost somewhere between C++14 and C++17, and most likely never moving beyond that.
They've made statements like that for a long time now. But they've never escaped using C++ when performance matters. The game dev roles very clearly ask for C++, for example.Rather, it seems that as computers have gotten faster, there's been more places where safety is preferable to performance.
The proof is on the pudding, how performance critical do you consider Pluton firmware, network cards firmware supporting Azure workloads?Two examples of stuff publicly rewritten into Rust.Games are special that isn't what Windows security cares about in first instance, when TinyGlade is the first ever commercial success using Rust.Yet most games are done with Unreal and Unity, and yes there is lots of C++ there, but is mostly Blueprints, Verse, C# on top, that large majority of studios reach for.
I have no magic window into Microsoft, but they've been saying they need to stop writing C++ for genuine decades now, and it's still prominent on their jobs site with new projects.I'm aware they're trying, but I just don't believe their statements from the evidence available.
> Now he has been pushing for a C++ preprocessor.He has been showing it, but not pushing it.  the difference is subtle but important. He is showing a lot of "what ifs" trying them, and pushing the useful ones back into the language.  Reflection is on track for C++26 in large part because he inspired a lot of people with his metaclasses talk (a long time ago, but doing things right takes time)
Wait, why does std::span not do the range checking? We ran into that exact thing at work and were really confused why the hell it doesn't do it currently.
I believe this is because [] doesn’t do checking normally, so this is seen as a consistency. I am not 100% sure, but I do remember it being a contentious decision..at() is added in C++26.
It looks like he's staying on the committee and what not, just changing his day job.  That's actually one of the benefits of having a committee & iso standardization process -- things aren't so reliant on a single engineer staying employed at a single company.I'm sure it's never as clean a situation as anyone would like, but hey, world is a rough place sometimes.
Python similarly has 2-3 factions in my experience: teams doing engineering in Python and using all the modern tooling, linting, packaging, types, testing, etc; teams doing data science and using modern but different tooling (i.e. Anaconda); and teams that don't get onboard in any of the language health initiatives and are on unsupported language versions with no packaging, tooling, linting, etc.Javascript/Node/Typescript has even more identifiable factions.I think developing factions around these things is unfortunately normal as languages grow up and get used in different ways. Rust has arguably tried to stay away from this, but the flip side is a higher learning curve because it just doesn't let certain factions exist. Go is probably the best attempt to prevent factions and gain wide adoption, but even then the generics crowd forced the language to adopt them.
When you put it this way, personas might be a better term than factions.
The first two factions you describe in Python (types, testing etc. vs. data science and Anaconda) can work together just fine.Source: I am in both factions, as are my colleagues :)
I have been saying this for a more than a decade, but the number one thing that killed C++ as an attractive modern language is (the lack of) modules - the ability to include C++ code  and libraries from others (perhaps with transitive dependencies), would allow an actual community of devs and companies spring up around the language.Instead we have greybeards and lone warriors, and million-line legacy codebases, half of which have their own idea on what a string or a thread is.
” killed C++ as an attractive modern language”I’m not sure were you got this perception that it’s dead.C++ remains the only game in town in many domains.That said, _unless you work in those domains_ there is no good reason to use C++ IMHO.Apart from the legacy codebases, there’s lots of C++ greenfield development.” the ability to include C++ code and libraries from others ”Libraries in vcpkg - a large number - are compatible enough to be used in this sense. It’s possible your specific domain is lacking contributions or you’ve been looking from the wrong places?
> I’m not sure were you got this perception that it’s dead.
> C++ remains the only game in town in many domains.Yeah but not because people like the language but because there is no alternative.At work almost every one of us devs doesn't enjoy working with C++ but since many dependencies in the embedded space are written in C++ you don't have much of a saying which language you choose. For example, Qt supports basically only C++.Rust is the only mature contender in that space currently.
Honestly not sure what you mean by that - in C#, for example it doesn't matter to the compiler where the code comes from, it can be JITed/inlined just the same even if its coming from a different dll.I haven't seen any perf impact of splitting stuff between files/js modules in typescript either.What I'm guessing is that you mean that static compilers, like that of C++, need to be able to 'see' large amounts of code to make clever inlining optimizations.Which shouldn't be the case if the code is well designed, and/or the compiler can prove invariants necessary for optimization without having to look at the body of the code.
Modules could be better, the problem are some greybeards and lone warriors (thankfully not all), that insist using C++ as it was plain old C.Basically, it is no different than renaming .js to .ts to take advantage of some stuff in Visual Studio Code, while keep writing plain old JavaScript.
I have seen alotof C++ code that has a lot of "this is clearly just C" in it. None of it is because of "greybeards and lone warriors". All of it was because it started as a C codebase, and sometime in the mid to late 90s when object-oriented fever swept the world they started just adding C++ on top of the existing C codebase.Given that the general industry approach to technical debt is "yes, more please", it is unsurprising to me that any sufficiently old C++ project still has lots and lots of plain C inside it.
There's also a ton of landmark tutorials out there which taught generations of coders how to write C++ before "modern C++", and are still the top tutorials you find when you look them up. Plus I see C libraries more commonly than C++, so you end up dealing with raw pointers despite your better efforts
Except the complaint equally applies to green field projects.
I think the struggle with modules has much more to do with the complexity of the problem at hand. I think the solution looks very easy should one be willing to dispense with large parts of the ecosystem. But if your goal is to keep the ecosystem together and not break the world (ala python 2/3 or perl5/6) and solve the problem at hand (waves vaguely at modules) - then its a really hard problem.I wish I could say modules don't work, but I have yet to understand them. Which is probably a big part of its problem.
All my hobby coding in C++ makes use of modules.Visual C++ and clang, alongside MSBuild and CMake/ninja.As for ecosystem fragmentation, it has been the same old story since WG14 and WG21 exist, each compiler and platform is their own snowflake of what they actually support.
> All my hobby coding in C++ makes use of modules.Do you have an example (of yours or others) that you could link?I've been trying to get this up and running myself, but can't seem to whisper the right CMake prayers.
Sure,https://github.com/pjmlp/RaytracingWeekend-CPP
Python3 is a great example. They looked at what others had done. They carefully thought about the problem. They build tools to migrate. They announced plans.  They really thought they had found a better answer that would work out because they had planned for everything.Of course we are now looking at things in hindsight and see what didn't work.
It's really important to be clear about the lessons to be learned from Python 3.1. Forward compatibility is more important than backward compatibility.
2. Automated refactoring tools don't help with 1.The problem wasn't that they broke a lot in Python 3. It was that you couldn't write your Python 2 in such a way as to be compatible with it until well into the transition process as the six package got popular and the devs fixed needlessly broken things in Python 2.
I would suggest languages facing the same problem learn from Perl 5's success, rather than the various failures of Python 3.Every† minor point release of Perl 5 creates backward-incompatible changes. These can be opted into individually, or on a per-file basis by simply specifying the version of Perl used.  It all works.  Differently-versioned Perl code can call each other as much as it likes.There was never any reason why Python 3 needed to be anything other than Python 2 with this at the top of the file:use Python 3.nFor various values of `n`.  Perhaps when enough time passes, that's just Python now, and you have to copy-paste this into all the legacy code:use Python 2That's it.  Any language can do this, they just have to decide not to make life hard for everyone.[†] Perhaps not literally every, but it may as well be.
There are many options with pros and cons.  Python was aware of them.  They made what looked like a reasonable decision to take a different paths.  On hindsight we know of the problems but you could not predict them with confidence in advance. (some may have predicted it but they would admit to guessing if they are honest)
Sure, not interested in changing the past, for many reasons not least of which is that it appears to be impossible.  The Python team surely did not go in to Python 3 blindly, but they botched it anyway.What I mean to say is that Python as a negative example only goes so far, because an example of failure isn't a template for success.  So "don't do what Python did" carries limited value for a language looking to make breaking changes.  For a language looking to make a major point release, that's the future, and the future can be changed; this is what I'm interested in here."Do what Perl 5 did" (anddo notdo what Perl 6 did, up to the point it got renamed) is a great place to start, however, because it worked, works, is working.  Languages are different enough that it isn't a completely transferable experience, but there's a lot to learn there.
One thing I cannot stand about C++ is the fractured nature ofeverything. Compilers, build tools, package management, etc... It feels like you need to be a wizard just to get a project compiling and start writing some code.
The worst part is when you want to bring along people that are not as much of a wizard as you are. I've been prototyping some multiplayer, online video game with MMO-like sharding for a while now, mostly the backend and core stuff for the project and wanted to get two of my friends on the project to develop the gameplay logic which is largely done through a dynamic scripting language, but some features (that, say, I did not foresee needed yet), require source changes to expose the APIs to the scripting language, now, these guys are capable of doing these changes but the onboarding process for a single potential co-developer is such a pain, I basically have to explain to them how to download a compiler, a package manager like vcpkg (which wasn't even that much usable for these types of things pre-versioning, and is still not working properly - i.e. trying to pin LuaJIT version to 2.0.5 for VM bytecode compatibility will attempt to build LuaJIT with cl.exe on Linux), a build system like CMake, and so on, then guide them through all the steps to get the compiler, the build system, and the libraries working, and then hope that in the end they will actually work and not force you to spend an entire day over a remote desktop software trying to get them to become productive.
Include more of your dependencies in the repo and build them aa part of the ordinary build process. Now a package manager does not need to get involved.
Manually copy-pasting source trees around sounds like such an outdated idea from decades ago on how to approach dependency management in a modern programming language. Not to mention that you then have to hook them up to the build system that you are using and not all of them will work out of the box with the one you are using for your project, sure, if you are using CMake and your dependency uses CMake, you can add a subproject, how do you deal with it when they're mixed-and-matched aside from rewriting the builds for every dependency you're pulling in; or without manually writing glue shell scripts to build them independently and put them into a directory? How do you then ensure the said shell script works across different platforms? There are way too many issues with that approach that are solved in other languages through a standardized project management tool.
You don't have to actually copypaste. You can use CMake and FetchContent/CPM. You can specify custom build commands or inline declare a project for anything small that you pull in that doesn't use CMake (you can call add_library with a glob on the folder FetchContent pulled in, for example - I've done so herehttps://github.com/basis-robotics/basis/blob/main/cpp/CMakeL...for a header only lib). For large external dependencies that are either very slow to compile or for some reason aren't CMake, reach for the system package manager or similar. If you want to be really cross platform and are supporting Qt/wxwidgets/etc, vcpkg+CMake+clang is solid combo, if a bit slow and a bit disk space heavy with the build cache.
And yet that's the right approach. It's not really copying but rather onboarding.You don't want to depend on a third-party hosting the code, so you need to copy it, and pin it to a specific version. You might also need to patch it since you'll be using it and most likely will run into problems with it.Using third-party code means taking ownership of all the problems one might encounter when trying to use it in your project, so you might as well just adapt it for it to work with your tools and processes.If you use a modular system this is essentially just maintaining a fork.
Have you taken a look at CPM ?https://github.com/cpm-cmake/CPM.cmake. It makes CMake project management easy - no need for separate package manager tool.
People vendor dependencies in Go too!
That’s similar to what vcpkg does under the covers. It clones the repo containing the dependency’s source code and then compiles it using the same compiler as the rest of your project. This avoids static libraries and ABI considerations while also avoiding having to copy/paste their entire source tree into your own.
Can't you just put that into a docker container?
This is more of a workaround than a solution; see my other comment in this thread.
you DO need to be a wizard to launch a large C++ project.Yes, languages that are beginner friendly are ... friendlier. Yes, languages that stick to one or a small number of programming paradigms are friendlier. But if you want the "flexible efficiency and raw power of C" and "something higher level than C", C++ is your baby.Maybe it would be better if we all used Java, Rust, and Go, but C++ sings its siren von Neumann song to the wizards, and there will always be wizard musicologists who steer their projects toward those rocks and, when they have just enough wax in their ears, they sail right past the rocks and come out the other side of the straits leading the rest of the fleet.You can choose to follow them or not, for there's no shame in coming in 4th.
Even the wizards are moving to Rust/Zig since C++ stdlib performance is becoming terrible thanks to the ABI-frozen till heat-death of the universe decision. Even wizards don't want to build a stdlib of their own from scratch.Feel the committee was smoking weed that day in la-la land. You can ignore all the safety stuff from Sean Baxter, but saying no to performance on the altar of permanent, un-specified ABI backward compatibility - when such was never mentioned as a design goal of C++ -  means its "Goodbye C++" for a long, long list of orgs and "wizards". The ABI was NEVER specified formally by the C++ standard - so why bother sacrificing the world for its immortal existence ?C++ is NO longer the choice of language for greenfield native projects and the committee takes the full blame.
Really looking forward to zig 1.0. I feel like C++ has become a language where professionals are fine with the historical grime but for hobbyist and people that need C++ occasionally there is just no motivation in the community to make this language more ergonomic.
ABI compatibility is one of those things that is necessary with such a long history, especially with commercial libraries that don't really have an equivalent in the newer languages. The issue with C++ that doesn't exist with it's competitors is that there is a long tail of software people use commercially that isn't source available that's incredibly important in certain use cases.I worked in a previous role on C++ CAD/simulation software that required vendored things like solid modelling kernels and it was incredibly painful. Occasionally one of the vendors would just not do the work and you'd end up having to spend half a year ripping out the dependency that worked perfectly well. The team working on the software were generally in favour of moving up through to modern standards, while I was there we did 03 -> 17 for e.g. but that didn't finish til 4 years after the C++17 standard came out for all sorts of reasons. When VS2017 came out everyone breathed a sigh of relief because suddenly we didn't have to wait to upgrade the compiler.
So here's the thing. Almost none of the problems I have with C++ are related to "flexible efficiency and raw power of C". You could easily have language that is even more flexible and powerful, but much easier to use. Or not even use, just install.C++ was always by far the most inefficient langauge to work with for me, because there's just so much chore and nonsense that you have to get through to get anything done, and almost none of it has any reasonable purpose, there's no efficency tradeoff. I'm pretty sure that the insane build situation or UB in uninitialized variables or unspecified argument evaluation order never really benefited anybody, they are just bad decisions in the language, and that's all.
> UB in uninitialized variablesYou will be happy to learn the uninitialized variables are not UB as of C++26.
They're just initialized to some unspecified value and cause almost-as-hard-to-diagnose faults.
the unspecified value is supposed to be something really obviously wrong (in particular sanitizers will look for it.) and so be easier to diagnose with tools.  Unlike 0 which is really hard to diagnose because often it is the right value and so when it isn't you won't realize what you did wrong.
So much for the theory, then there is the hard reality how standard library is implemented, the variantions across implementations, and how the ongoing ABI drama is preventing any performance improvements to it.
> but C++ sings its siren von Neumann song to the wizards, and there will always be wizard musicologists who steer their projects toward those rocks and, when they have just enough wax in their ears, they sail right past the rocks and come out the other side of the straits leading the rest of the fleet.beautiful, in equal parts true, sad, and endearing.but also remember the vasa.
I really really like this article. I think the two camps the author describes very much reflect my experience over the past couple of decades at a dotcom startup, then a game developer, and now at GoogleHowever, I think the author is a little off on the root cause. They emphasize tooling: the ability to build reliably and cleanly from source. That's a piece of it, but a relatively small piece.I think the real distinguishing factor between the two camps isautomated testing. The author mentions testing a couple of times, but I want to emphasize how critical that is.If you don't have a comprehensive set of test suites that you are willing to rely on when making code changes, then your source code is a black box. It doesn't matter if you have the world's greatest automated refactoring tools that output the most beautiful looking code changes. If you don't have automated tests to validate that the change doesn't break an app and cost the company money, you won't be able to land it.Working on a "legacy C++ app" (like, for example, Madden NFL back when I was at EA) was like working on a giant black box. You could fairly confidently add new features and new code onto the side. But if you wanted to touch existing code, you needed a very compelling reason to do so in order to outweigh the risk of breaking something unexpectedly. Without automated tests, there was simply no reliable way to determine if a change caused a regression.And, because C++ is C++, even entirely harmless seeming code changes can cause regressions. Once you've got things like reinterpret_cast<>, damn near any change can break damn near anything else.So people working in these codebases behave sort of like surgeons with a "do no harm" philosophy. They touch as little as possible, as non-invasively as possible. Otherwise, the risk of harming the patient is too high.It's a miserable way to program long-term. But it'sreallyhard to get out of that mess once you're in it. It takes a monumental amount of political capital from engineering leadership to build a strong testing culture, re-architect a codebase to be testable, and write all the tests.A lot of C++ committee changes aimed at legacy C++ developers are about "how can we help these people that are already in a mess survive?" That's averydifferent problem than asking, "Given a healthy, tested codebase, how can we make developers working in it go faster?"
> A lot of C++ committee changes aimed at legacy C++ developers are about "how can we help these people that are already in a mess survive?" That's a very different problem than asking, "Given a healthy, tested codebase, how can we make developers working in it go faster?"Having also worked at a few gamedev studios, IME there isn't a real distinction between the two since it isalwaysa matter of time for the former to become the latter.Sometimes it doesn't even take that long, all it takes is a single innocuous vertical slice with a pointlessly immovable deadline to inject enough harm in a codebase so you spend the next year fighting bugs that shouldn't have existed in the first place while also having to do everything else at the same time (and all planned timeframes made with only the "everything else" in mind, of course).IMO even if it doesn't sound good, it is much more practical to learn how to deal with the mud than assume pigs do not exist :-P
> Having also worked at a few gamedev studios, IME there isn't a real distinction between the two since it is always a matter of time for the former to become the latter.That was very much my experience at EA, but has definitely not been my experience at Google. While everyone struggles with tech debt, at Google I've worked in many codebases that have been continuously well-maintained with good test coverage for over a decade.Really, once you build a culture that says, "People not on your team may edit your code without asking and will rely on your tests to make sure they don't break things,", teams gethighlyincentivized to write tests.
Agreed. As much as I want it to be simpler to build C++ programs from source, it's pretty much always _possible_ in my experience, it can just a PITA frequently.I think that tests are a sure-fire way to improve the quality of your code, but I'd throw another piece in to the ring: sanitizers [0]. Projects that have good tests and run them regularly with TSan/ASan/UBSan in my experience are much better to work on because it means that it's much less likely there's deep seeded issues that are lurking. It gives you increased confidence that you're not introducing hard-to-detect issues as you go.These tools aren't just exclusive to C++. I've said the same thing about C, Go, Zig, Odin, etc. Projects that use them (and have good automated tests) tend to be in good shape, and projects that don't tend to take a long time to make any progress on.[0]https://github.com/google/sanitizers
If you're comparing Herb Sutter and the Google people at the standard committee, there is one thing that was clear: Herb was good at getting people to agree on compromises that served everybody, while Google was mostly claiming they knew better than everybody else and pushing their own agenda.
Profiles aren't a mess because they're intended for legacy codebases instead of big tech monorepos. They're a mess because they're not a serious effort. There's no actual vision of what problems they're trying to solve or what the use cases are, or even what kind of guarantee profiles are going to make.
“We must minimize the need to change existing code. For adoption in existing code, decades of experience has consistently shown that most customers with large code bases cannot and will not change even 1% of their lines of code in order to satisfy strictness rules, not even for safety reasons unless regulatory requirements compel them to do so.” – Herb Sutter

  with large code bases cannot and will not change even 1% of their lines of code in order to satisfy strictness rulesDo people really say this? Voice this in committee?  I have been in a few companies, and one fairly large one, and all are happy to and looking forward to upgrade newer standards and already spend a lot of time updating their build systems. Changing 1% of code on top of that is probably not really that much compared
>Changing 1% of code on top of that is probably not really that much comparedQuite a few companies have millions and millions of lines of code. Changing 1% of it would mean changing more than 10K lines of code, perhaps even more than 100K. In much bigger code bases, where changing anything has a risk of breaking something — not just because you might make a mistake, but because your program is full of Undefined Behaviour, and changing anything might manifest latent bugs.Given that, I'm not surprised people say that Sutter quote with a straight face.
Many of my customers are in an industry with a huge C++ code base and it's all under active development. Safety certification requirements are onerous and lead-times for development are long: many are now experimenting with C++17 and C++20 is on the long-term horizon but not yet a requirement. Because of the safety certification requirements and the fact that the expected lifecycle of the software is the order of decades after their products have been released, changing any lines of their code for any reason is always risky. Lives can be at stake.But this is a multi-billion-dollar industry. If you're working on scripting a little browser "app" for a phone things may be different.
“Little browser apps for phones” are a trillion-dollar industry
Hundreds of hours of manual testing. I don't have to do safety certificates, but my code gets 500 hours of manual testing (I'm not allowed to give real numbers, these numbers are close enough) - they find enough critical can't ship issues where the fix is risky enough to start all over that we typically are doing 2500 hours of manual testing. on every release.We have a large automated test suite that runs on every build and takes hours.  The problem with automated tests is they only verify situations you thought of work the way you think they should, while human testers find slight variations of setup that you wouldn't think matter until they do. Human tests also find cases where the way you expect things to work don't make sense in the real world.
Wait until you find out about the cat test. It found a failure mode no human had thought of. No amount of the developer claiming a test like that was not fair was enough to invalidate the results. No actual cats were harmed but treats may have been given.
Do you have more context? I'm having trouble googling what you're referencing.
Simulate a cat walking on the keyboard to handle weird inputs?
Isn't that just fuzzing? I thought maybe there was a specific thing called the cat test.
People just don't make mass changes to existing working code.  Mostly they cannot.  Even if the tooling was available, which it's not, it's also about reeducating their developers, who don't want to or can't change. Plus it'd have to be recertified. It's all cost with no benefit.Except, allegedly, at Google.  But is there any evidence they actually do this, eg. in public code bases?  Or is it just hype?
Google do this to their internal monorepo.This is one of the reason why they are bad at open sourcing - their internal code almost never match what is released
Could be selection bias. Companies (or departments within companies) who are still actively developing their C++ code probably tend to hire more developers and consultants than companies who are doing minimal maintenance on their code base, and that might correlate well with the “two factions of C++” discussed here.“Our code is an asset” ⇒ code kept up-to-date“Our code is a burden, but we need it” ⇒ change averse
> Changing 1% of code on top of that is probably not really that much comparedChanging 1% across all modules is a nightmare. Changing one module which is 1% of the code is nothing.
A company that I worked at had a few very large C++ related migrations, and they were all very very expensive.The first was removing `long` from the code, since a lot of code assumed its size (is it like `int` or like `long long int`?) and as machines were upgraded it caused problems.The second was moving to C++11/14/17. Most of the difficulty was toolchains on unixen that did not support the new versions of the language, or for which support was incomplete, or for which upgrading to a version with support broke existing builds.The third was moving to Linux from big iron unixen. As far as I understand, this initiative is still underway. It was already underway in 2011 when I joined the company.This is a rich company with a large, healthy engineering department. I imagine that most other companies would not or could not bother.
That old joke about Stroustrup inventing C++ to keep developers perpetually employed keeps ringing true.
C++ is not just C++ but also the C preprocessor, the STL, the linker, the C libraries and SDKs you can't help but depend on, the build system, the build scripts, the package manager, the IDEs and IDE add-ons, the various quirks on various platforms, etc.  That's on top of knowing the code base of your application.Being really good at C++ almost demands that you surrender entire lobes of your brain to mastering the language.  It is too demanding and too dehumanizing.  Developers need a language and a complete tool chain that is designed as a cohesive whole, with as little implicit behavior, special cases and clever tricks as possible.  Simple and straight-forward.  Performance tweaks, memory optimizations and anything else that is not straightforward should be done exclusively by the compiler.  I.E. we should be leveraging computers to do what they do best, freeing our attention so we can focus on the next nifty feature we're adding.Zig is trying to do much of this, and it is a huge undertaking.  I think an even bigger undertaking than what Zig is attempting is needed.  The new "language" would also include a sophisticated IDE/compiler/static-analyzer/AI-advisor/Unit-Test-Generator that could detect and block the vast majority of memory safety errors, data races and other difficult bugs, and reveal such issues as the code is being written.  The tool chain would be sophisticated enough to handle the cognitive load rather than force the developer to bear that burden.
The "new" language you're describing sounds like Swift.
> Nimble, modern, highly capable tech corporations that understand that their code is an asset. (This isn’t strictly big tech. Any sane greenfield C++ startup will also fall into this category.)Oh I see, this is a fantasy.
Keyword is "sane". You can probably count all "sane greenfield C++ startups" on one hand.
It's also just plain wrong. Even the cleanest most beautiful and efficient code is a liability. You sell software, not code.It's all about the magnitude of the liability, not the direction
Code is an asset in the same way that any process documents in your organization are. They represent codified solutions to problems.You do not need to re-solve this problem, and when a similar problem occurs, you can adapt the existing solution to the new problem.Another way to think about it: if code was not an asset, we would delete it immediately after compilation.
Having no code corresponding to the software in service is a bigger liability than having it
Good thing they aren't thin-skinned and censorious! Being able to take well-meant criticism in stride is so important, I'm glad that's a core value for them.
Ports of massive legacy codebases are possible and they happen. They can be extremely difficult, they take will and effort but they can get done. The idea that you have to slow down the development of the language standard for people who won't port to the new version is weird- Those people won't be updating compilers anyway.How do I know this? I migrated a codebase of about 20m lines of C++ at a major investment bank from pre-ansi compilers to ansi conformance across 3 platforms (Linux, Solaris and Windows).  Not all the code ran on all 3 platforms (I'm looking at you, Solaris) but the vast majority did. Some of it was 20 years old before I touched it - we're talking pre-STL not even just pre ansi. The team was me + one other dude for Linux and Solaris and me + one other different dude for windows, and to give you an idea the target for gcc went from gcc 2.7[1] to gcc 4[2], so a pretty massive change. The build tooling was all CMake + a bunch of special custom shell we had developed to set env vars etc and a CI/CD pipeline that was all custom (and years ahead of its time). Version control was CVS.  So, single central code repo and if there was a version conflict an expert (of which I was one but it gives me cold sweats) had to go in, edit the RCS files by hand and if they screwed up all version control for everyone was totally hosed until someone restored from backup and redid the fix successfully.While we were doing the port to make things harder there was a community of 667 developers[3] actively developing features on this codebase and it had to get pushed out come hell or high water every 2 weeks.  Also, this being the securities division of a major investment bank, if anything screwed up real money would be lost.It was a lot of work, but it got done. I did all my work using vim and quickfix lists (not any fancy pants tooling) including on windows but my windows colleague used visual C++ for his work.[4][1] Released in 1995[2] Released in 2005[3] yes.  The CTO once memorably described it to me as "The number of the beast plus Kirat". Referring to one particularly prolific developer who is somewhat of a legend on Wall Street.[4] This was in the era of "debugging the error novel" so you're talking 70 pages of ascii sometimes for a single error message with a template backtrace, and of course when you're porting you're getting tens of thousands of these errors. I actually wrote FAQs (for myself as much as anything) about when you were supposed to change "class" to "typename", when you needed "typedef typename" and when you just needed "typedef" etc.  So glad I don't do that any more.
Was it Morgan Stanley?  That is the only shop I can think of that is so focused on C++.  Hell, they hired Bjarne Stroustrup.But since you say version control was CVS, then I guess it was Goldman.  They still have that sheizen for SecDB/Slang today.And I assume that "Kirat" is Kirat Singh of Goldman SecDB/JPM Athena/BofA Quartz/Beacon?
Yes goldman and yes that Kirat. Fun fact, the Windows port colleague was John Madsen who later became CTO of Goldman I think.
Google brought up this; Kirat Singh -https://apacentrepreneur.com/kirat-singh-passion-and-determi...Very impressive indeed.
> Stories of people trying their best to participate in the C++-standard committee process across multiple yearsThis links to:https://thephd.dev/finally-embed-in-c23It was a fascinating story, particularly about how people finally coming to terms with accepting that a seemly ugly way of doing things really is the best way (you just can't "parse better").The feature itself is interesting too.https://gcc.godbolt.org/z/jGajc6xd5
It's fascinating how muchmore complicatedthis ends up being to deliver in the C and C++ ecosystem.#embed has to pretend - in principle - that we're going to conjure all these byte values into existence, as actual numbers, and then by the "as if" rule the compiler is not really going to do that because it would be crazy slow. The reality that we're just going to shove the data into the program as if it was an array is an (obviously, implemented everywhere) optimisation, rather than part of the language specification.The analogous Rust `include_bytes!` just gets you a &'static [u8; N]  -- an immutable reference to an array of N bytes which lives forever.At first I thought OK, well, maybe the C approach lets you do some clever compile stuff that Rust can't do. Nope. If I have a compile time function checksum which calculates a 64-bit checksum of the slice passed by immutable reference - and a file of 128MB of data called firmware.bin, Rust is completely fine with let sum = checksum(include_bytes!("firmware.bin"));  and that results in a 64-bit value, the 128MB file evaporated after being checksummed.
“Governments are telling you to stop using C++”.This invokes the imagery of a  1950s Apollo era scientist saying something serious. But I promise you there is no visionary low level language authority in the background. It’s just a staffer being influenced by the circle of blogs prominent on programming Reddit and twitter.> no overhead principleIt’s actually nice to hear they are asserting a more conservative outlook and have some guiding design principle.Bjarne is more of a super-bureaucrat than a designer. In the early days he pulled C++ into whatever language movements were popular. For a while it looked like Rust was having that influence.But the outcome has been a refinement of C++ library safety features which are moderate and easy to adopt.
Woah, my post made it to the front page and I'm late. Hi!In hindsight I would've probably written a few things differently, but I really didn't want to fall into a trap of getting stuck editing.
https://security.googleblog.com/2024/11/retrofitting-spatial...
The real, everpresent and probably future nail in the coffin of C++ is the lack of a standard apckage manager and build system. The rest is just what happened to be picked up by social/news as it is easier and flashier to talk about.
Ah, we are from the same tribes! Let's go talk to the the bazel and to the vcpkg tribes. But what about the fact that cmake isn't a build system, also conan 2.0 was a bit rough the last I saw.. maybe thats why clion/intellij dropped support for it out of the box and now uses vcpkg?
I've been contributing some C++ packages to xrepo, which personally imo is the best of all worlds. c:
There is a Bazel tribe? I've heard it mentioned a couple of time but I have yet to encounter a C++ project I need to build which supports Bazel but not CMake. In fact, just _any_ Bazel support seems to me quite rare. Am I living in a bubble?As for vcpkg - yeah, that's popular, for sure.
Daisy Hollman says she has "drunk the Bazel kool-aid" and is a big proponent of its usage outside Google.
Bazel is a project created by Google, and Hollman works for Google. So - perhaps the Bazel tribe is people working at Google? There _are_ quite a few of those....
The author doesn’t appear to consider the use of binary-only (typically commercial licensed) libraries.  There’s still a need for an ABI and “modern tooling” isn’t a magic wand.
I'd guess that majority of such binary-only libraries use C ABI interfaces. The entire Windows ecosystem of COM modules works on top of C ABI's.
Until the moment when you are forced to use a third-party SDK with std:: and boost:: (yeah, WTF?) types in the interface.Oh, and you can't avoid that, say, you are working on a trading bot and that's the only "supported" way to connect to an exchange.In the end people usually just reverse engineer and reimplement to get rid of such cursed blob. Fortunately, it works - the vendor can't effectively push all clients to update their SDK too, so all wire protocols are infinitely backward compatible.
The last time I was forced to deal with such a proprietary SDK (that required an ancient Windows C++ runtime, and segfaulted like crazy, natch), rather than waste months reverse-engineering it, I wrapped it in a separate process and talked to it via IPC. That got the job done, and every time their shitty code locked up or crashed, I just restarted the wrapper process from the main application.
Serialized data over stdin/stdout is becoming my favorite protocol for ABI compatibility.The amount of problems this solves is incredible, and it creates none of the ops issues with configuring and launching some new kind of Docker image.
For mummified binary dependencies, C# allows tediously fine control over stack frames in DLL function calls, and similar FFI systems are likely to be equally malleable; there's probably a blind spot towards reverse engineering in C++, due to the expectation that a random ABI should "just work".
The problem is actually not ABI, it's ODR violation. You can make it work, just make your own wrapper in C ABI, link it with whatever dependency (and version) that your vendor insists on, then `-fvisibility=hidden` and partial link the entire shit to avoid ODR violation.People reverse these SDK partly because it makes the codebase saner, and partly because, well, this is trading, a saner implementation is almost guaranteed to be faster than vendor's bullshit one, and guess who cares about being a little bit faster than everyone else?
I think the discussions in these threads show how accurate the framing of this article is. You have some people celebrating Google and friends (slowly) leaving the C++ ecosystem and those that continue to emphasize the flaws that have driven companies away from it in recent history (safety being #1) on the list.
Also see "On 'Safe' C++", which goes deeper into many of the insights brought up by this article. <https://news.ycombinator.com/item?id=42186475>
Having skimmed it, I hope more people read that article.
Several months later, I learned I had experienced slight brain damage due to hypoxia and I’ve been slowly recovering ever since. The worst part of all of this is that I said in that post that I was enjoying golang. In other words, I had brain damage and suddenly found writing Go to be fun. Take from that what you willOMG. ;) It's an interesting rant nonetheless.
One example of this is [...] the new proposed (but not yet approved) Boost website. This is located at boost.io and I’m not going to turn that into a clickable link, and that’s because this proposed website brings with it a new logo. [...] This logo features a Nazi dog whistle. The Nazi SS lightning bolts. 

    The thing about dog whistles like this is that you can feign ignorance or act like someone is seeing something that isn’t there, but for something egregious it’s very hard to defend it in this case.

    Of course, there’s other political dog whistles out there in the tech world right now. Justine Tunney named her C library, cosmopolitan5, which I personally believe is named after the term Rootless Cosmopolitan. This is a pejorative Soviet epithet which was used primarily during Joseph Stalin’s antisemitic campaign in the late 40s and early 50s. This is obviously much harder to prove6 as Justine has done a very good job of deleting some very eyebrow raising tweets over the years, even having them scrubbed from The Internet Archive’s Wayback Machine [...]

    Justine, unfortunately, doesn’t appear to have made any amends either, at least publicly, or even acknowledged her past behavior, though she is more than happy to reference her time in the Occupy Wall Street movement. These days however, she’s busy working on llamafiles for Mozilla. For those of you not in the know, a llamafile is basically for turning an LLM’s weights into an executable.And then he makes (yet another!) detour to AI and C++ which I am going to follow.It's a massive post though. Right now I am an hour in and probably about 75% done and I am skipping most of the linked articles. Except for the Ender's game parts. I highly recommend those.
I read it.To save people the trouble it seemed like a manic rant intended to pick several bones (at least the author is self aware enough to admit as much). It's heavy on the "trust me, I have sources" and light on actual content. It's got enough drama and insinuations from calling people liars, narcissists, to finally nazis. It veers from committee drama, to Trump, to feminism, to AI... very hard to follow.Worthy of a daytime soap opera but other than that there's nothing notable there. Except it does make me want to avoid all these people, on both sides of whatever drama this is.
It has the tone you describe, but I think letting that lead you to avoid both sides as problematic is premature. Melodramatic rant doesn't mean wrong, and if the author is even just half right, there are things we as a community should rectify. Avoiding both sides is an issue if one side has far less power. To make a daring analogy, under the #MeToo banner there were certainly accusations that were false or played up, but many came from sincere people who were backed into a corner. When you hear such an accusation without sufficient evidence yet, the nature of the issue is such that you should be considerate of the underdog. Like it or not, taking a side can sometimes be the only reasonable position.
The C++ community can deal with it, I’m not part of it because I’ve been hearing stuff like this for a loooong time from both sides. I’m sure there’s a lot of truth to it. But in my experience either these situations get fixed early or they fester until a huge explosions (like metoo). Either way, all the drama is a far cry from the things I’m interested in - memory safety. If you can’t talk about that without wading through a sludge of toxic personalities, time is better spent elsewhere.The JeanHeyd Meneide saga really says it all about what’s wrong with the C++ community. I would say their blog post is far more enlightening and avoids all the accusations and combative tone of the linked piece while still making clear what the problem is. And it’s technically enlightening to boot.
Any mirrors/archives? DNS not resolving for meEDIT: found one on wayback:https://web.archive.org/web/20241124225457/https://herecomes...
Weird, it works for me.
I’m not sure I understand the whole ABI argument. Isn’t the raison d’être for namespace versions precisely to evolve the language? Why can’t the existing implementations be copied into a std::v2 but with a changed ABI. Existing ABI issues are non-issues because the old code remains while new code will by default compile against v2 picking up all the goodies and can downgrade the types they actually use across ABI in the places they need by changing the namespace version used for a given compilation unit via compile-time flags (or something along these lines)?Were namespace versions determined to not solve this problem? That would be the most ironic thing after all if the change management system introduced in c++11 to avoid std::string is either unused, untrusted, or unworkable for the purpose it was intended.
The problem is libraries using the stdlib types.if your library has a public function that takes a string argument or returns a string or a struct containing a string then that implicitly is an ABI dependency on either std::v1 or std::v2. The library would have to actively add versions for both ABIs. And if the stdlib type is used in a struct/class it can't even differentiate between those types in the ABI.
> The library would have to actively add versions for both ABIsIt would be up to the library to decide what to do. It could decide to only expose v1 or just v2.> And if the stdlib type is used in a struct/class it can't even differentiate between those types in the ABI.This could be solvable through annotations in the header (explicitly written or implicit through the use of flags) that indicate the version of the types used the std types are referenced implicitly (you could even have warnings and errors to that effect if you encounter it).It seems like a solvable problem to me.
I personally like these discussions about C++. Yes, I think C++ should continue to be C++. I also like it that way.On the other hand, having a bit more transparency into the workgroups and their way of doing things may allow the process become a bit more efficient, approachable, and maybe would allow shedding some of the problems which have accumulated due to being so isolated from the world.Some of the alleged events really leave a bad taste in the mouth, and really casts a shade of doubt for the future of C++.Lastly, alienating people by shredding their work and bullying them emotionally is not the best way to build a next generation of caretakers for one of the biggest languages in the world. It might not fall overnight, but it'll certainly rot from its core if not tended properly. Nothing is too big to fail.
> Relatively modern, capable tech corporations that understand that their code is an asset.I strongly disagree with this. The more code you have, the more resources you have to spend maintaining it. There is a very relevant example close by in the post: the bit about Google having a clang-based tool that can refqactor the entire codebase. Great! Problem is, an engineer had to spend their time writing that, and you had to pay that engineer money - all because you have an unmanageable amount of code.The real tech asset is processes: the things you have figured out in order to manage such an ungodly amount of code. Your most senior engineers, specifically what's in their heads, are an asset too.
Two factions? Considering C++ has everything, I'd assume there are tens of factions.
This is true. That is why there is no leadership committee for the C++ ecosystem. There is no way to select one.
Languages should not have a package management system. They all have a all the world is my language blindspot and fail hard when you have anything else. Sometimes you can build plugins in a different language but they still assume the one true language is all you want.package management belongs to the os - or at least something else.don't get me wrong, package management is a real problem and needs to be solved. I'm arguing against a language package manager we need a language agnostic package manager.
I think C++ is a living proof that not having a standard tooling around the language makes the language a complete pain in the ass to use, with any other language that does standard package managing/tooling out of the box, I can just pin the versions, commit a file to the repository, and on any computer that I'm working on I just issue a single command and everything is handled for me; meanwhile one of the C++ projects I've been working on, it turned out that I cannot build it on my server because one of the libraries I'm using only worked with clang17 which my desktop OS provides but the Debian I'm using on my server is shipping with clang16, and the library was not compatible with the earlier version of some C++ implementation, meanwhile Arch on my desktop updated to clang18, which also broke the library in some fashion, so now I'm sitting here with two systems, one where I want to deploy my software, and one where I want to develop the software, both of which are completely defunct and unable to build my project anymore; now I have to figure out how to build the specific version of clang on both systems and ensure I override a bunch of environment variables when configuring the builds on both of these systems, and then do the same on every new computer I'm developing/deploying on - with a proper tool I could just tell the project file to "use this compiler with this version with this standard" and things would just work. Some people will tell you "yeah bro just use docker with this and that and you will have a reproducible build system everywhere", but the thing is - I do not want to learn a completely unrelated tool and spend hours writing some scripts just to be able to continue working on my project when in any other programming language (like Go, Rust, JS), I can just install the runtime, clone the repo, run a command, and everything is handled for me seamlessly like it should be in 2024.
The problem for me is a "political" one, not a matter of convenience: When I choose a linux distro I implicitly trust the distro maintainers to not backdoor the liveCD, so I might as well trust them to maintain packages transparently. If something happens upstream, we expect the distro maintainers to patch out undesirable behavior, integrate changes into the system as a whole or warn us of changes. Most distros are the same in functionality: the choice of a certain distro is mostly a choice of which political institution (such as a business or non-profit) that we trust to maintain the interoperability of the OS.Languages need to be more agnostic than a package manager requires because I should not have to rope another organization into my trust model.Cargo already goes too far in encouraging a single repository (crates.io) for everything through its default behavior. Who maintains crates.io? Where is the transparency? This is the most important information the user should know when deciding to use crates.io, which is whether or not they can trust the maintainers not to backdoor code, and it is rarely discussed or even mentioned!The default cargo crate (template?) encourages people to use permissive licensing for their code. So that is an example where you are already making implicit political decisions on behalf of the ecosystem and developers. That is alarming and should not be for the language maintainers to decide at all.In C/C++ you have a separation of the standard from the implementation. This is really what makes C/C++ code long-lived, because you do not have to worry about the standard being hijacked by a single group. You have a standard and multiple competing implementations, like the WWW. I cannot encourage the use of Rust while there is only a single widely-accepted implementation.
The problem with that is that no Linux distro maintainer will ever put effort into maintaining every version of every library and compiler perpetually for a specific, seemingly random, programming language (or at least, reasonably, within few major versions including all minor releases in between), but with a tool that versions dependencies and allows for, say, git-based upstream with tag-versioned releases, you can expect to pick any specific version and for things to just work; managing library code for a specific programming language, be it any language, does not seem like the responsibility of an operating system, if anything, the package manager from your OS should be able to just supply the tool to manage the said language (like you currently can with npm, cargo or go); that also does not touch the topic of making things work across different platforms, sure, you maybe found a way to solve this issue in your imaginary Linux distro, how do you solve the problem for a co-developer that uses Windows, or macOS?Additionally, you do not have to necessarily enforce these things on the language level, the standard and the tooling could live as two independent projects coming from the same entity. You could still use the compiler and the libraries from your OS, and build the code like that, or you could just reach out to an optional standardized tool that serves as a glue for all the external tools in a standardized way.Yes, there are a lot of valid concerns with this approach as well, but personally for me, as a frustrated C++ developer, who is most likely going to still use the language for a decade to come, I feel like all the other languages I had mentioned in my previous post had addressed what is my biggest point of frustration with C++, so it's definitely an issue that could be solved. Many tried to do it independently, but due to personal differences, no funding, and different ideas of what should be the scope of such tooling, we ended up with a very fragmented ecosystem of tools, none of which have yet to date been able to fully address an issue that other languages solved.
> The default cargo crate (template?) encourages people to use permissive licensing for their code. So that is an example where you are already making implicit political decisions on behalf of the ecosystem and developers. That is alarming and should not be for the language maintainers to decide at all.You and I must be using two very different versions of Cargo, because on mine the default template doesn't specify a license.
What you are asking for is standard command line flags for the compiler.  Which probably cannot happen though it would be nice.That and a better package manager so your clang wrong version problem cannot have.  Which is what I was trying to get at.
I'd recommend using upstream apt llvm repos if you are using Debian or debian-derivatives like Ubuntu, to make sure you have the same compiler everywhere.
> Some people will tell you "yeah bro just use docker with this and that and you will have a reproducible build system everywhere", but the thing is - I do not want to learn a completely unrelated tool and spend hours writing some scripts just to be able to continue working on my projectYou're working with some seriously hairy technologies, dealing with very knotty compatibility issues, and you don't want to learn... Docker?I find this odd because it's relatively simple (certainly much simpler that a lot of what you're currently dealing with), well documented, has a very small, simple syntax and would probably solve your problems with much less effort than setting up a third development machine.
Docker solves the problems in some cases.  However it forces you to ignore those knotty compatibility issues which is limiting.  (You can't run on *BSD, Mac, windows... if you use docker)  As such for many docker is not in the list of acceptable answers - in particular any open source project should consider docker not an option to solve their problems.
My understanding of the post I was replying to was that the compatibility issues were due to different versions of Linux having different clang versions. If I've understood correctly then Docker is highly likely to be a good solution.> any open source project should consider docker not an option to solve their problemThat's generalising far too much.
Specifications for package interchange are absolutely essential, which is distinct from language endorsed package managers.Python doesn't have a language package manager, you're free to use pip or poetry or uv or whatever, but it does have PEP 517/518, which allow all Python package managers to interact with a common package ecosystem which encompasses polyglot codebases.C++ is only starting to address this problem with efforts like CPS. We have a plethora of packaging formats, Debian, pkg-config, conan, CMake configs, but they cannot speak fluently to one another so the package ecosystem is fractured, presenting an immense obstacle to any integration effort.
> Python doesn't have a language package manager, you're free to use pip or poetry or uv or whatever, but it does have PEP 517/518, which allow all Python package managers to interact with a common package ecosystem which encompasses polyglot codebases.This is a long-standing pain point. LWN has a series of reports covering this, one of which is:https://lwn.net/Articles/920832/
Python polyglot code bases are not a solved problem at all. There have been difficulties installing TensorFlow and PyTorch with poetry for some time, and the installs still regularly break. This is the reason so many people use Conda. In HPC people are increasingly using Spack and EasyBuild to stop you having 10 versions of BLAS installed with all your Python dependencies.Comparing it to other languages isn’t really fair since they don’t have polyglot code bases in the same way, and where native packages exist in for e.g. Npm, then you run into the same problems anyway.
Disagree completely. OS package managers are one of the biggest sources of problems.Basically, once you have an OS level package manager, you have issues of versioning and ABI. You have people writing to the lowest common denominator - see for example being limited to the compiler and libraries available on an old Red Hat version. This need to maintain ABI compatibility has been one of the hugest issues with evolving C++.The OS package manager ends up being a Procrustean bed forcing everything into its mold whether or not it actually fits.Also, this doesn't even have the issue of multiple operating systems and even distros which have different package managers.Rust and Go having their own package managers has helped greatly with real world usage and evolution.
This is a weird opinion, but I think that the OS package manager's complexity is largely owing to the unix directory structure which it just dumps all binaries in /bin, all configuration files in /etc, all libraries in /lib. It comes from a time where everything on the OS was developed by the same group of people.By dumping all the same file types in massive top-level directories, you need a separate program (the package manager) to keep track of which files belong to which packages and dealing with their versions and ABI and stuff. Each package represents code developed by a specific group with a certain model of the system's interoperability.GoboLinux has an interesting play on the problem by changing the directory structure so that the filesystem does most of the heavy lifting.
Interesting point, and I'm included to agree with your main point. I don't think the OS level is preferable, however:Point 1: I do not want my program to only run on only one OS, or to require custom code to make it multi-platform.Point 2: What if there's no OS?
>Point 1: I do not want my program to only run on only one OS, or to require custom code to make it multi-platform.To run on only one OS at build time? I usually just set up cross-compilers from linux if I am making cross-platform C/C++ code.>Point 2: What if there's no OS?You can use a system like bitbake I think.
Which Linux distribution has packages for macOS, Windows, and Android?
And that list isn't even exhaustive regarding OSes in production.
This sets up an untenable N*M explosion:Will the GhostBSD maintainers pin the right version of Haskell's aeson package?Will the Fedora Asahi devs stay on top of the latest Ocaml TLS developments?Will MS package PureScript's code for DOM manipulation?
I think the term "package management system" is a bit over broad a term to talk about.If we are talking about global shared dependencies, sure it may belong in the OS.If we are talking about directly shared code, it may as well belong in the language layer.If we are talking about combining independent opaque libraries, then it might belong in a different "pseudo os" level like NPM.
> package management belongs to the osIt clearly doesn't except if you're a fan of dll hell and outdated packages.
Window's package management is famously bad.  However bugs in their implementation cannot be used to shoot down the concept.
If your solution fails on the large majority of computers, can it really be called a solution? 'All the world is my language' blindspots are nothing compared to 'all the world is GNU/Linux' blindspots.
Oh but of course!The solution to…a problem created directly by a specific approach is to…do even more work ourselves to try and untangle ourselves? And just cross our fingers and just _hope_ that every app/library is fully amenable to being patched this way?Alternatively, we could realise that this isn’t really feasible at the scale that the ecosystem operates at now, and that instead of taking an approach that requires us to “do extra work to untangle ourselves” we should try and…not have that problem in the first place.
I don't think it's unreasonable to have a system where every program uses the same version of a library.>And just cross our fingers and just _hope_ that every app/library is fully amenable to being patched this way?It requires some foresight in designing the application, and whether or not you even choose to use that application in the first place. We should strive to decrease the complexity of the system as a whole. The fact that packages are using different versions of the same library in the first place is a canary and the system should disincentivize that use case to some extent. Using static libraries or a chroot or a sandbox for everything is sweeping the problems under the carpet.>taking an approach that requires us to “do extra work to untangle ourselves” we should try and…not have that problem in the first place.I would prefer a system that allows you to link every application to the same library as a default, but also allows for some per-application override, perhaps by using symlinks. That would cover the majority of use cases. But I do not think that dynamic linking is generally in vain.In my own projects, I try to rely on static linking as much as possible, so I understand your perspective as a developer. But as a user I do not want programs to have their own dependencies separate from the rest of the system.
> I don't think it's unreasonable to have a system where every program uses the same version of a library.I really think it is. Even at the scale of a single app it may sometimes make sense to have multiple versions of a same library, if for instance it implements a given algorithm in two different ways and both ways have useful properties
Then shouldn't these APIs be exposed as different libraries?
maybe ? in the end it's up to the person developing said library
I have seen this (linking with multiple versions of the same library) for maintaining backwards compatibility, for example to support parsing a data file from a previous version, but never for selecting different algorithms.
> I don't think it's unreasonable to have a system where every program uses the same version of a library.If there were guarantees that every library would always be both forwards and backwards compatible, that would be reasonable. Sadly, that's not the case.
Could a more streamlined “conception” of something like Gentoo fix this?Applications ship their lock files + version constraints. Gets merged into a user/os level set of packages. You update one package, OS can figure out what it has to rebuild and goes off and does that.Still shit-out-of-luck for anything proprietary, and it’s still super possible for users to end up looking at compile failures, but technically fits the bill?
> The solution to…a problem created directly by a specific approach is to…do even more work ourselves to try and untangle ourselves?The solution is to be more professional. DLL hell comes from libraries that break compatibility: serious libraries should not break compatibility, or at least not often. Then when they do and you happen to have the issue, it's totally fair to go patch the library you depend on that depends on the breaking lib. Even in proprietary software.The modern way is to use ZeroVer [1] and language package managers that pull hundreds of dependencies in the blink of an eye. Then asking that people compile everything themselves or use the one system deemed worthy of support (usually Windows and the very latest Ubuntu). And of course not caring about security one bit.[1]:https://0ver.org/
This only works in the context of a single distribution. The moment you have two competing distributions, you're going to have to fork and end up with distro specific applications. Package maintainers won't be able to keep up and software becomes outdated.
But like…why?Let’s say we make a “thing” which contains packages for all participating languages.98% of the time, aren’t users just going to go “filter down to my language” and just continue what they’re doing, except with a somewhat worse overall experience, depending on whatever the “lowest common denominator” API + semantics we use for this shared package management solution.Multi-language build systems already exist, which happily serve the needs to those projects which find themselves needing cross-language (+distributed) builds. Could there be some easier versions of these? Sure, but I don’t feel like “throw everyone in the same big box” is the solution here.
> I don’t feel like “throw everyone in the same big box” is the solutionIt has to be - while nobody needs more than a subset of that big box, the intersection of what everyone needs turns out to be throw everyone in the same big box.  If you have anything less than that one big box you end up many standards and then everyone chooses which standard and in turn something important you need choose the other standard and you can't use it (ie the situation we are in now)Of course making that "one standard to rule them all" easy enough to use is a hard problem.  It may be itself impossible and thus everyone drops back to the current mess.
I don’t entirely buy the argument, but I am intrigued.> the intersection of what everyone needs turns out to be throw everyone in the same big boxI don’t follow: what’s the intersection of JS/Python/Go/Rust package management? What are they all needing that isn’t “download and store packages”? It can’t be OS level configuration, because that’s going to vary by OS and language.
> package management belongs to the osOs package managers do a fundamentally different task than dependency management tools used in development.They ship a bunch of applications and the libraries you need to run the applications.If you need different version of libfoo than e.g. Firefox does, you're out of luck.Need to support a customer with an older release which needs a different version of libfoo? Not gonna happen.Unless you're talking about Nix or Guix, your OS package manager is not a substitute for a dependency management tools.
Fair enough.  The world needs a package manager that is language agnostics and provides input to OS package managers as well as build tools (which should also be language agnostic).
And works even across UNIX and mainframe OSes without package managers....
Agreed. At least, languages should not require its own package management system to be used. There should be a way to invoke the compiler or interpreter without involving that language's own package management system, so that something else (like Bazel) can build on top. Fortunately, most common languages are all like that. You can invoke rustc without cargo. You can use python without pip. You can use javac without maven.
ideally yes. In practice if they wnat to provide a reasonnably good experience they have to do it
npm, maven, and NuGet have caused me far more problems in trying to reproduce builds than the OS package manager ever will.
It does not require particular  careful inspection to see that with all these zillions features comes into C++ 20, C++ still does not a have a straightforward string split function. And I still feel printf is more reliable and easier to use than all these “modern” fmt.There must be some extremely ideological reason behind these horrible “modern” C++ standards.There are some good trend happening during C++ 11, but now it is completely out of control now.
I am working on a new C++ project in 2024 for my part time project. And this article provided me enough information to battle future "Why not use XYZ instead" discussion. ;)My Rational for Using C++ in 2024: (A) Extreme computational performance desired. (B) I learned C++ 20 years back. (C) C++ has good enough Cross-Platform (OS) compatibility.
C++ is dead by entropy. So complex nobody can truly learn it anymore.
I feel the need to point out that `const` is a viral annotation in C++
I think he has this about right. The project I contribute to (and no, I'm not a massive contributor) is LibreOffice and it is a C++ codebase. It has a decent build system that is easy for anyone to run out of the box. It uses modern C++17+ code, and though it has a lot of legacy code, it is being constantly modified by people like Noel Grandin via clang plugins (along with a lot of manual effort).This code was originally developed in the late 1980s.A good packaging tool would have helpeda lot.
I'm stoked to hear they're on C++17 now.When I contributed to LibreOffice (GSoC 2012) they were still on C++03 !
Well, can't really blame them in 2012. Especially that C++11 did bring an ABI break. Looks like they keep it fresh, although C++17 is getting a bit dated. Migration from C++17 to 20 or even 23 is probably a breeze though compared to migrating 03 to 11.
IIRC it wasn't just the ABI break that was a problem, it was the fact that they wanted to build on systems that didn't have a C++11-compliant compiler available yet.
In 2012 that was reasonable.  In 2024 that would be unreasonable, but they are not stuck on C++03 in 2024.   C++17 today with serious plans to force upgrade to C++20 in the near future is a reasonable place to be today.
> It uses modern C++17+ codeHa ha ha, that's funny. It uses pre-98 C++ code, that's set in stone because of extension/UNO APIs. Yes, you can use C++17 in a bunch of places, but not for the basic structures, classes, idioms etc.And - that's coming from a huge LibreOffice supporter. Speak at conventions, got the T-shirts, everything.
You are referring to the UNO API. The internal code is most definitely not stuck in "pre-98 C++ code".
The main problem with bad C++ tooling is often the same, it's having a modular system that relies on importing/exporting binaries, then tracking binary versions when combining applications.You should only track source versions and build things from source as needed.
> The C++ committee seems pretty committed (committeed, if you will)I'll will not, thanks.
Related - Is C doing anything about memory safety so it can be called memory safe?
No, it would be impossible to make C memory safe without just making a new language.
They’re doing some things but there’s no proposals to do anything in the large to move it towards memory safety by default.
What many newer programming platforms (I deliberately don't say "language") got right, is that you can't design a language in a vacuum. If you design a language and leave the implementation open you'll iterate too slowly and eventually you'll grind to a halt or diverge in implementations.A good programming platform has to consist of tooling which includes package managers, compilers, linters, etc. Ideally, in this orbit you would also have "Language Servers" or similar. At the very least, the compiler and language should be written with this in mind, e.g. written for the ground up to support incremental compilation and so on.Go, C#, and Rust all have tooling-first and more importantly first-party tooling. The people who design the language MUST be able to iterate quickly with the people who make the compiler, who in turn should be able to walk down the hall and talk to the people who make the package manager, the package manager repository, and so on.
A plague o' both both your houses!
Replace C++ with asbestos (no, I'm serious, not just stark), and we're basically having exactly the same conversation that's gone on overdecadesin the meatspace world, with analogous players, sunk cost/investment calculus, and migration consternation. The only part of the conversation that is missing is the liability conversation and damages.And I do take asbestos as a serious example. Asbestos is still manufactured and used! Believe or not there are "safe" uses of asbestos and there are protocols around using them. Nevermind the fact that there is a lot of FUD and dishonesty about where exactly the line cuts on what is safe versus not safe...for example we are finding out how brake dust affects the wider environment as we crawl out from under the tent of utter misinformation of a highly motivated entrenched industry.I feel like this is not a new human phenomenon. We made particularly poor choices in what tech we became dependent on, and lo and behold, the entrenched interests keep telling us it's not that bad and we should keep doing it because...reasons.It will eventually play out the way it must; C++ might seem a lot more innocuous than asbestos, and in some ways that's true, but it resists all effort to reform it and will probably end up needing to just be phased out.
Is asbestos superior to other established solutions in terms of "performance", and only lacking on safety?
Asbestos was once considered one of the best material for the industry with many desirable properties like high durability, insulation, flexibility, cheap cost etc etc. I don't think human has found a drop-in replacement for asbestos yet.
> Speaking of big tech, did you notice that Herb Sutter is leaving Microsoft, and that it seems like MSVC is slow to implement C++23 features, and asking the community for prioritization.Uh, they took decades to implement a bunch of C99 features.  Is that predictive?  I suspect it is.
The idea that RAII covers "99% of your ass" is "the low-IQ level statement".Temporal safety is the primary hard problem from a memory safety standpoint, and RAII does nothing to solve it at least the moment a memory allocation crosses abstraction boundaries.
What's an example? I'm just a hobbyist when it comes to c++
Here is a real safety issue that I found and fixed a couple weeks ago. This is an ancient language runtime which originally ran on MS-DOS, Amiga, Atari, and at least a dozen now-discontinued commercial Unices. I've been porting it to 64-bit OSes as a weekend hack. While this particular issue is unlikely to appear in modern applications, a similar pattern might manifest today as ause-after-freeerror withstd::string_viewor an invalid iterator.Background:typedef int Text;The Text type is used to encode different kinds of string-like values. Negative values represent a generated dictionary. Valid indexes in the string intern table (https://en.wikipedia.org/wiki/String_interning) represent a stored string. Other values represent generated variable names.const char *textToStr(Text t) - This takes a Text value and returns a pointer to a null-terminated string. Iftis a string intern index, then it returns a pointer to the stored string. Iftrepresents either a generated dictionary or generated variable name, then it calls snprintf on a static buffer and returns the buffer's address.Problem:The use of a static buffer intextToStrintroduces a temporal safety issue when multiple calls are made in the same statement. Here’s an excerpt from a diagnostic error message, simplified for clarity:printf(stderr, "Invalid use of \"%s\" with \"%s\"",
                textToStr(e),
                textToStr(s));If botheandsare generated dictionaries or variables, then each call totextToStroverwrites the static buffer used by the other. Since the evaluation order of function arguments in C++ is undefined, the result is unpredictable and depends on the compiler and runtime.
If you're saying C++ won't automatically save you from design mistakes, then I agree. This is a poorly-specified function. The caller can't know exactly how they need to handle the returned pointer.Potential solutions:* Return std::string and eat the cost of sometimes copying interned strings* std::string with your own allocator could probably deal with the two cases fairly cheaply and transparently to the caller* overload an operator<< or similar and put your conversion result directly into a stream, rather than a buffer that then goes into a stream* put your generated values in a container that can keep pointers into it valid under all the operations you do on it (I think STL sets would work), keep them there for the rest of the lifetime of the program, and return your pointers to them (or to the interned constant strings)I think many of these could be termed RAII, so I lean toward the idea in this subthread that RAII and other C++ idioms will help you stay safe, if you use them.P.S. The function is also not safe if called by multiple threads concurrently. Maybe thread-local storage could be an easy fix for that part, but the other solutions should fix it too. [But if you have any shared storage that caches the generated values such as the last solution, it needs to be protected from concurrency.]
This is a 30+ year-old codebase that originally started as pre-ANSI C. The design tradeoffs were very different at the time. I've managed to compile it as C++23 with unreasonably high warning levels, and it's now warning-free. However, there are still many unresolved issues.One notable limitation is the use of aSIGINThandler implemented withsetjmp/longjmp, which reduces the effectiveness of RAII.RegardingtextToStr, there were four instances where it was called twice in the same expression. To avoid potential problems, I replaced these calls with a temporarystd::stringto store the result of the first call before the second is made. In hindsight, none of these cases likely would have caused issues, but I preferred not to take risks. TheSigIntGuardensures that thestd::stringdestructor runs even if aSIGINToccurs:{
        SigIntGuard guard;
        std::string const member{textToView(textOf(fst(b)))};
        std::print(errorStream, R"(No member "{}" in class "{}")", member, textToView(klass(c).text));
    }
No, if you have temporal safety issues you didn't understand RAII. That is pretty much the whole point of RAII.
If you want anyone to believe you, you're going to have to give more than just a blank assertion.  Can you give at least a sketch of your reason for your claim?
Reasoning is, if your objects outlive the scope of your class, then they most likely belong to a class that's higher in the hierarchy (they already do, de facto).
Please explain how you would solve the iterator invalidation problem using only C++ and RAII. Thanks.
This small thread is about temporal safety so you're out of luck.
One, iterator invalidation can be a temporal safety problem. Specifically, if you have an iterator into a vector and you insert into the same vector, the vector might reallocate, resulting in the iterator pointing into invalid memory.Two, consider the unique_ptr example then. Perhaps a library takes a reference  to the contents of the unique_ptr to do some calculation. (by passing the reference into a function) Later, someone comes along and sticks a call to std::async inside the calculation function for better latency. Depending on the launch policy, this will result in a use after free either if the future is first used after the unique_ptr is dead, or if the thread the future is running on does not run until after the unique_ptr is dead.EDIT: Originally I was just thinking of the person who inserted the std::async as being an idiot, but consider that the function might have been templated, with the implicit assumption that you would pass by value.
Every study on security vulnerabilities has shown that "just don't screw up bro" doesn't scale.Even if we ignore the absolute clown move of having no bounds checks by default (and std::span doesn't have themat all), it's very easy to get into trouble with anything involving C++ iterators and references.
>Every study on security vulnerabilities has shown that "just don't screw up bro" doesn't scale.Let's see them.
Well, ignoring all the reports from google, microsoft and mozilla, all of whom are part of a cabal spreading misinformation on the percentage of vulnerabilities caused by memory unsafety in C++ (all 3 arrived at around 70% so it's clearly a made up number they colluded to spread), and ignoring the reports from the United States government (probably infiltrated by rust cultists), I can recommend the paper Memory Errors: The Past, the Present, and the Future
Are you being sarcastic? I genuinely cannot tell.
Yes, except for the legitimate paper recommendation.
RAII only helps with 1 of 4 primary cases of safety. RAII deals (badly) with temporal safety, but not spacial safety (bounds errors etc), safe initialization (use before initialization), or undefined behavior (overflow/underflow, aliasing, etc).
Use-after-free (or reference/iterator invalidation in general) is the main issue. RAII doesn't help there at all. RAII helps with deterministically cleaning up resources, which is important, but barely related to safety.
How does RAIInothelp with safe initialization?  It's right in the name.
> equivalent to people unable to grasp type coercion on JS and thus blaming the language for it (literally just use '===' and stop bitching about it).They're not even remotely equivalent. A single eslint rule has immediately and permanently fixed this in every Javascript project I've worked on, both for me and my coworkers' code. RAII helps, but in C++, no amount of linters and language features can fully protect you.
As a relative newcomer to C++, I have found RAII to be fine for writing in object-oriented style. But I would only subject myself to the suffering and complexity of C++ if I really wanted excellent performance, and while RAII does not generally have a runtime cost by itself, engineering for full performance tends to exclude the things that RAII makes easy. If you manage memory via arenas, you want to make your types trivially destructible. If you don't use exceptions, then RAII is not necessary to ensure cleanup. In addition, use of destructors tends towards template APIs that generate lots of duplicate code, when an API that used a C-style function pointer for generic disposal would have produced much smaller code.And C++'s object model can add additional complexity and sources of UB. In C++20 previously valid code that reads a trivially destructible thread_local after it has been destroyed became UB, even though nothing has actually happened to the backing storage yet.
As an old-timer, I think you have some serious misconception about how RAII works, and what it does for you.> Arena managementThere's nothing that stops you from using arena allocators in C++. (See pmr allocators in C++17 for handling complex non-POD types).> The cost of RAII_you're going to have to clean up one way or another. RAII can be zero-overhead, and usually generates less code than the C idiom of "goto Cleanup".> Use of destructors leads toward template APIs.Not getting that. Use of destructors leads to use of destructors. Not much else.> If you don't use exceptions....Why on earth would  you not use exceptions? Proper error handling in C is a complete nightmare.But even if you don't, lifetime management is a huge problem in C. Not at all trivial to clean things up when you're done. Debugging memory leaks in C code was always a nightmare. The only thing worse was debugging wild memory writes. C++ RAII: very difficult to leak things (impossible, if you're doing it right, which isn't hard), and if it ever does happen almost always related to using C apis that should have been properly wrapped with RAII in the first place.Granted, wrapping C handles in RAII was a bit tedious in C++89; but C++17 now allows you to write a really tidy AutoClose template for doing RAII close/free of C library pointers now. Not in the standard library, but really easy to roll your own:// call snd_pcm_close when the variable goes out of close.
    using snd_pcm_T = pipedal::AutoClose<snd_pcm_t*,snd_pcm_close>;

    snd_pcm_T pcm_handle = snd_pcm_open(....);> C++ 20 undefined behavior of a read-after-free problem.That's not UB; that's a serious bug. And C's behavior would also be "UB" if you read after freeing a pointer.
>As an old-timer, I think you have some serious misconception about how RAII works, and what it does for you.I appreciate the education :-)>There's nothing that stops you from using arena allocators in C++.This is true, but arenas have two wonderful properties - if your only disposable resource is memory, you don't need to implement disposal at all; and you can free large numbers of individual allocations in constant time for immediate reuse. RAII doesn't help for either of these cases, right?>Use of destructors leads to use of destructorsI guess what I mean is... It's totally possible and common to have a zillion copies of std::vector in your binary, even though the basic functionality for trivially copyable trivially destructible types is identical and could be serviced from the same implementation, parameterized only on size and alignment. Destruction could be handled with a function pointer. But part of the reason templates are used so heavily seems to be that there's an expectation that libraries should handle types with destructors as the common case.>lifetime management is a huge problem in C. Not at all trivial to clean things up when you're done.Absolutely true if you're linking pairs of many malloc and free calls. But if you have a model where a per-frame or per-request or per-operation arena is used for all allocations with the same lifetime, you don't have this problem.>And C's behavior would also be "UB" if you read after freeing a pointer.The specific issue I ran into was the destructor of one thread_local reading the value of another thread_local. In C++17 the way to do this was to make one of them trivially destructible, as storage for thread locals is released after all code on that thread has finished, and the lifetime for trivially destructible types ends when storage is freed. In C++20 this was changed, such that the lifetime of a thread local ends when destroyed (rather than when storage is freed) if it's trivially destructible. C thread local lifetimes are tied to storage only and don't have this problem.
You can use `unique_ptr` with a custom deleter for wrapping C libraries.using snd_pcm_T = std::unique_ptr<
    snd_pcm_t,
    decltype([](void* ptr){snd_pcm_close(ptr);})
  >;
  auto pcm_handle = snd_pcm_T(snd_pcm_open(...));The lambda in decltype is C++20, otherwise the deleter type could be declared out-of-line.
What's to get? It's an unsupported claim with substantial counterexamples in the form of every large C++ project. If everyone in the world gets RAII wrong then it doesn't matter what it's theoretically capable of.
Sure pal, in an imaginary world where:Apache/nginx don't exist,Chrome/V8 don't exist,Firefox doesn't exist,GCC/Clang don't exist,MySQL doesn't exist,TensorFlow doesn't exist,VLC doesn't exist,and the list goes on and on ...
every single one of those has had exploits
Did I say they don't?All software has bugs.Is this supposed to be news?
Problem with memory corruption as a bug is that unlike most classes of bug, memory corruption allows remote code execution. (see return oriented programming for the basic version, block oriented programming for the more complex version that bypasses most (all?) mitigation strategies) There are other types of bug that allow remote code execution like this, such as SQL or command-line injection, but those can be solved with better libraries.* However, memory management requires a strong enough type system in the language.* Sorta, for command-line injection you have to know the way the command you are using processes flags and environmental variables in order to know that the filtering you are doing will work. It is absolutely better to use a library instead if you can get away with it.
I'm well aware, but in practice there needs to be some way of at least autovectorising loops built in to the compiler, even JIT/GC'd languages like C# will do this for you.
.NET's compiler does not perform loop autovectorization as it has not been as profitable of a compiler throughput investment as other optimizations (but it does many small optimizations that employ SIMD operations otherwise like unrolling string and span comparisons, copies, moving large structs, zeroing, etc., it also optimizes the SIMD operations themselves ala LLVM)..NET does however offer best-in-class portable SIMD API and large API surface of platform intrinsics both of which are heavily used by CoreLib and many performance-oriented libraries. You can usually port intrinsified implementations hand-written in C++ to C# while making the code more readable and portable and not losing any performance (though sometimes you have to make different choices to make the compiler happy).https://github.com/dotnet/runtime/blob/main/docs/coding-guid...
Oh, that's surprising, I thought RyuJIT could do it with certain types!
If you're interested, here's the overview of planned compiler work for .NET 10:https://github.com/dotnet/runtime/issues/108988Autovectorization is usuallyveryfragile and in areas where you care about it hand-written implementation always provides much better results that will not randomly break on minor changes to compiler version or the code, that must be carefully guarded against.It would be still nice to have it eventually, and I was told that JIT team actively discusses this but there are just many more lower hanging fruits that will light up in disproportionately more instances of user code.If it's any consolation, Clang/LLVM is not a silver bullet either and you will find situations where .NET's compiler output is competitive or even better:https://godbolt.org/z/3aKnePaez
Which features in particular?One of C++'s core tenants is (and has been since the 90's) zero-cost abstractions. Or really, "zero-runtime-cost abstractions"; compile times tend to increase.Obviously some abstractions necessarily require more computation (e.g. raw pointers vs reference-counted smart pointers). But in many cases new features (if implemented correctly!) give better semantics and additional compile-time safety while still compiling down to equivalent binary.
So, here's the thing: Officially, C++ is committed to "What you don’t use, you don’t pay for (zero-overhead rule)”. This is item 2.4 in the reaffirmed design goals:https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p34...but the current ABI _forces_ some abstractions to have unnecessary cost. For example:"Why can a T* be passed in register, but a unique_ptr<T> cannot?"https://stackoverflow.com/q/58339165/1593077another example is improvements in the implementation of parts of the standard library.And that is not the only thing that prevents zero-cost abstraction. C++ does not support pointer restrction, see:https://stackoverflow.com/tags/restrict-qualifier/infoin practice, compilers support it for some contexts.(Anoter, minor issue is the discrepancy of "No viral annotation" and "no heavy annotation" with the need to mark things noexcept to avoid exceptio handling overhead.)
For unique_ptr: This is not a problem that can be solved by the standards committee, they don't control the SysV / Itanium / Win64 standards. You can still use raw pointers if you want to, nothing has beenlostfrom C.For restrict: Universally supported as `__restrict`, thus not a priority for anyone to "officially" solve. Most major performance complaints fall into this category. Eg, std::regex is bad, sure, but nobody uses std::regex so fixing it doesn't matter.
> Don't see what prevents the solution from the language side.The standard has nothing to say about calling convention. Calling conventions are defined by the ABI standards. unique_ptr is a class template, how a class is passed between routines is defined by ABI standards, ergo how unique_ptr is passed between routines is defined by the ABI standards. I don't know what else you're implying here. Unless you're saying we should have language-level smart pointers, not class templates, in which case yes that's an awful idea.> That's not C++...If you're arguing about some abstract, Platonic ideal of C++ that is divorced from the actual implementations that exist in the world, I don't know what your point is. We write code to be compiled by compilers that exist, not printed out and contemplated in a museum. The compilers support __restrict, people use it all the time, so its not a problem.> Even the compilers...Where else do you want it? What would its meaning even be to something like a member variable?Restrict is pointless in any scenario that doesn't involve a potentially aliasing ABI boundary, ie, function parameters. In every other scenario it is ignored.
> The standard has nothing to say about calling convention. Calling conventions are defined by the ABI standards. unique_ptr is a class template, how a class is passed between routines is defined by ABI standards, ergo how unique_ptr is passed between routines is defined by the ABI standards.The ABI defines the calling convention but it is restricted by the guarantees the language already makes. Specifically this part from the System V ABI spec quoted in the linked SO discussions:> If a C++ object has either a non-trivial copy constructor or a non-trivial destructor, it is passed by invisible reference (the object is replaced in the parameter list by a pointer that has class INTEGER).> An object with either a non-trivial copy constructor or a non-trivial destructor cannot be passed by value because such objects must have well defined addresses. Similar issues apply when returning an object from a function.What is needed so that we can have smart pointers passed in registers is:a) A way to specify that the address of the object itself may change (even though the object is not trivially destructible). P1144's [[trivially_relocatable]] would cover this requirement.b) The System V ABI needs to be adapted to make use of this new information. Note that this also requires to make the callee responsible for destruction which may or may not be desirable generally (makes fusing allocation and deallocation harder or impossible in many cases) - raw pointers leave this decision to the programmer.c) The standard library needs to add the new attribute to unique_ptr et al. This would now be an ABI break (the goal is to improve the ABI, duh).So in essence BOTH the language AND the ABI need to be adapted to achieve the optimum behavior here. And even then with only [[trivially_relocatable]] there is still a difference with raw pointers less flexible in which function the destruction happens. And making the ABI depend on [[trivially_relocatable]] limits where the attribute can be added - so the the ABI depends on the the language spec and the standard library spec depends on the ABI for its compatibility requirements.
> The standard has nothing to say about calling convention.If the ABI refers to the standard, the standard can constrain/manipulate the ABI. Specifically, the standard could say destructors and copy ctors must, under certain conditions, be considered trivial for ABI purposes.But really, account42 has it right when he talks about co-changes to the language and the ABI - that's the better way to think about it.> I don't know what else you're implying here.You're feigning ignorance. I said what I implied.> Where else do you want it?In struct fields of course. (Also the corner case of restriction of the "this" pointer, but that could arguably be covered by function parameters + fields)
This isn't about language ABI, which is the realm of the various implementations which have their own stability guarantees.ABI stability in the context of the standards committee is about library ABI, specifically the standard library. When the committee updated the wording about C++'s std::string in C++11, it meant implementers needed to change the layout of a std::string, making this "new" std::string incompatible with the "old" std::string. Any libraries passing std::string across API boundaries needed to be recompiled with the "new" std::string.This has no effect on FFIs for interop with other languages, which are not passing STL types across language boundaries to begin with (a std::string has no meaning in Python).ABI stability for the standard library is motivated by large, old, coroporate codebases which had poor API practices, passed STL types across ABI boundaries, and subsequently lost access to the source code of those libraries and applications or otherwise cannot recompile them for some reason. Many people question the wisdom of catering to such users.
> ABI stability for the standard library is motivated by large, old, coroporate codebases which had poor API practices, passed STL types across ABI boundaries, and subsequently lost access to the source code of those libraries and applications or otherwise cannot recompile them for some reason. Many people question the wisdom of catering to such users.It's also motivated by Linux distributions and other complex systems where rebuilding and installing the world in one go is not possible/feasible.
I don't get why users of these old libraries cannot just pin the compiler to a specific standard and let the rest of us enjoy advancements. At some point the "I want to be using bleeding edge C++ on my side but still be able to dynamically link against arcane artifacts" argument should not hold water for the committee. That is if they really care about the language and not their sponsors.
